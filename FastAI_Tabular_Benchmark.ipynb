{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:07:18.563699Z",
     "start_time": "2019-07-03T22:07:18.413690Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:07:30.159362Z",
     "start_time": "2019-07-03T22:07:19.970779Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the feature-engineered train_clean and test_clean from the Kaggle competition data, run `rossman_data_clean.ipynb`. One important step that deals with time series is this:\n",
    "\n",
    "```python\n",
    "add_datepart(train, \"Date\", drop=False)\n",
    "add_datepart(test, \"Date\", drop=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:41:38.321801Z",
     "start_time": "2019-06-24T12:41:37.621761Z"
    }
   },
   "outputs": [],
   "source": [
    "#trainBench = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")\n",
    "#testBench = pd.read_csv(\"c:/Benchmarking/testBench.csv\")\n",
    "validBench = pd.read_csv(\"c:/Benchmarking/validBench.csv\")\n",
    "#path = Config().data_path()/'rossmann'\n",
    "train_df = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:42:02.770968Z",
     "start_time": "2019-06-24T12:42:02.378549Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100553, 20405)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(validBench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:43:16.421725Z",
     "start_time": "2019-06-24T12:43:15.855693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100550</th>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>28961</td>\n",
       "      <td>1</td>\n",
       "      <td>3376</td>\n",
       "      <td>7385</td>\n",
       "      <td>9893</td>\n",
       "      <td>1018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>100551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100551</th>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>28961</td>\n",
       "      <td>1</td>\n",
       "      <td>3645</td>\n",
       "      <td>7385</td>\n",
       "      <td>9893</td>\n",
       "      <td>1018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>100552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100552</th>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>28961</td>\n",
       "      <td>1</td>\n",
       "      <td>3279</td>\n",
       "      <td>7385</td>\n",
       "      <td>9893</td>\n",
       "      <td>1018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>100553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "100550     1144          0       1              0  Hyper Market   \n",
       "100551     1144          0       1              0  Hyper Market   \n",
       "100552     1144          0       1              0  Hyper Market   \n",
       "\n",
       "       AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "100550        General              28961       1           3376   \n",
       "100551        General              28961       1           3645   \n",
       "100552        General              28961       1           3279   \n",
       "\n",
       "        Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  \\\n",
       "100550            7385        9893                1018  2018               2   \n",
       "100551            7385        9893                1018  2018               2   \n",
       "100552            7385        9893                1018  2018               2   \n",
       "\n",
       "        Week  Day of year  Day of month  Day of week (number)      ID  \n",
       "100550     9           57            26                     2  100551  \n",
       "100551     9           58            27                     3  100552  \n",
       "100552     9           59            28                     4  100553  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:43:44.478603Z",
     "start_time": "2019-06-24T12:43:44.126191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>838</td>\n",
       "      <td>10</td>\n",
       "      <td>5605</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>1293</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>838</td>\n",
       "      <td>10</td>\n",
       "      <td>6635</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>1293</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>100555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>838</td>\n",
       "      <td>10</td>\n",
       "      <td>5981</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>1293</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>100556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "0     1145          0       1              0  Hyper Market   \n",
       "1     1145          0       1              0  Hyper Market   \n",
       "2     1145          0       1              0  Hyper Market   \n",
       "\n",
       "             AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "0  With Non-Food Department                838      10           5605   \n",
       "1  With Non-Food Department                838      10           6635   \n",
       "2  With Non-Food Department                838      10           5981   \n",
       "\n",
       "   Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  Week  \\\n",
       "0            7215       11849                1293  2016               3    10   \n",
       "1            7215       11849                1293  2016               3    10   \n",
       "2            7215       11849                1293  2016               3    10   \n",
       "\n",
       "   Day of year  Day of month  Day of week (number)      ID  \n",
       "0           61             1                     3  100554  \n",
       "1           62             2                     4  100555  \n",
       "2           64             4                     6  100556  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validBench.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:47:40.253202Z",
     "start_time": "2019-06-24T12:47:39.862382Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.concat([train_df,validBench], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T12:47:50.503145Z",
     "start_time": "2019-06-24T12:47:50.161734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120958"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(df); n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:11:04.854055Z",
     "start_time": "2019-06-22T18:11:04.483439Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = np.random.permutation(range(n))[:2000]\n",
    "idx.sort()\n",
    "small_train_df = train_df.iloc[idx[:1000]]\n",
    "small_test_df = train_df.iloc[idx[1000:]]\n",
    "small_cont_vars = ['Region_AreaKM2', 'Region_GDP']\n",
    "small_cat_vars =  ['StoreID', 'Day of week (number)', 'Week']\n",
    "small_train_df = small_train_df[small_cat_vars + small_cont_vars + ['NumberOfSales']]\n",
    "small_test_df = small_test_df[small_cat_vars + small_cont_vars + ['NumberOfSales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:11:08.876828Z",
     "start_time": "2019-06-22T18:11:08.520609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>NumberOfSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>7484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>6350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>8743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     StoreID  Day of week (number)  Week  Region_AreaKM2  Region_GDP  \\\n",
       "118     1000                     3    27            9643       17130   \n",
       "345     1000                     6     6            9643       17130   \n",
       "443     1000                     6    20            9643       17130   \n",
       "524     1000                     3    32            9643       17130   \n",
       "567     1000                     4    38            9643       17130   \n",
       "\n",
       "     NumberOfSales  \n",
       "118           6082  \n",
       "345           7484  \n",
       "443           6350  \n",
       "524           8743  \n",
       "567           7234  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:11:14.009651Z",
     "start_time": "2019-06-22T18:11:13.667235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>NumberOfSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50274</th>\n",
       "      <td>1071</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50287</th>\n",
       "      <td>1071</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>5211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50379</th>\n",
       "      <td>1071</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>5278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50427</th>\n",
       "      <td>1071</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>2916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50461</th>\n",
       "      <td>1071</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>3128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StoreID  Day of week (number)  Week  Region_AreaKM2  Region_GDP  \\\n",
       "50274     1071                     4    34            7215       11849   \n",
       "50287     1071                     3    36            7215       11849   \n",
       "50379     1071                     4    49            7215       11849   \n",
       "50427     1071                     3     4            7215       11849   \n",
       "50461     1071                     2     9            7215       11849   \n",
       "\n",
       "       NumberOfSales  \n",
       "50274           4426  \n",
       "50287           5211  \n",
       "50379           5278  \n",
       "50427           2916  \n",
       "50461           3128  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:12:10.729851Z",
     "start_time": "2019-06-22T18:12:10.251026Z"
    }
   },
   "outputs": [],
   "source": [
    "categorify = Categorify(small_cat_vars, small_cont_vars)\n",
    "categorify(small_train_df)\n",
    "categorify(small_test_df, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:12:16.443475Z",
     "start_time": "2019-06-22T18:12:16.092455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>NumberOfSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50274</th>\n",
       "      <td>1071</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>4426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50287</th>\n",
       "      <td>1071</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>5211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50379</th>\n",
       "      <td>1071</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>5278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50427</th>\n",
       "      <td>1071</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>2916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50461</th>\n",
       "      <td>1071</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>3128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      StoreID Day of week (number) Week  Region_AreaKM2  Region_GDP  \\\n",
       "50274    1071                    4   34            7215       11849   \n",
       "50287    1071                    3   36            7215       11849   \n",
       "50379    1071                    4   49            7215       11849   \n",
       "50427    1071                    3    4            7215       11849   \n",
       "50461    1071                    2    9            7215       11849   \n",
       "\n",
       "       NumberOfSales  \n",
       "50274           4426  \n",
       "50287           5211  \n",
       "50379           5278  \n",
       "50427           2916  \n",
       "50461           3128  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:12:53.434575Z",
     "start_time": "2019-06-22T18:12:53.108565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "            52, 53],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df.Week.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T14:52:25.221580Z",
     "start_time": "2019-06-23T14:52:24.891173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118    26\n",
       "345     5\n",
       "443    19\n",
       "524    31\n",
       "567    37\n",
       "dtype: int8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df['Week'].cat.codes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:14:03.955381Z",
     "start_time": "2019-06-22T18:14:03.624371Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_missing = FillMissing(small_cat_vars, small_cont_vars)\n",
    "fill_missing(small_train_df)\n",
    "fill_missing(small_test_df, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T18:15:49.815261Z",
     "start_time": "2019-06-22T18:15:49.458254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>NumberOfSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1000</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>7484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>1000</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>6350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    StoreID Day of week (number) Week  Region_AreaKM2  Region_GDP  \\\n",
       "118    1000                    3   27            9643       17130   \n",
       "345    1000                    6    6            9643       17130   \n",
       "443    1000                    6   20            9643       17130   \n",
       "\n",
       "     NumberOfSales  \n",
       "118           6082  \n",
       "345           7484  \n",
       "443           6350  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PromoInterval</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Mean_Humidity</th>\n",
       "      <th>Sales</th>\n",
       "      <th>CompetitionDistance_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185749</th>\n",
       "      <td>622</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>93</td>\n",
       "      <td>4508</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Store DayOfWeek PromoInterval  CompetitionDistance  Mean_Humidity  \\\n",
       "185749   622         2           NaN               2300.0             93   \n",
       "\n",
       "        Sales  CompetitionDistance_na  \n",
       "185749   4508                    True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_df[small_train_df['CompetitionDistance_na'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:07:56.447866Z",
     "start_time": "2019-07-03T22:07:55.900835Z"
    }
   },
   "outputs": [],
   "source": [
    "#trainBench = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")\n",
    "test_df = pd.read_csv(\"c:/Benchmarking/testBench.csv\")\n",
    "#validBench = pd.read_csv(\"c:/Benchmarking/validBench.csv\")\n",
    "train_df = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")\n",
    "valid_df = pd.read_csv(\"c:/Benchmarking/validBench.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:07:57.894949Z",
     "start_time": "2019-07-03T22:07:57.682936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100553, 20405, 2745)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(valid_df),len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:00.210081Z",
     "start_time": "2019-07-03T22:07:59.988068Z"
    }
   },
   "outputs": [],
   "source": [
    "procs=[FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:39:08.552813Z",
     "start_time": "2019-06-27T20:39:08.235795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100551</th>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>28961</td>\n",
       "      <td>1</td>\n",
       "      <td>3645</td>\n",
       "      <td>7385</td>\n",
       "      <td>9893</td>\n",
       "      <td>1018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>100552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100552</th>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>28961</td>\n",
       "      <td>1</td>\n",
       "      <td>3279</td>\n",
       "      <td>7385</td>\n",
       "      <td>9893</td>\n",
       "      <td>1018</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>100553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "100551     1144          0       1              0  Hyper Market   \n",
       "100552     1144          0       1              0  Hyper Market   \n",
       "\n",
       "       AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "100551        General              28961       1           3645   \n",
       "100552        General              28961       1           3279   \n",
       "\n",
       "        Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  \\\n",
       "100551            7385        9893                1018  2018               2   \n",
       "100552            7385        9893                1018  2018               2   \n",
       "\n",
       "        Week  Day of year  Day of month  Day of week (number)      ID  \n",
       "100551     9           58            27                     3  100552  \n",
       "100552     9           59            28                     4  100553  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:39:11.121960Z",
     "start_time": "2019-06-27T20:39:10.804942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "      <td>2668</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>120957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20404</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "      <td>2802</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>120958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "20403     1173          0       1              0  Hyper Market   \n",
       "20404     1173          0       1              0  Hyper Market   \n",
       "\n",
       "                 AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "20403  With Non-Food Department                150       9           2668   \n",
       "20404  With Non-Food Department                150       9           2802   \n",
       "\n",
       "       Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  \\\n",
       "20403           15566       15017                8146  2018               2   \n",
       "20404           15566       15017                8146  2018               2   \n",
       "\n",
       "       Week  Day of year  Day of month  Day of week (number)      ID  \n",
       "20403     9           58            27                     3  120957  \n",
       "20404     9           59            28                     4  120958  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:06.561444Z",
     "start_time": "2019-07-03T22:08:06.356433Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_vars = ['StoreID', 'Day of week (number)', 'Year', 'Month (number)', 'Day of month', 'IsHoliday', 'IsOpen',\n",
    "    'StoreType', 'AssortmentType', 'Region', 'Week', 'Day of year', 'HasPromotions'\n",
    "    ]\n",
    "\n",
    "cont_vars = ['NearestCompetitor', 'Region_AreaKM2', 'Region_GDP', 'Region_PopulationK',\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:14.382892Z",
     "start_time": "2019-07-03T22:08:14.138878Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df=pd.concat([train_df,valid_df], axis=0)\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "train_df['StoreID'] = train_df['StoreID'].astype('category')\n",
    "#train_df['StoreID'] = train_df['StoreID'].cat.codes\n",
    "#train_df['StoreID'] = str(train_df['StoreID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:39:17.809915Z",
     "start_time": "2019-06-27T20:39:17.426695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120955</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "      <td>2541</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>120956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120956</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "      <td>2668</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>120957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120957</th>\n",
       "      <td>1173</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>150</td>\n",
       "      <td>9</td>\n",
       "      <td>2802</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>120958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "120955    1173          0       1              0  Hyper Market   \n",
       "120956    1173          0       1              0  Hyper Market   \n",
       "120957    1173          0       1              0  Hyper Market   \n",
       "\n",
       "                  AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "120955  With Non-Food Department                150       9           2541   \n",
       "120956  With Non-Food Department                150       9           2668   \n",
       "120957  With Non-Food Department                150       9           2802   \n",
       "\n",
       "        Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  \\\n",
       "120955           15566       15017                8146  2018               2   \n",
       "120956           15566       15017                8146  2018               2   \n",
       "120957           15566       15017                8146  2018               2   \n",
       "\n",
       "        Week  Day of year  Day of month  Day of week (number)      ID  \n",
       "120955     9           57            26                     2  120956  \n",
       "120956     9           58            27                     3  120957  \n",
       "120957     9           59            28                     4  120958  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:27.587647Z",
     "start_time": "2019-07-03T22:08:27.341633Z"
    }
   },
   "outputs": [],
   "source": [
    "dep_var = 'NumberOfSales'\n",
    "df = train_df[cat_vars + cont_vars + [dep_var]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:28.740713Z",
     "start_time": "2019-07-03T22:08:28.514700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120958, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:39.661338Z",
     "start_time": "2019-07-03T22:08:39.374321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120958 entries, 0 to 120957\n",
      "Data columns (total 18 columns):\n",
      "StoreID                 120958 non-null category\n",
      "Day of week (number)    120958 non-null int64\n",
      "Year                    120958 non-null int64\n",
      "Month (number)          120958 non-null int64\n",
      "Day of month            120958 non-null int64\n",
      "IsHoliday               120958 non-null int64\n",
      "IsOpen                  120958 non-null int64\n",
      "StoreType               120958 non-null object\n",
      "AssortmentType          120958 non-null object\n",
      "Region                  120958 non-null int64\n",
      "Week                    120958 non-null int64\n",
      "Day of year             120958 non-null int64\n",
      "HasPromotions           120958 non-null int64\n",
      "NearestCompetitor       120958 non-null int64\n",
      "Region_AreaKM2          120958 non-null int64\n",
      "Region_GDP              120958 non-null int64\n",
      "Region_PopulationK      120958 non-null int64\n",
      "NumberOfSales           120958 non-null int64\n",
      "dtypes: category(1), int64(15), object(2)\n",
      "memory usage: 15.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T14:21:16.296403Z",
     "start_time": "2019-06-24T14:21:15.938382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20405"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
    "cut=20405\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:08:46.350720Z",
     "start_time": "2019-07-03T22:08:46.136708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(100553, 120957)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#valid_idx = range(cut-1)\n",
    "valid_idx = range(len(df)-20405, len(df)-1)\n",
    "valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T00:12:11.199597Z",
     "start_time": "2019-06-27T00:12:10.889580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>Region</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>NumberOfSales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120956</th>\n",
       "      <td>1173</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120957</th>\n",
       "      <td>1173</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>15566</td>\n",
       "      <td>15017</td>\n",
       "      <td>8146</td>\n",
       "      <td>2802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StoreID  Day of week (number)  Year  Month (number)  Day of month  \\\n",
       "120956    1173                     3  2018               2            27   \n",
       "120957    1173                     4  2018               2            28   \n",
       "\n",
       "        IsHoliday  IsOpen     StoreType            AssortmentType  Region  \\\n",
       "120956          0       1  Hyper Market  With Non-Food Department       9   \n",
       "120957          0       1  Hyper Market  With Non-Food Department       9   \n",
       "\n",
       "        Week  Day of year  HasPromotions  NearestCompetitor  Region_AreaKM2  \\\n",
       "120956     9           58              0                150           15566   \n",
       "120957     9           59              0                150           15566   \n",
       "\n",
       "        Region_GDP  Region_PopulationK  NumberOfSales  \n",
       "120956       15017                8146           2668  \n",
       "120957       15017                8146           2802  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:09:09.998073Z",
     "start_time": "2019-07-03T22:09:09.779060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120958\n",
      "Index(['StoreID', 'Day of week (number)', 'Year', 'Month (number)',\n",
      "       'Day of month', 'IsHoliday', 'IsOpen', 'StoreType', 'AssortmentType',\n",
      "       'Region', 'Week', 'Day of year', 'HasPromotions', 'NearestCompetitor',\n",
      "       'Region_AreaKM2', 'Region_GDP', 'Region_PopulationK', 'NumberOfSales'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['StoreID',\n",
       " 'Day of week (number)',\n",
       " 'Year',\n",
       " 'Month (number)',\n",
       " 'Day of month',\n",
       " 'IsHoliday',\n",
       " 'IsOpen',\n",
       " 'StoreType',\n",
       " 'AssortmentType',\n",
       " 'Region',\n",
       " 'Week',\n",
       " 'Day of year',\n",
       " 'HasPromotions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df.columns)\n",
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:06:24.428276Z",
     "start_time": "2019-06-26T14:06:24.125258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    5676\n",
       " 1    8111\n",
       " 2    8300\n",
       " 3    7154\n",
       " 4       0\n",
       " Name: NumberOfSales, dtype: int64, 0    1000\n",
       " 1    1000\n",
       " 2    1000\n",
       " 3    1000\n",
       " 4    1000\n",
       " Name: StoreID, dtype: category\n",
       " Categories (174, int64): [1000, 1001, 1002, 1003, ..., 1170, 1171, 1172, 1173])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[dep_var].head(),df['StoreID'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataBunch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:09:21.969757Z",
     "start_time": "2019-07-03T22:09:21.273718Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_seed(seed_value):\n",
    "    import random \n",
    "    random.seed(seed_value) # Python\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    import torch\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "random_seed(0) #must be called before the first fit\n",
    "\n",
    "path=\"c:/Benchmarking/testBench.csv\"\n",
    "data = (TabularList.from_df(df, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n",
    "                .split_by_idx(valid_idx)\n",
    "                .label_from_df(cols=dep_var, label_cls=FloatList, log=False)\n",
    "                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))\n",
    "                .databunch(num_workers=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:00:27.835497Z",
     "start_time": "2019-06-24T18:00:27.492477Z"
    }
   },
   "outputs": [],
   "source": [
    "#doc(FloatList)\n",
    "doc(TabularList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T00:12:35.975014Z",
     "start_time": "2019-06-27T00:12:35.650996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>5676</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>8111</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StoreID  IsHoliday  IsOpen  HasPromotions     StoreType AssortmentType  \\\n",
       "0    1000          0       1              0  Hyper Market        General   \n",
       "1    1000          0       1              0  Hyper Market        General   \n",
       "\n",
       "   NearestCompetitor  Region  NumberOfSales  Region_AreaKM2  Region_GDP  \\\n",
       "0                326       7           5676            9643       17130   \n",
       "1                326       7           8111            9643       17130   \n",
       "\n",
       "   Region_PopulationK  Year  Month (number)  Week  Day of year  Day of month  \\\n",
       "0                2770  2016               3    10           61             1   \n",
       "1                2770  2016               3    10           62             2   \n",
       "\n",
       "   Day of week (number)  ID  \n",
       "0                     3   1  \n",
       "1                     4   2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:40:05.237382Z",
     "start_time": "2019-06-27T20:40:04.930365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TabularLine StoreID 1000; Day of week (number) 3; Year 2016; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 61; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,\n",
       "  FloatItem 5676.0),\n",
       " 100554)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0], len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:09:37.026619Z",
     "start_time": "2019-07-03T22:09:36.795605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>838</td>\n",
       "      <td>10</td>\n",
       "      <td>5605</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>1293</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>100554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1145</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>With Non-Food Department</td>\n",
       "      <td>838</td>\n",
       "      <td>10</td>\n",
       "      <td>6635</td>\n",
       "      <td>7215</td>\n",
       "      <td>11849</td>\n",
       "      <td>1293</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>100555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StoreID  IsHoliday  IsOpen  HasPromotions     StoreType  \\\n",
       "0     1145          0       1              0  Hyper Market   \n",
       "1     1145          0       1              0  Hyper Market   \n",
       "\n",
       "             AssortmentType  NearestCompetitor  Region  NumberOfSales  \\\n",
       "0  With Non-Food Department                838      10           5605   \n",
       "1  With Non-Food Department                838      10           6635   \n",
       "\n",
       "   Region_AreaKM2  Region_GDP  Region_PopulationK  Year  Month (number)  Week  \\\n",
       "0            7215       11849                1293  2016               3    10   \n",
       "1            7215       11849                1293  2016               3    10   \n",
       "\n",
       "   Day of year  Day of month  Day of week (number)      ID  \n",
       "0           61             1                     3  100554  \n",
       "1           62             2                     4  100555  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:09:41.747889Z",
     "start_time": "2019-07-03T22:09:41.532876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TabularLine StoreID 1145; Day of week (number) 3; Year 2016; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 10; Day of year 61; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; ,\n",
       "  FloatItem 5605.0),\n",
       " 20404)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.valid_ds[0],len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:13:06.521318Z",
     "start_time": "2019-06-26T14:13:06.224301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data_block.LabelList"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:13:38.921214Z",
     "start_time": "2019-06-26T14:13:38.600196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>NumberOfSales</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>Week</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hyper Market</td>\n",
       "      <td>General</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2_Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StoreID  IsHoliday  IsOpen  HasPromotions     StoreType AssortmentType  \\\n",
       "0     1000          0       1              0  Hyper Market        General   \n",
       "1     1000          0       1              0  Hyper Market        General   \n",
       "\n",
       "   NearestCompetitor  Region  NumberOfSales  Region_AreaKM2  Region_GDP  \\\n",
       "0                326       7            NaN            9643       17130   \n",
       "1                326       7            NaN            9643       17130   \n",
       "\n",
       "   Region_PopulationK  Year  Month (number)  Week  Day of year  Day of month  \\\n",
       "0                2770  2018               3     9           60             1   \n",
       "1                2770  2018               3     9           61             2   \n",
       "\n",
       "   Day of week (number)      ID  \n",
       "0                     5  1_Test  \n",
       "1                     6  2_Test  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:40:20.631263Z",
     "start_time": "2019-06-27T20:40:20.324245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TabularLine StoreID 1000; Day of week (number) 5; Year 2018; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 9; Day of year 60; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,\n",
       "  EmptyLabel ),\n",
       " 2745)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test_ds[0], len(data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:14:21.985384Z",
     "start_time": "2019-06-26T14:14:21.677366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataBunch.one_batch of TabularDataBunch;\n",
       "\n",
       "Train: LabelList (100554 items)\n",
       "x: TabularList\n",
       "StoreID 1000; Day of week (number) 3; Year 2016; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 61; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 4; Year 2016; Month (number) 3; Day of month 2; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 62; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 6; Year 2016; Month (number) 3; Day of month 4; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 64; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 7; Year 2016; Month (number) 3; Day of month 5; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 65; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 1; Year 2016; Month (number) 3; Day of month 6; IsHoliday 0; IsOpen 0; StoreType Hyper Market; AssortmentType General; Region 7; Week 11; Day of year 66; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; \n",
       "y: FloatList\n",
       "5676.0,8111.0,8300.0,7154.0,0.0\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (20404 items)\n",
       "x: TabularList\n",
       "StoreID 1145; Day of week (number) 3; Year 2016; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 10; Day of year 61; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; ,StoreID 1145; Day of week (number) 4; Year 2016; Month (number) 3; Day of month 2; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 10; Day of year 62; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; ,StoreID 1145; Day of week (number) 6; Year 2016; Month (number) 3; Day of month 4; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 10; Day of year 64; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; ,StoreID 1145; Day of week (number) 7; Year 2016; Month (number) 3; Day of month 5; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 10; Day of year 65; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; ,StoreID 1145; Day of week (number) 1; Year 2016; Month (number) 3; Day of month 6; IsHoliday 0; IsOpen 0; StoreType Hyper Market; AssortmentType With Non-Food Department; Region 10; Week 11; Day of year 66; HasPromotions 0; NearestCompetitor -0.6478; Region_AreaKM2 -0.5520; Region_GDP -0.6524; Region_PopulationK -0.8127; \n",
       "y: FloatList\n",
       "5605.0,6635.0,5981.0,3954.0,0.0\n",
       "Path: .;\n",
       "\n",
       "Test: LabelList (2745 items)\n",
       "x: TabularList\n",
       "StoreID 1000; Day of week (number) 5; Year 2018; Month (number) 3; Day of month 1; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 9; Day of year 60; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 6; Year 2018; Month (number) 3; Day of month 2; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 9; Day of year 61; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 7; Year 2018; Month (number) 3; Day of month 3; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 9; Day of year 62; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 1; Year 2018; Month (number) 3; Day of month 4; IsHoliday 0; IsOpen 0; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 63; HasPromotions 0; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; ,StoreID 1000; Day of week (number) 2; Year 2018; Month (number) 3; Day of month 5; IsHoliday 0; IsOpen 1; StoreType Hyper Market; AssortmentType General; Region 7; Week 10; Day of year 64; HasPromotions 1; NearestCompetitor -0.6879; Region_AreaKM2 -0.2982; Region_GDP 1.1556; Region_PopulationK -0.2763; \n",
       "y: EmptyLabelList\n",
       ",,,,\n",
       "Path: .>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.one_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:15:21.618635Z",
     "start_time": "2019-06-26T14:15:21.277615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_df['NumberOfSales'])*1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:19:08.529307Z",
     "start_time": "2019-07-03T22:19:08.278292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0.0000, 31969.1992])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_log_y = np.log(np.max(train_df['NumberOfSales'])*1.2)\n",
    "max_log_y = np.max(train_df['NumberOfSales'])*1.2\n",
    "y_range = torch.tensor([0, max_log_y], device=defaults.device)\n",
    "y_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:37:15.311885Z",
     "start_time": "2019-06-26T15:37:14.969865Z"
    }
   },
   "outputs": [],
   "source": [
    "learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, \n",
    "                        #y_range=y_range, metrics=exp_rmspe)\n",
    "                        y_range=y_range, metrics=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T18:26:01.525785Z",
     "start_time": "2019-06-24T18:26:01.192765Z"
    }
   },
   "outputs": [],
   "source": [
    "doc(tabular_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:37:19.615122Z",
     "start_time": "2019-06-26T15:37:19.291104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StoreID',\n",
       " 'Day of week (number)',\n",
       " 'Year',\n",
       " 'Month (number)',\n",
       " 'Day of month',\n",
       " 'IsHoliday',\n",
       " 'IsOpen',\n",
       " 'StoreType',\n",
       " 'AssortmentType',\n",
       " 'Region',\n",
       " 'Week',\n",
       " 'Day of year',\n",
       " 'HasPromotions']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:37:21.587235Z",
     "start_time": "2019-06-26T15:37:21.258216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(175, 29)\n",
       "    (1): Embedding(8, 5)\n",
       "    (2): Embedding(4, 3)\n",
       "    (3): Embedding(13, 7)\n",
       "    (4): Embedding(32, 11)\n",
       "    (5): Embedding(3, 3)\n",
       "    (6): Embedding(3, 3)\n",
       "    (7): Embedding(5, 4)\n",
       "    (8): Embedding(3, 3)\n",
       "    (9): Embedding(12, 6)\n",
       "    (10): Embedding(54, 15)\n",
       "    (11): Embedding(367, 44)\n",
       "    (12): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=140, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.001)\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.01)\n",
       "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T14:30:20.384940Z",
     "start_time": "2019-06-26T14:30:20.076923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StoreID',\n",
       " 'Day of week (number)',\n",
       " 'Year',\n",
       " 'Month (number)',\n",
       " 'Day of month',\n",
       " 'IsHoliday',\n",
       " 'IsOpen',\n",
       " 'StoreType',\n",
       " 'AssortmentType',\n",
       " 'Region',\n",
       " 'Week',\n",
       " 'Day of year',\n",
       " 'HasPromotions']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds.cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:38:17.911770Z",
     "start_time": "2019-06-26T15:37:36.610603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-6, end_lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:38:22.498817Z",
     "start_time": "2019-06-26T15:38:21.674770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T20:28:52.036599Z",
     "start_time": "2019-06-24T19:42:58.847696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 45:52 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>224588.625000</td>\n",
       "      <td>2450118.500000</td>\n",
       "      <td>1189.639526</td>\n",
       "      <td>02:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>246409.593750</td>\n",
       "      <td>2461842.250000</td>\n",
       "      <td>1198.248657</td>\n",
       "      <td>02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>221807.500000</td>\n",
       "      <td>2231369.500000</td>\n",
       "      <td>1151.760986</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>260768.296875</td>\n",
       "      <td>2263154.750000</td>\n",
       "      <td>1149.410522</td>\n",
       "      <td>02:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>339583.093750</td>\n",
       "      <td>2365407.750000</td>\n",
       "      <td>1180.717773</td>\n",
       "      <td>02:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>298914.875000</td>\n",
       "      <td>2329027.250000</td>\n",
       "      <td>1176.230957</td>\n",
       "      <td>03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>253039.328125</td>\n",
       "      <td>2160480.750000</td>\n",
       "      <td>1132.467163</td>\n",
       "      <td>03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>239490.390625</td>\n",
       "      <td>2582893.250000</td>\n",
       "      <td>1218.835571</td>\n",
       "      <td>03:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>213683.437500</td>\n",
       "      <td>2162576.500000</td>\n",
       "      <td>1132.567871</td>\n",
       "      <td>03:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>173545.734375</td>\n",
       "      <td>2250416.250000</td>\n",
       "      <td>1145.837280</td>\n",
       "      <td>03:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>178789.062500</td>\n",
       "      <td>2366148.750000</td>\n",
       "      <td>1173.848267</td>\n",
       "      <td>03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>143958.718750</td>\n",
       "      <td>2359839.000000</td>\n",
       "      <td>1172.259155</td>\n",
       "      <td>03:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>155415.703125</td>\n",
       "      <td>2443266.000000</td>\n",
       "      <td>1182.288940</td>\n",
       "      <td>03:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>117898.960938</td>\n",
       "      <td>2398194.000000</td>\n",
       "      <td>1179.656982</td>\n",
       "      <td>03:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>112245.617188</td>\n",
       "      <td>2466424.000000</td>\n",
       "      <td>1189.088257</td>\n",
       "      <td>03:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(6, 4e-4, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:41:34.671575Z",
     "start_time": "2019-06-26T15:38:50.506923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 02:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>778279.625000</td>\n",
       "      <td>2646097.000000</td>\n",
       "      <td>1308.030518</td>\n",
       "      <td>02:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 4e-4, wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T23:37:53.057953Z",
     "start_time": "2019-06-24T23:37:52.709934Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.save('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-24T23:37:56.864568Z",
     "start_time": "2019-06-24T23:37:55.981121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T00:15:44.071041Z",
     "start_time": "2019-06-25T00:15:41.154874Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load('1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T00:01:25.100723Z",
     "start_time": "2019-06-24T23:42:20.490382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 19:04 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>165615.109375</td>\n",
       "      <td>2338660.500000</td>\n",
       "      <td>1152.103271</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>215228.390625</td>\n",
       "      <td>2309487.250000</td>\n",
       "      <td>1159.095825</td>\n",
       "      <td>03:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>206901.656250</td>\n",
       "      <td>2210177.250000</td>\n",
       "      <td>1117.933350</td>\n",
       "      <td>03:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>150979.062500</td>\n",
       "      <td>2343226.250000</td>\n",
       "      <td>1169.857422</td>\n",
       "      <td>03:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>140408.859375</td>\n",
       "      <td>2516216.750000</td>\n",
       "      <td>1197.743530</td>\n",
       "      <td>03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>108072.585938</td>\n",
       "      <td>2473835.500000</td>\n",
       "      <td>1186.167236</td>\n",
       "      <td>03:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(6, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T00:01:50.949201Z",
     "start_time": "2019-06-25T00:01:50.175157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 11:31 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>exp_rmspe</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.011840</th>\n",
       "    <th>0.013236</th>\n",
       "    <th>0.110483</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.010765</th>\n",
       "    <th>0.057664</th>\n",
       "    <th>0.129586</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.010101</th>\n",
       "    <th>0.042744</th>\n",
       "    <th>0.111584</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.008820</th>\n",
       "    <th>0.116893</th>\n",
       "    <th>0.135458</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.009144</th>\n",
       "    <th>0.017969</th>\n",
       "    <th>0.126323</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10th place in the competition was 0.108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds=learn.get_preds(DatasetType.Test)\n",
    "test_df[\"Sales\"]=np.exp(test_preds[0].data).numpy().T[0]\n",
    "test_df[[\"Id\",\"Sales\"]]=test_df[[\"Id\",\"Sales\"]].astype(\"int\")\n",
    "test_df[[\"Id\",\"Sales\"]].to_csv(\"rossmann_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:47:58.293754Z",
     "start_time": "2019-06-26T15:47:57.948735Z"
    }
   },
   "outputs": [],
   "source": [
    "learn2 = tabular_learner(data, layers=[1000,500,200], ps=[0.09,0.5,0.5], emb_drop=0.04, \n",
    "                        #y_range=y_range, metrics=explained_variance)\n",
    "                        y_range=y_range, metrics=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:47:59.497823Z",
     "start_time": "2019-06-26T15:47:59.164804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(175, 29)\n",
       "    (1): Embedding(8, 5)\n",
       "    (2): Embedding(4, 3)\n",
       "    (3): Embedding(13, 7)\n",
       "    (4): Embedding(32, 11)\n",
       "    (5): Embedding(3, 3)\n",
       "    (6): Embedding(3, 3)\n",
       "    (7): Embedding(5, 4)\n",
       "    (8): Embedding(3, 3)\n",
       "    (9): Embedding(12, 6)\n",
       "    (10): Embedding(54, 15)\n",
       "    (11): Embedding(367, 44)\n",
       "    (12): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=140, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.09)\n",
       "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5)\n",
       "    (8): Linear(in_features=500, out_features=200, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.5)\n",
       "    (12): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn2.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:48:44.833088Z",
     "start_time": "2019-06-26T15:48:02.149319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn2.lr_find(start_lr=1e-6, end_lr=0.1, wd=0.5)\n",
    "learn2.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T17:33:10.518723Z",
     "start_time": "2019-06-26T17:27:15.654644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 05:54 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1653461.250000</td>\n",
       "      <td>2905619.250000</td>\n",
       "      <td>1377.647949</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>822542.062500</td>\n",
       "      <td>2607245.250000</td>\n",
       "      <td>1312.510864</td>\n",
       "      <td>02:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn2.fit_one_cycle(2, 1e-2, wd=0.5, div_factor=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/basic_train.html#Learner.get_preds\n",
    "Return predictions and targets on ds_type dataset.\n",
    "The first element of the tuple is a tensor that contains all the predictions.\n",
    "The second element of the tuple is a tensor that contains all the target labels.\n",
    "To also get prediction loss (3rd tensor) along with the predictions and the targets, set with_loss=True in the arguments.\n",
    "DatasetType: Enum = [Train, Valid, Test, Single, Fix]\n",
    "y_pred[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:53:20.644174Z",
     "start_time": "2019-06-26T15:52:41.684124Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=learn2.get_preds(DatasetType.Valid,with_loss=True) #0.792515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:53:21.839243Z",
     "start_time": "2019-06-26T15:53:21.523224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[3968.2031],\n",
       "         [5016.3999],\n",
       "         [5204.3936],\n",
       "         ...,\n",
       "         [ 380.8044],\n",
       "         [3759.1304],\n",
       "         [3545.3110]]),\n",
       " tensor([5605., 6635., 5981.,  ...,    0., 2541., 2668.]),\n",
       " tensor([2679104.0000, 2619866.2500,  603117.5625,  ...,  145012.0312,\n",
       "         1483841.6250,  769674.6250])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:53:23.035311Z",
     "start_time": "2019-06-26T15:53:22.719293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3968.2031],\n",
       "        [5016.3999],\n",
       "        [5204.3936],\n",
       "        ...,\n",
       "        [ 380.8044],\n",
       "        [3759.1304],\n",
       "        [3545.3110]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:53:24.212378Z",
     "start_time": "2019-06-26T15:53:23.896360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3968.2031],\n",
       "        [5016.3999],\n",
       "        [5204.3936],\n",
       "        ...,\n",
       "        [ 380.8044],\n",
       "        [3759.1304],\n",
       "        [3545.3110]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:56:06.892384Z",
     "start_time": "2019-06-26T15:56:06.551364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6642938256263733\n",
      "1207.442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2222956.8"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=learn2.get_preds(DatasetType.Valid)\n",
    "EVv=explained_variance_score(y_pred[1], y_pred[0])\n",
    "print(EVv)\n",
    "MAEv=mean_absolute_error(y_pred[1], y_pred[0])\n",
    "print(MAEv)\n",
    "#MSEv=mean_squared_error(validBench_y, y_pred)\n",
    "MSEv=mean_squared_error(y_pred[1].data, y_pred[0].data)\n",
    "MSEv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T15:04:17.156514Z",
     "start_time": "2019-06-26T15:04:16.418472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn2.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T02:56:03.590572Z",
     "start_time": "2019-06-25T02:56:02.955536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn2.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T18:09:56.766768Z",
     "start_time": "2019-06-26T18:09:56.398747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2905619.2, 2607245.2]\n",
      "2607245.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1312.5108642578125"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(learn2.recorder.val_losses)\n",
    "print(learn2.recorder.val_losses[-1:][0])\n",
    "float(learn2.recorder.metrics[-1:][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T00:47:43.998116Z",
     "start_time": "2019-06-27T00:47:43.629095Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.hp import choice\n",
    "from skopt import gp_minimize\n",
    "import fastai\n",
    "from fastprogress import force_console_behavior\n",
    "import fastprogress\n",
    "\n",
    "\n",
    "Space = [\n",
    "    [500,1000,1100], #'layer1'\n",
    "    [50,100,250,500,1000], #'layer2'\n",
    "    [50,100,250,500,1000], #layer 3\n",
    "        #{'ps': hp.uniform('ps',0,1)},\n",
    "    [5e-4,1e-3,5e-3,1e-2], #lr\n",
    "    [0.2,0.4,0.5,0.6], #wd\n",
    "    [8,10,12,15,25], #div_factor\n",
    "    [0.09,0.08,0.1], #ps1 x[6]\n",
    "    [0.5,0.4,0.6], #ps2\n",
    "    [0.5,0.4,0.6], #ps3\n",
    "    [0.04,0.03,0.05] #emb_drop\n",
    "]\n",
    "\n",
    "xc=0;xb=0;\n",
    "\n",
    "def objective(x):\n",
    "    print(x)\n",
    "    # suppress widgets\n",
    "#     fastprogress.fastprogress.NO_BAR = True\n",
    "#     master_bar, progress_bar = force_console_behavior()\n",
    "#     fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n",
    "#     fastprogress.fastprogress.WRITER_FN = str\n",
    "    \n",
    "    # learn using params from hyperopt\n",
    "    random_seed(0)\n",
    "    learn2 = tabular_learner(data, layers=[int(x[0]),int(x[1]),int(x[2])], ps=[x[6],x[7],x[8]], emb_drop=x[9], \n",
    "                        #y_range=y_range, metrics=explained_variance)\n",
    "                        y_range=y_range, metrics=mae)\n",
    "    learn2.fit_one_cycle(1, x[3], wd=x[4], div_factor=int(x[5]))\n",
    "\n",
    "    return float(learn2.recorder.metrics[-1:][0][0])\n",
    "\n",
    "# best = fmin(objective,\n",
    "#     space=Space,\n",
    "#     algo=tpe.suggest,\n",
    "#     max_evals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T02:49:02.270732Z",
     "start_time": "2019-06-27T00:47:56.494830Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 0.01, 0.5, 10, 0.09, 0.5, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>688727.375000</td>\n",
       "      <td>2789954.500000</td>\n",
       "      <td>1346.940552</td>\n",
       "      <td>02:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100, 250, 100, 0.01, 0.4, 15, 0.08, 0.6, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1104406.625000</td>\n",
       "      <td>2203089.250000</td>\n",
       "      <td>1222.422607</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 250, 0.005, 0.5, 12, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:18 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>773990.312500</td>\n",
       "      <td>2354254.500000</td>\n",
       "      <td>1234.035034</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 100, 0.0005, 0.4, 8, 0.09, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:18 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2566485.250000</td>\n",
       "      <td>2062796.125000</td>\n",
       "      <td>1120.466919</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 50, 0.01, 0.5, 8, 0.08, 0.6, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:04 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1276933.125000</td>\n",
       "      <td>2325077.500000</td>\n",
       "      <td>1255.606201</td>\n",
       "      <td>02:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 1000, 0.005, 0.5, 25, 0.08, 0.4, 0.4, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:55 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>760844.562500</td>\n",
       "      <td>2221316.250000</td>\n",
       "      <td>1199.322632</td>\n",
       "      <td>01:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 500, 100, 0.01, 0.6, 15, 0.09, 0.4, 0.6, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:42 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1255008.000000</td>\n",
       "      <td>2189372.500000</td>\n",
       "      <td>1216.012695</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100, 250, 50, 0.001, 0.2, 12, 0.1, 0.4, 0.4, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:27 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1704375.125000</td>\n",
       "      <td>1968327.125000</td>\n",
       "      <td>1128.338501</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100, 1000, 100, 0.005, 0.6, 12, 0.1, 0.6, 0.5, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:15 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1191669.000000</td>\n",
       "      <td>2323434.500000</td>\n",
       "      <td>1263.983276</td>\n",
       "      <td>02:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 1000, 500, 0.01, 0.4, 10, 0.08, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:05 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>752191.375000</td>\n",
       "      <td>2427047.750000</td>\n",
       "      <td>1250.370239</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 50.0, 100.0, 0.005, 0.4, 8.0, 0.08, 0.4, 0.4, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:07 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>929512.375000</td>\n",
       "      <td>2242007.250000</td>\n",
       "      <td>1217.476685</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 500.0, 0.001, 0.2, 12.0, 0.1, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:20 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1254691.000000</td>\n",
       "      <td>2014910.375000</td>\n",
       "      <td>1123.746216</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 50.0, 0.001, 0.2, 8.0, 0.09, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:24 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2195590.000000</td>\n",
       "      <td>2198689.500000</td>\n",
       "      <td>1199.568848</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 12.0, 0.1, 0.4, 0.4, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:32 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1297538.125000</td>\n",
       "      <td>1898292.875000</td>\n",
       "      <td>1092.347534</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:34 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1282320.250000</td>\n",
       "      <td>1913907.250000</td>\n",
       "      <td>1093.372803</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:35 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1282320.250000</td>\n",
       "      <td>1913907.250000</td>\n",
       "      <td>1093.372803</td>\n",
       "      <td>01:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:14 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1622164.750000</td>\n",
       "      <td>2082129.000000</td>\n",
       "      <td>1121.314209</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1282320.250000</td>\n",
       "      <td>1913907.250000</td>\n",
       "      <td>1093.372803</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 1000.0, 0.01, 0.4, 25.0, 0.1, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:44 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>780803.125000</td>\n",
       "      <td>2207900.250000</td>\n",
       "      <td>1195.877686</td>\n",
       "      <td>01:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 250.0, 0.0005, 0.5, 25.0, 0.1, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:13 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1620574.000000</td>\n",
       "      <td>1957900.500000</td>\n",
       "      <td>1091.850098</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 50.0, 0.0005, 0.2, 12.0, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:30 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2745167.250000</td>\n",
       "      <td>1981202.375000</td>\n",
       "      <td>1094.335327</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:15 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1594898.125000</td>\n",
       "      <td>1987326.000000</td>\n",
       "      <td>1109.230591</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 500.0, 0.0005, 0.5, 8.0, 0.1, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:45 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1350717.750000</td>\n",
       "      <td>2037305.500000</td>\n",
       "      <td>1145.554443</td>\n",
       "      <td>01:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:15 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1622164.750000</td>\n",
       "      <td>2082129.000000</td>\n",
       "      <td>1121.314209</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 50.0, 0.0005, 0.6, 12.0, 0.1, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:07 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2457507.750000</td>\n",
       "      <td>2098365.750000</td>\n",
       "      <td>1182.765869</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 100.0, 50.0, 0.0005, 0.2, 10.0, 0.08, 0.5, 0.5, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:12 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5381947.500000</td>\n",
       "      <td>2738773.500000</td>\n",
       "      <td>1262.707642</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 250.0, 0.001, 0.5, 10.0, 0.1, 0.4, 0.6, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1319634.625000</td>\n",
       "      <td>2072805.500000</td>\n",
       "      <td>1172.477539</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 100.0, 0.005, 0.2, 15.0, 0.1, 0.5, 0.6, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:08 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1185130.625000</td>\n",
       "      <td>2157939.500000</td>\n",
       "      <td>1186.709961</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 50.0, 250.0, 0.005, 0.2, 8.0, 0.09, 0.5, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:16 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>933235.437500</td>\n",
       "      <td>2098769.000000</td>\n",
       "      <td>1157.125977</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 100.0, 50.0, 0.005, 0.4, 10.0, 0.1, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:11 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1325546.750000</td>\n",
       "      <td>2311137.500000</td>\n",
       "      <td>1230.137573</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:30 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1303400.125000</td>\n",
       "      <td>1939697.750000</td>\n",
       "      <td>1108.568359</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 100.0, 1000.0, 0.005, 0.6, 15.0, 0.09, 0.4, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:18 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>673579.875000</td>\n",
       "      <td>2299821.000000</td>\n",
       "      <td>1211.077515</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 500.0, 50.0, 0.01, 0.5, 8.0, 0.1, 0.4, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:16 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1256723.875000</td>\n",
       "      <td>2373600.000000</td>\n",
       "      <td>1263.550293</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.1, 0.4, 0.4, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1263805.375000</td>\n",
       "      <td>1926757.875000</td>\n",
       "      <td>1089.112183</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 100.0, 0.0005, 0.2, 25.0, 0.1, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:22 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2498925.750000</td>\n",
       "      <td>2065382.500000</td>\n",
       "      <td>1118.238647</td>\n",
       "      <td>01:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.1, 0.6, 0.6, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:24 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2956374.500000</td>\n",
       "      <td>2087782.375000</td>\n",
       "      <td>1128.574585</td>\n",
       "      <td>01:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 10.0, 0.1, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:52 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1478277.000000</td>\n",
       "      <td>1976289.250000</td>\n",
       "      <td>1102.320190</td>\n",
       "      <td>01:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.5, 15.0, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:48 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1323449.500000</td>\n",
       "      <td>1928997.250000</td>\n",
       "      <td>1101.340576</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.1, 0.6, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1578983.750000</td>\n",
       "      <td>1976823.125000</td>\n",
       "      <td>1100.571655</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:50 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1317447.875000</td>\n",
       "      <td>1983683.750000</td>\n",
       "      <td>1105.490601</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:48 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1407012.750000</td>\n",
       "      <td>1854469.875000</td>\n",
       "      <td>1057.550781</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 500.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:40 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1666955.625000</td>\n",
       "      <td>1981831.250000</td>\n",
       "      <td>1119.384399</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:42 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1491537.625000</td>\n",
       "      <td>2060097.875000</td>\n",
       "      <td>1132.421509</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 500.0, 1000.0, 0.0005, 0.2, 25.0, 0.09, 0.5, 0.4, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:16 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1317209.750000</td>\n",
       "      <td>1942109.250000</td>\n",
       "      <td>1108.893555</td>\n",
       "      <td>02:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 500.0, 500.0, 0.0005, 0.6, 15.0, 0.1, 0.6, 0.5, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:57 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1575089.750000</td>\n",
       "      <td>2055789.125000</td>\n",
       "      <td>1158.357056</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 1000.0, 250.0, 0.0005, 0.6, 12.0, 0.09, 0.6, 0.4, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:21 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1381574.875000</td>\n",
       "      <td>2135962.750000</td>\n",
       "      <td>1193.590820</td>\n",
       "      <td>02:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.09, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:58 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1473387.000000</td>\n",
       "      <td>2021996.500000</td>\n",
       "      <td>1132.598389</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 100.0, 1000.0, 0.01, 0.4, 10.0, 0.1, 0.6, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:29 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>758560.875000</td>\n",
       "      <td>2349289.500000</td>\n",
       "      <td>1230.405151</td>\n",
       "      <td>01:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 1000.0, 100.0, 0.001, 0.2, 12.0, 0.1, 0.6, 0.6, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:43 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1613067.625000</td>\n",
       "      <td>2060113.750000</td>\n",
       "      <td>1163.950195</td>\n",
       "      <td>01:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 50.0, 1000.0, 0.01, 0.2, 15.0, 0.1, 0.5, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:27 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>844590.937500</td>\n",
       "      <td>2295483.250000</td>\n",
       "      <td>1205.614258</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 100.0, 100.0, 0.001, 0.5, 25.0, 0.09, 0.6, 0.6, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:13 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1834103.875000</td>\n",
       "      <td>2249145.250000</td>\n",
       "      <td>1209.236084</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 10.0, 0.08, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:52 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1513565.125000</td>\n",
       "      <td>2001780.500000</td>\n",
       "      <td>1111.307251</td>\n",
       "      <td>01:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 500.0, 1000.0, 0.001, 0.6, 8.0, 0.09, 0.4, 0.6, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:56 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1021338.500000</td>\n",
       "      <td>1950804.375000</td>\n",
       "      <td>1120.934814</td>\n",
       "      <td>01:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 500.0, 50.0, 0.001, 0.2, 15.0, 0.08, 0.5, 0.4, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:40 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1497323.625000</td>\n",
       "      <td>2081029.375000</td>\n",
       "      <td>1154.499146</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 50.0, 50.0, 0.001, 0.5, 15.0, 0.08, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 00:58 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2518912.500000</td>\n",
       "      <td>2147236.750000</td>\n",
       "      <td>1167.850708</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 50.0, 500.0, 0.0005, 0.2, 10.0, 0.08, 0.5, 0.4, 0.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:18 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1590379.875000</td>\n",
       "      <td>2115723.500000</td>\n",
       "      <td>1167.624512</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.6, 8.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1214495.625000</td>\n",
       "      <td>1957882.750000</td>\n",
       "      <td>1099.762329</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.08, 0.4, 0.6, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:52 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1459264.625000</td>\n",
       "      <td>2048266.250000</td>\n",
       "      <td>1134.071167</td>\n",
       "      <td>01:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.08, 0.4, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:50 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1353290.875000</td>\n",
       "      <td>1954093.125000</td>\n",
       "      <td>1093.957886</td>\n",
       "      <td>01:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.09, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:49 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1519556.625000</td>\n",
       "      <td>2052885.750000</td>\n",
       "      <td>1130.325684</td>\n",
       "      <td>01:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:10 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2137379.500000</td>\n",
       "      <td>1898931.625000</td>\n",
       "      <td>1097.807983</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 250.0, 0.0005, 0.2, 25.0, 0.09, 0.4, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:27 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1835672.000000</td>\n",
       "      <td>1888025.500000</td>\n",
       "      <td>1101.697266</td>\n",
       "      <td>01:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:399: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:08 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2137379.500000</td>\n",
       "      <td>1898931.625000</td>\n",
       "      <td>1097.807983</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 100.0, 0.0005, 0.4, 10.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:23 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2198500.500000</td>\n",
       "      <td>1981865.000000</td>\n",
       "      <td>1100.237061</td>\n",
       "      <td>01:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000.0, 250.0, 100.0, 0.0005, 0.2, 15.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:25 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2367265.500000</td>\n",
       "      <td>2114353.500000</td>\n",
       "      <td>1132.648926</td>\n",
       "      <td>01:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 250.0, 0.0005, 0.4, 8.0, 0.09, 0.4, 0.6, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:41 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1927624.250000</td>\n",
       "      <td>1978431.000000</td>\n",
       "      <td>1140.510254</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1586184.500000</td>\n",
       "      <td>1914105.875000</td>\n",
       "      <td>1080.042114</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 500.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:19 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1596818.000000</td>\n",
       "      <td>2031926.750000</td>\n",
       "      <td>1120.544067</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 12.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:30 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1352505.875000</td>\n",
       "      <td>1922889.750000</td>\n",
       "      <td>1092.815186</td>\n",
       "      <td>01:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1511122.375000</td>\n",
       "      <td>1878103.750000</td>\n",
       "      <td>1070.550049</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 500.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:20 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1593591.375000</td>\n",
       "      <td>2016464.125000</td>\n",
       "      <td>1112.438843</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.5, 15.0, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:33 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1481462.000000</td>\n",
       "      <td>1895770.875000</td>\n",
       "      <td>1089.051147</td>\n",
       "      <td>01:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.6, 15.0, 0.09, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:32 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1516744.625000</td>\n",
       "      <td>1998453.500000</td>\n",
       "      <td>1114.306274</td>\n",
       "      <td>01:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:31 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1396522.000000</td>\n",
       "      <td>1910541.125000</td>\n",
       "      <td>1090.510254</td>\n",
       "      <td>01:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.6, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 01:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1439736.000000</td>\n",
       "      <td>1999592.750000</td>\n",
       "      <td>1122.880127</td>\n",
       "      <td>01:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          fun: 1057.55078125\n",
      "    func_vals: array([1346.940552, 1222.422607, 1234.035034, 1120.466919, ..., 1089.051147, 1114.306274, 1090.510254, 1122.880127])\n",
      "       models: [GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396)]\n",
      " random_state: <mtrand.RandomState object at 0x000000002ED43900>\n",
      "        space: Space([Categorical(categories=(500, 1000, 1100), prior=None),\n",
      "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
      "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
      "       Categorical(categories=(0.0005, 0.001, 0.005, 0.01), prior=None),\n",
      "       Categorical(categories=(0.2, 0.4, 0.5, 0.6), prior=None),\n",
      "       Categorical(categories=(8, 10, 12, 15, 25), prior=None),\n",
      "       Categorical(categories=(0.09, 0.08, 0.1), prior=None),\n",
      "       Categorical(categories=(0.5, 0.4, 0.6), prior=None),\n",
      "       Categorical(categories=(0.5, 0.4, 0.6), prior=None),\n",
      "       Categorical(categories=(0.04, 0.03, 0.05), prior=None)])\n",
      "        specs: {'args': {'n_jobs': 1, 'kappa': 1.96, 'xi': 0.01, 'n_restarts_optimizer': 5, 'n_points': 10000, 'callback': None, 'verbose': False, 'random_state': <mtrand.RandomState object at 0x000000002ED43900>, 'y0': None, 'x0': None, 'acq_optimizer': 'auto', 'acq_func': 'gp_hedge', 'n_random_starts': 10, 'n_calls': 75, 'base_estimator': GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
      "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
      "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
      "             optimizer='fmin_l_bfgs_b', random_state=209652396), 'dimensions': Space([Categorical(categories=(500, 1000, 1100), prior=None),\n",
      "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
      "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
      "       Categorical(categories=(0.0005, 0.001, 0.005, 0.01), prior=None),\n",
      "       Categorical(categories=(0.2, 0.4, 0.5, 0.6), prior=None),\n",
      "       Categorical(categories=(8, 10, 12, 15, 25), prior=None),\n",
      "       Categorical(categories=(0.09, 0.08, 0.1), prior=None),\n",
      "       Categorical(categories=(0.5, 0.4, 0.6), prior=None),\n",
      "       Categorical(categories=(0.5, 0.4, 0.6), prior=None),\n",
      "       Categorical(categories=(0.04, 0.03, 0.05), prior=None)]), 'func': <function objective at 0x0000000037E60C80>}, 'function': 'base_minimize'}\n",
      "            x: [1100.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x_iters: [[1000, 1000, 1000, 0.01, 0.5, 10, 0.09, 0.5, 0.5, 0.03], [1100, 250, 100, 0.01, 0.4, 15, 0.08, 0.6, 0.5, 0.05], [1000, 1000, 250, 0.005, 0.5, 12, 0.08, 0.6, 0.5, 0.03], [500, 500, 100, 0.0005, 0.4, 8, 0.09, 0.4, 0.6, 0.03], [1000, 1000, 50, 0.01, 0.5, 8, 0.08, 0.6, 0.4, 0.04], [500, 500, 1000, 0.005, 0.5, 25, 0.08, 0.4, 0.4, 0.03], [1000, 500, 100, 0.01, 0.6, 15, 0.09, 0.4, 0.6, 0.05], [1100, 250, 50, 0.001, 0.2, 12, 0.1, 0.4, 0.4, 0.05], [1100, 1000, 100, 0.005, 0.6, 12, 0.1, 0.6, 0.5, 0.04], [500, 1000, 500, 0.01, 0.4, 10, 0.08, 0.5, 0.4, 0.04], [1000.0, 50.0, 100.0, 0.005, 0.4, 8.0, 0.08, 0.4, 0.4, 0.03], [500.0, 250.0, 500.0, 0.001, 0.2, 12.0, 0.1, 0.4, 0.6, 0.03], [1000.0, 250.0, 50.0, 0.001, 0.2, 8.0, 0.09, 0.4, 0.6, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.5, 12.0, 0.1, 0.4, 0.4, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04], [500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04], [500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.5, 0.4, 0.04], [500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.04], [1000.0, 250.0, 1000.0, 0.01, 0.4, 25.0, 0.1, 0.6, 0.5, 0.03], [500.0, 250.0, 250.0, 0.0005, 0.5, 25.0, 0.1, 0.5, 0.4, 0.04], [1100.0, 250.0, 50.0, 0.0005, 0.2, 12.0, 0.1, 0.4, 0.5, 0.03], [500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.4, 0.4, 0.04], [1100.0, 250.0, 500.0, 0.0005, 0.5, 8.0, 0.1, 0.4, 0.4, 0.04], [500.0, 250.0, 250.0, 0.0005, 0.5, 10.0, 0.1, 0.5, 0.4, 0.04], [500.0, 250.0, 50.0, 0.0005, 0.6, 12.0, 0.1, 0.5, 0.4, 0.04], [1100.0, 100.0, 50.0, 0.0005, 0.2, 10.0, 0.08, 0.5, 0.5, 0.04], [1100.0, 250.0, 250.0, 0.001, 0.5, 10.0, 0.1, 0.4, 0.6, 0.05], [500.0, 250.0, 100.0, 0.005, 0.2, 15.0, 0.1, 0.5, 0.6, 0.04], [1100.0, 50.0, 250.0, 0.005, 0.2, 8.0, 0.09, 0.5, 0.5, 0.03], [1000.0, 100.0, 50.0, 0.005, 0.4, 10.0, 0.1, 0.4, 0.6, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.1, 0.4, 0.4, 0.05], [500.0, 100.0, 1000.0, 0.005, 0.6, 15.0, 0.09, 0.4, 0.4, 0.04], [500.0, 500.0, 50.0, 0.01, 0.5, 8.0, 0.1, 0.4, 0.5, 0.05], [1100.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.1, 0.4, 0.4, 0.03], [1000.0, 250.0, 100.0, 0.0005, 0.2, 25.0, 0.1, 0.6, 0.5, 0.03], [1000.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.1, 0.6, 0.6, 0.05], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 10.0, 0.1, 0.4, 0.6, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.5, 15.0, 0.1, 0.4, 0.5, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.1, 0.6, 0.6, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.1, 0.4, 0.5, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.03], [1100.0, 250.0, 500.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.6, 0.03], [1000.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.05], [1100.0, 500.0, 1000.0, 0.0005, 0.2, 25.0, 0.09, 0.5, 0.4, 0.05], [1000.0, 500.0, 500.0, 0.0005, 0.6, 15.0, 0.1, 0.6, 0.5, 0.04], [1100.0, 1000.0, 250.0, 0.0005, 0.6, 12.0, 0.09, 0.6, 0.4, 0.05], [1100.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.09, 0.6, 0.5, 0.03], [1100.0, 100.0, 1000.0, 0.01, 0.4, 10.0, 0.1, 0.6, 0.5, 0.05], [500.0, 1000.0, 100.0, 0.001, 0.2, 12.0, 0.1, 0.6, 0.6, 0.04], [1100.0, 50.0, 1000.0, 0.01, 0.2, 15.0, 0.1, 0.5, 0.5, 0.05], [1000.0, 100.0, 100.0, 0.001, 0.5, 25.0, 0.09, 0.6, 0.6, 0.04], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 10.0, 0.08, 0.4, 0.6, 0.03], [500.0, 500.0, 1000.0, 0.001, 0.6, 8.0, 0.09, 0.4, 0.6, 0.05], [1100.0, 500.0, 50.0, 0.001, 0.2, 15.0, 0.08, 0.5, 0.4, 0.03], [500.0, 50.0, 50.0, 0.001, 0.5, 15.0, 0.08, 0.4, 0.6, 0.03], [1100.0, 50.0, 500.0, 0.0005, 0.2, 10.0, 0.08, 0.5, 0.4, 0.04], [1100.0, 250.0, 1000.0, 0.0005, 0.6, 8.0, 0.08, 0.4, 0.5, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.5, 25.0, 0.08, 0.4, 0.6, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.08, 0.4, 0.5, 0.05], [1100.0, 250.0, 1000.0, 0.0005, 0.2, 8.0, 0.09, 0.6, 0.5, 0.03], [500.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.08, 0.4, 0.5, 0.03], [1000.0, 250.0, 250.0, 0.0005, 0.2, 25.0, 0.09, 0.4, 0.5, 0.05], [500.0, 250.0, 100.0, 0.0005, 0.4, 25.0, 0.08, 0.4, 0.5, 0.03], [1000.0, 250.0, 100.0, 0.0005, 0.4, 10.0, 0.08, 0.4, 0.5, 0.03], [1000.0, 250.0, 100.0, 0.0005, 0.2, 15.0, 0.08, 0.4, 0.5, 0.03], [1100.0, 250.0, 250.0, 0.0005, 0.4, 8.0, 0.09, 0.4, 0.6, 0.05], [500.0, 250.0, 1000.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.03], [500.0, 250.0, 500.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.5, 12.0, 0.08, 0.4, 0.5, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.6, 0.5, 0.03], [500.0, 250.0, 500.0, 0.0005, 0.4, 25.0, 0.08, 0.6, 0.5, 0.05], [500.0, 250.0, 1000.0, 0.0005, 0.5, 15.0, 0.08, 0.6, 0.5, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.6, 15.0, 0.09, 0.6, 0.5, 0.03], [500.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.4, 0.5, 0.03], [1100.0, 250.0, 1000.0, 0.0005, 0.4, 15.0, 0.08, 0.6, 0.5, 0.03]]\n"
     ]
    }
   ],
   "source": [
    "best=gp_minimize(objective, Space, n_calls=75, random_state=0)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T14:29:27.210203Z",
     "start_time": "2019-06-27T14:29:26.605169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3a514748>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(best.func_vals)),best.func_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T13:39:04.335753Z",
     "start_time": "2019-06-27T13:39:03.936730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1100.0, 250.0, 1000.0, 0.0005, 0.2, 15.0, 0.1, 0.4, 0.5, 0.03]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.x\n",
    "#[1100, 250, 1000, 0.0005, 0.2, 15, 0.1, 0.4, 0.5, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T13:38:29.183578Z",
     "start_time": "2019-06-27T13:38:28.773555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1057.55078125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T14:01:38.288549Z",
     "start_time": "2019-06-27T14:01:37.587509Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x4009db70>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T14:15:44.554717Z",
     "start_time": "2019-06-27T14:12:38.653018Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-97bdf510bbae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\plots.py\u001b[0m in \u001b[0;36mplot_objective\u001b[1;34m(result, levels, n_points, n_samples, size, zscale, dimensions)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 xi, yi, zi = partial_dependence(space, result.models[-1],\n\u001b[0;32m    342\u001b[0m                                                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                                                 rvs_transformed, n_points)\n\u001b[0m\u001b[0;32m    344\u001b[0m                 ax[i, j].contourf(xi, yi, zi, levels,\n\u001b[0;32m    345\u001b[0m                                   locator=locator, cmap='viridis_r')\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\plots.py\u001b[0m in \u001b[0;36mpartial_dependence\u001b[1;34m(space, model, i, j, sample_points, n_samples, n_points)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mrvs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mrvs_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m             \u001b[0mzi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\learning\\gaussian_process\\gpr.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, return_std, return_cov, return_mean_grad, return_std_grad)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Predict based on GP posterior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mK_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Line 4 (y_mean = f_star)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0my_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train_mean_\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my_mean\u001b[0m  \u001b[1;31m# undo normal.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mK1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mK2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK1_gradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK2_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    767\u001b[0m                                        K2_gradient * K1[:, :, np.newaxis]))\n\u001b[0;32m    768\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\learning\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manisotropic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             grad = (-np.expand_dims(kernel_prod, axis=-1) *\n\u001b[1;32m--> 413\u001b[1;33m                     np.array(indicator, dtype=np.float32))\n\u001b[0m\u001b[0;32m    414\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             grad = -np.expand_dims(kernel_prod * np.sum(indicator, axis=2),\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skopt.plots\n",
    "_ = skopt.plots.plot_objective(best, levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T14:16:40.382522Z",
     "start_time": "2019-06-27T14:16:27.184906Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-435ff49eade4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_evaluations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_evaluations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\plots.py\u001b[0m in \u001b[0;36mplot_evaluations\u001b[1;34m(result, bins, dimensions)\u001b[0m\n\u001b[0;32m    402\u001b[0m                     \u001b[0mbins_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                 ax[i, i].hist(samples[:, j], bins=bins_,\n\u001b[1;32m--> 404\u001b[1;33m                               range=space.dimensions[j].bounds)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;31m# lower triangle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, normed, **kwargs)\u001b[0m\n\u001b[0;32m   6589\u001b[0m             \u001b[1;31m# this will automatically overwrite bins,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6590\u001b[0m             \u001b[1;31m# so that each histogram uses the same bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6591\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhist_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6592\u001b[0m             \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# causes problems later if it's an int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6593\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmlast\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[0mbin_edges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;31m# Histogram is an integer or a float array depending on the weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36m_get_bin_edges\u001b[1;34m(a, bins, range, weights)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`bins` must be positive, when an integer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         \u001b[0mfirst_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_outer_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36m_get_outer_edges\u001b[1;34m(a, range)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \"\"\"\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mfirst_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfirst_edge\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlast_edge\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_evaluations\n",
    "_ = plot_evaluations(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T18:47:25.936010Z",
     "start_time": "2019-06-26T18:47:25.572989Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          fun: 1087.251708984375\n",
       "    func_vals: array([1253.550049, 1185.695557, 1214.476318, 1211.067993, 1198.736572, 1087.251709, 1157.464233, 1114.233276,\n",
       "       1196.921875, 1223.354004])\n",
       "       models: [GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0) + WhiteKernel(noise_level=1),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b', random_state=209652396)]\n",
       " random_state: <mtrand.RandomState object at 0x00000000489795A0>\n",
       "        space: Space([Categorical(categories=(500, 1000, 1100), prior=None),\n",
       "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
       "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
       "       Categorical(categories=(0.0005, 0.001, 0.005, 0.01), prior=None),\n",
       "       Categorical(categories=(0.2, 0.4, 0.5, 0.6), prior=None),\n",
       "       Categorical(categories=(8, 10, 12, 15, 25), prior=None)])\n",
       "        specs: {'args': {'n_jobs': 1, 'kappa': 1.96, 'xi': 0.01, 'n_restarts_optimizer': 5, 'n_points': 10000, 'callback': None, 'verbose': False, 'random_state': <mtrand.RandomState object at 0x00000000489795A0>, 'y0': None, 'x0': None, 'acq_optimizer': 'auto', 'acq_func': 'gp_hedge', 'n_random_starts': 10, 'n_calls': 10, 'base_estimator': GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
       "             kernel=1**2 * HammingKernel(0, 0, 0, 0, 0, 0),\n",
       "             n_restarts_optimizer=2, noise='gaussian', normalize_y=True,\n",
       "             optimizer='fmin_l_bfgs_b', random_state=209652396), 'dimensions': Space([Categorical(categories=(500, 1000, 1100), prior=None),\n",
       "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
       "       Categorical(categories=(50, 100, 250, 500, 1000), prior=None),\n",
       "       Categorical(categories=(0.0005, 0.001, 0.005, 0.01), prior=None),\n",
       "       Categorical(categories=(0.2, 0.4, 0.5, 0.6), prior=None),\n",
       "       Categorical(categories=(8, 10, 12, 15, 25), prior=None)]), 'func': <function objective at 0x0000000048F4BAE8>}, 'function': 'base_minimize'}\n",
       "            x: [500, 500, 100, 0.0005, 0.4, 8]\n",
       "      x_iters: [[1000, 1000, 1000, 0.01, 0.5, 10], [500, 50, 100, 0.001, 0.6, 12], [1000, 1000, 100, 0.005, 0.4, 25], [500, 1000, 250, 0.01, 0.5, 15], [1100, 250, 250, 0.01, 0.2, 12], [500, 500, 100, 0.0005, 0.4, 8], [500, 100, 1000, 0.001, 0.5, 25], [500, 1000, 500, 0.0005, 0.4, 15], [1000, 100, 50, 0.005, 0.6, 15], [1000, 1000, 250, 0.001, 0.4, 15]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T18:47:46.739730Z",
     "start_time": "2019-06-26T18:47:46.366708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087.251708984375"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T18:47:52.154039Z",
     "start_time": "2019-06-26T18:47:51.762017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1253.550049, 1185.695557, 1214.476318, 1211.067993, 1198.736572, 1087.251709, 1157.464233, 1114.233276,\n",
       "       1196.921875, 1223.354004])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.func_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T18:54:01.102787Z",
     "start_time": "2019-06-26T18:54:00.726765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500, 500, 100, 0.0005, 0.4, 8]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.x #1139.558960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:55:38.710378Z",
     "start_time": "2019-06-27T20:53:37.583627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100, 250, 1000, 0.0005, 0.2, 15, 0.1, 0.4, 0.5, 0.03]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 02:00 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1407012.750000</td>\n",
       "      <td>1854469.875000</td>\n",
       "      <td>1057.550781</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x=best.x\n",
    "random_seed(0)\n",
    "x=[1100, 250, 1000, 0.0005, 0.2, 15, 0.1, 0.4, 0.5, 0.03]\n",
    "print(x)\n",
    "learn3 = tabular_learner(data, layers=[int(x[0]),int(x[1]),int(x[2])], ps=[x[6],x[7],x[8]], emb_drop=x[9], \n",
    "                    #y_range=y_range, metrics=explained_variance)\n",
    "                    y_range=y_range, metrics=mae)\n",
    "learn3.fit_one_cycle(1, x[3], wd=x[4], div_factor=int(x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T20:30:13.878407Z",
     "start_time": "2019-06-27T20:30:09.426153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1854469.9, tensor(1057.5508)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn3.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T18:47:10.666033Z",
     "start_time": "2019-06-27T18:47:10.274011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100554, 18)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds.inner_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling and Disabling the Status Bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:12:11.598460Z",
     "start_time": "2019-07-03T22:12:11.375447Z"
    }
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "import fastprogress\n",
    "\n",
    "#     fastprogress.fastprogress.NO_BAR = True\n",
    "#     master_bar, progress_bar = force_console_behavior()\n",
    "#     fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n",
    "#     fastprogress.fastprogress.WRITER_FN = str\n",
    "\n",
    "def disable_progress():\n",
    "    fastprogress.fastprogress.NO_BAR = True\n",
    "    master_bar, progress_bar = fastprogress.force_console_behavior()\n",
    "    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n",
    "    fastprogress.fastprogress.WRITER_FN = str\n",
    "    \n",
    "def enable_progress():\n",
    "    #fastprogress.fastprogress.NO_BAR=False\n",
    "    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = fastprogress.master_bar, fastprogress.progress_bar\n",
    "    \n",
    "enable_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Eval and Score 3AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:43:05.982524Z",
     "start_time": "2019-07-03T22:43:05.725510Z"
    },
    "code_folding": [
     0,
     17,
     23
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win\n",
      "7\n",
      "3.6.7\n",
      "Fast.ai\n",
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "#Defining Insert Values and Header\n",
    "import platform\n",
    "import sys\n",
    "OpSys=platform.system()[:3]\n",
    "print(OpSys)\n",
    "OpSysVer=platform.release()\n",
    "print(OpSysVer)\n",
    "LangVer=sys.version[:5]\n",
    "print(LangVer)\n",
    "import sklearn\n",
    "Lib='Fast.ai'\n",
    "print(Lib)\n",
    "LibVer= sklearn.__version__\n",
    "print(LibVer)\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "workbook_name = 'C:\\\\Benchmarking\\\\AlgoPerf.xlsx'\n",
    "def xlsADD(row):\n",
    "    wb = load_workbook(workbook_name)\n",
    "    page = wb.active\n",
    "    page.append(row)\n",
    "    wb.save(filename=workbook_name) \n",
    "\n",
    "def InsertHeader():\n",
    "    Result=('OpSys','OpVer', \n",
    "            'Lang', \n",
    "            'LangVer', \n",
    "            'Lib', \n",
    "            'Lib.Ver', \n",
    "            'Algo',\n",
    "            'M_FitTime', \n",
    "            'XVR_FitTime',\n",
    "            'XVR_ScorTime',\n",
    "            'XV_Time',\n",
    "            'XV_EV',\n",
    "            'XV_MAE', \n",
    "            'XV_MSE', \n",
    "            'XV_RMSE', \n",
    "            'XV_R2',\n",
    "            'TS_PredTime',\n",
    "            'TS_EV',\n",
    "            'TS_MAE', \n",
    "            'TS_MSE', \n",
    "            'TS_RMSE', \n",
    "            'TS_R2',\n",
    "            'MeanEV',\n",
    "            'MdlParam', \n",
    "            'FeatImp',\n",
    "            'TdTypes',\n",
    "            'Comments')\n",
    "    #print(Result)\n",
    "    xlsADD(Result)\n",
    "    \n",
    "def InsertValues():\n",
    "    Result=(OpSys, \n",
    "            OpSysVer, \n",
    "            'Python', \n",
    "            LangVer, \n",
    "            Lib, \n",
    "            LibVer,\n",
    "            Algo,\n",
    "            M_FitTime, \n",
    "            XVR_FT,\n",
    "            ST,\n",
    "            XValidTime,\n",
    "            EV,\n",
    "            MAE, \n",
    "            MSE, \n",
    "            RMSE, \n",
    "            R2,\n",
    "            PredTime,\n",
    "            EVv,\n",
    "            MAEv, \n",
    "            MSEv, \n",
    "            RMSEv, \n",
    "            R2v,\n",
    "            EVtot.mean(),\n",
    "            str(Params), \n",
    "            str(d),\n",
    "            str(TrainDataTypes),\n",
    "            Comments)\n",
    "    #print(Result)\n",
    "    xlsADD(Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:43:26.351689Z",
     "start_time": "2019-07-03T22:43:26.052672Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Cross Validate and Score 3 def function:\n",
    "params={}\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "def CrossEval3(data:DataBunch, SplitPercent=20, mp={}, verbose=False,\n",
    "               AlgName=\"\", Comment=\"\", metrics=mae, Retrain=False, Epochs=1):\n",
    "    global Algo\n",
    "    Algo='Fast.ai Tabular'\n",
    "    print(\"Current Model: \", Algo)\n",
    "       \n",
    "    X=data.train_ds.inner_df\n",
    "    global TrainDataTypes\n",
    "    TrainDataTypes=X.dtypes\n",
    "    Nrows,_ =X.shape\n",
    "    SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "    X = X.iloc[:SplitPoint, :]\n",
    "    #if verbose: \n",
    "    print(\"Training data set shape:\",X.shape)\n",
    "    \n",
    "    global M_FitTime, XValidTime, PredTime\n",
    "    global EV, MAE, MSE, RMSE, R2, XVR_FT, ST \n",
    "    \n",
    "    EVar=[]; MAEar=[]; MSEar=[]; R2ar=[]; RMSEar=[];XVR_FTar=[]\n",
    "\n",
    "    # time series cross validation manually\n",
    "    M_FitTime=0\n",
    "    \n",
    "    #suppress widgets\n",
    "    disable_progress()\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "#    Xcols=list(set(X.names)-set('NumberOfSales'))\n",
    "    dep_var='NumberOfSales'\n",
    "    path=\"c:/Benchmarking/trainBench.csv\"\n",
    "    start=time.time() \n",
    "    global learn\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n",
    "        #train = X[min(train_index):max(train_index),:]\n",
    "        #test = X[min(test_index):max(test_index),:]\n",
    "        print(len(train_index),len(val_index))\n",
    "        random_seed(0) #must be called before the first fit\n",
    "        data_fold = (TabularList.from_df(X, cat_names=cat_vars, cont_names=cont_vars, procs=procs,)\n",
    "                .split_by_idx(val_index)\n",
    "                .label_from_df(cols=dep_var, label_cls=FloatList, log=False)\n",
    "                .databunch(num_workers=0))\n",
    "        #random_seed(0)\n",
    "        learn = tabular_learner(data_fold, layers=[int(mp['layer1']),int(mp['layer2'])\n",
    "                               #,int(mp['layer3'])\n",
    "                               ], \n",
    "                               ps=[mp['ps1'],mp['ps2']\n",
    "                               #,mp['ps3']\n",
    "                               ], emb_drop=mp['emb_drop'], \n",
    "                               y_range=y_range, metrics=metrics)\n",
    "        XVR_FT = time.time()\n",
    "        learn.fit_one_cycle(Epochs, mp['lr'], wd=mp['wd'], div_factor=int(mp['div_factor']))\n",
    "        XVR_FT = time.time()-XVR_FT\n",
    "        \n",
    "        \n",
    "        y_pred=learn.get_preds(DatasetType.Valid)\n",
    "        EVar.append(explained_variance_score(y_pred[1], y_pred[0]))\n",
    "        MAEar.append(mean_absolute_error(y_pred[1], y_pred[0]))\n",
    "        MSEar.append(mean_squared_error(y_pred[1], y_pred[0]))\n",
    "        R2ar.append(r2_score(y_pred[1], y_pred[0]))\n",
    "        XVR_FTar.append(XVR_FT)\n",
    "    \n",
    "    M_FitTime=XVR_FT\n",
    "    XValidTime = time.time() - start \n",
    "    RMSEar=np.sqrt(np.array(MSEar))\n",
    "    #XVR_FTar=[0, 0, 0, 0, 0]\n",
    "    STar=[0, 0, 0, 0, 0]\n",
    "    \n",
    "    EV = np.array(EVar).mean()\n",
    "    MAE=np.array(MAEar).mean()\n",
    "    MSE=np.array(MSEar).mean()\n",
    "    RMSE=np.array(RMSEar).mean()\n",
    "    R2=np.array(R2ar).mean()\n",
    "    XVR_FT=np.array(XVR_FTar).sum()\n",
    "    ST=np.array(STar).sum()\n",
    "    \n",
    "    if verbose:print(\"Measured Time for one fit: \", M_FitTime)\n",
    "\n",
    "    global Params\n",
    "    Params=str(mp)\n",
    "    if verbose:print(\" \"); \n",
    "\n",
    "    if verbose:print(\"Cross Validation Performance: \")\n",
    "    print(\"Cross Validation Time: %0.6f\" % (XValidTime))\n",
    "    if verbose:print(\"EV: %0.6f\" % (EV))\n",
    "    if verbose:print(\"EV for each Fold:\",EVar)\n",
    "    #MAE is less sensitive to outliers, The contant value that minimizes the MAE is the median of the target values\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAE))\n",
    "    if verbose:print(\"MAE for each Fold:\",MAEar)\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSE))\n",
    "    if verbose:print(\"MSE for each Fold:\",MSEar)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSE))\n",
    "    if verbose:print(\"RMSE for each Fold:\",RMSEar)\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    if verbose:print(\"XV R2 Actuals:\",R2ar)\n",
    "    if verbose:print(\"Cross Validation R2: %0.6f\" % (R2))\n",
    "    if verbose:print(\"XVR_fit_time Actuals: \", (XVR_FTar))\n",
    "    if verbose:print(\"XVR_fit_time: %0.6f\" % (XVR_FT))\n",
    "    if verbose:print(\"score_time Actuals: \", (STar))      \n",
    "    if verbose:print(\"score_time: %0.6f\" % (ST))\n",
    "    disable_progress()\n",
    "    \n",
    "    #**************Insert comments about this run here:\n",
    "    global Comments    \n",
    "    if Comment==\"\":\n",
    "        Comments=str(mp)\n",
    "    else:\n",
    "        Comments=Comment\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Score Validation Set: \n",
    "    if Retrain:\n",
    "        random_seed(0)\n",
    "        learn = tabular_learner(data, layers=[int(mp['layer1']),int(mp['layer2']),\n",
    "                           int(mp['layer3'])], \n",
    "                           ps=[mp['ps1'],mp['ps2'],mp['ps3']], emb_drop=mp['emb_drop'], \n",
    "                           y_range=y_range, metrics=metrics)\n",
    "        learn.fit_one_cycle(Epochs, mp['lr'], wd=mp['wd'], div_factor=int(mp['div_factor']))\n",
    "    \n",
    "    start=time.time()\n",
    "    y_pred=learn.get_preds(data.valid_ds)\n",
    "    PredTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    global EVv, MAEv, MSEv, RMSEv, R2v, EVtot\n",
    "    if verbose:print(\"Prediction Time: \", PredTime)\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation data set shape:\",data.valid_ds.inner_df.shape)\n",
    "    EVv=explained_variance_score(y_pred[1], y_pred[0])\n",
    "    if verbose:print(\"Validation Set Explained Variance (EV): %0.2f\" % (EVv))\n",
    "    #MAE is less sensitive to outliers, The constant value that minimizes the MAE is the median of the target values\n",
    "    MAEv=mean_absolute_error(y_pred[1], y_pred[0])\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAEv))\n",
    "    #MAE considers outliers, The constant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    MSEv=mean_squared_error(y_pred[1], y_pred[0])\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSEv))\n",
    "    RMSEv=np.sqrt(MSEv)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSEv))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    R2v=r2_score(y_pred[1], y_pred[0])\n",
    "    if verbose:print(\"Validation Set R2: %0.6f\" % (R2v))\n",
    "    EVtot=EVar.copy()\n",
    "    EVtot = np.append(EVtot,EVv)\n",
    "    EVtotMean=EVtot.mean()\n",
    "    if verbose:print(\"Total Mean EV: \",EVtot,EVtotMean)\n",
    "    MAEtot=MAEar.copy()\n",
    "    MAEtot=np.append(MAEtot,MAEv)\n",
    "    MAEtotMean=MAEtot.mean()\n",
    "    if verbose:print(\"Total Mean MAE: \",MAEtot,MAEtotMean)\n",
    "    \n",
    "    if verbose:print(\"Validation Set EV, XVal EV, Mean EV: \",EVv, EV, EVtotMean)\n",
    "    global d\n",
    "    d=\"\"\n",
    "    enable_progress()\n",
    "    #InsertHeader()\n",
    "    InsertValues()\n",
    "    return EVv, EV, EVtotMean, R2v, R2, -MAE, -MAEv, -MAEtotMean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T01:10:33.046590Z",
     "start_time": "2019-06-28T01:10:32.696570Z"
    }
   },
   "outputs": [],
   "source": [
    "enable_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:20:17.042225Z",
     "start_time": "2019-07-03T22:19:45.477420Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (5027, 18)\n",
      "842 837\n",
      "1679 837\n",
      "2516 837\n",
      "3353 837\n",
      "4190 837\n",
      "Measured Time for one fit:  6.018344402313232\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 30.023717\n",
      "EV: 0.490703\n",
      "EV for each Fold: [0.6253307461738586, 0.7475986480712891, 0.6870673894882202, 0.03943741321563721, 0.3540828227996826]\n",
      "MAE: 2046.738037\n",
      "MAE for each Fold: [1016.81555, 1190.5186, 902.9877, 3677.33, 3446.0386]\n",
      "MSE: 8135925.000000\n",
      "MSE for each Fold: [2085192.8, 2164606.5, 1539879.1, 18187932.0, 16702015.0]\n",
      "RMSE: 2501.547852\n",
      "RMSE for each Fold: [1444.0197 1471.2601 1240.9187 4264.731  4086.8098]\n",
      "XV R2 Actuals: [0.531320879931481, 0.5611093234491483, 0.6680983472764007, -2.524846057365191, -0.9577003015467311]\n",
      "Cross Validation R2: -0.344404\n",
      "XVR_fit_time Actuals:  [5.621321439743042, 5.608320951461792, 5.537316799163818, 5.534316539764404, 6.018344402313232]\n",
      "XVR_fit_time: 28.319620\n",
      "score_time Actuals:  [0, 0, 0, 0, 0]\n",
      "score_time: 0.000000\n",
      " \n",
      " \n",
      "Prediction Time:  1.2650723457336426\n",
      " \n",
      "Validation data set shape: (20404, 18)\n",
      "Validation Set Explained Variance (EV): 0.76\n",
      "MAE: 1068.172852\n",
      "MSE: 2242351.250000\n",
      "RMSE: 1497.448242\n",
      "Validation Set R2: 0.691935\n",
      "Total Mean EV:  [0.625331 0.747599 0.687067 0.039437 0.354083 0.761975] 0.5359153052171072\n",
      "Total Mean MAE:  [1016.81555 1190.5186   902.9877  3677.33    3446.0386  1068.1729 ] 1883.6439\n",
      "Validation Set EV, XVal EV, Mean EV:  0.7619748115539551 0.49070340394973755 0.5359153052171072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7619748115539551,\n",
       " 0.49070340394973755,\n",
       " 0.5359153052171072,\n",
       " 0.6919346360184233,\n",
       " -0.3444035616509784,\n",
       " -2046.738,\n",
       " -1068.1729,\n",
       " -1883.6439)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CrossEval3 test run 2 layers:\n",
    "Comments=\"Two layers\"\n",
    "\n",
    "#xc={'layer1': 1300.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
    "xc={'layer1': 1400.0, 'layer2': 1200.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, \n",
    "    'ps1': 0.11, 'ps2': 0.5, 'emb_drop': 0.04}\n",
    "CrossEval3(data, 5, mp=xc, verbose=True,\n",
    "           Comment=Comments,\n",
    "           metrics=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T21:14:52.938552Z",
     "start_time": "2019-06-29T20:58:54.500733Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Measured Time for one fit:  184.8225712776184\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 923.791838\n",
      "EV: 0.601464\n",
      "EV for each Fold: [0.757057249546051, 0.5592396259307861, 0.6504535675048828, 0.6429572105407715, 0.3976142406463623]\n",
      "MAE: 1235.511963\n",
      "MAE for each Fold: [969.6693, 1368.666, 1232.5692, 1234.8987, 1371.7561]\n",
      "MSE: 3356365.500000\n",
      "MSE for each Fold: [1595370.2, 3946769.8, 2475700.2, 2740140.2, 6023848.5]\n",
      "RMSE: 1786.570679\n",
      "RMSE for each Fold: [1263.0797 1986.6478 1573.4358 1655.3369 2454.353 ]\n",
      "XV R2 Actuals: [0.7401073187061312, 0.55760542342705, 0.6445095894860154, 0.6196864095473174, 0.39433742454898213]\n",
      "Cross Validation R2: 0.591249\n",
      "XVR_fit_time Actuals:  [163.9153757095337, 176.2500810623169, 174.87100195884705, 190.25988221168518, 184.8225712776184]\n",
      "XVR_fit_time: 890.118912\n",
      "score_time Actuals:  [0, 0, 0, 0, 0]\n",
      "score_time: 0.000000\n",
      " \n",
      " \n",
      "Prediction Time:  33.73892951011658\n",
      " \n",
      "Validation data set shape: (20404, 18)\n",
      "Validation Set Explained Variance (EV): 0.94\n",
      "MAE: 465.904541\n",
      "MSE: 431690.281250\n",
      "RMSE: 657.031433\n",
      "Validation Set R2: 0.939883\n",
      "Total Mean EV:  [0.757057 0.55924  0.650454 0.642957 0.397614 0.940302] 0.6579372584819794\n",
      "Total Mean MAE:  [ 969.6693  1368.666   1232.5692  1234.8987  1371.7561   465.90454] 1107.244\n",
      "Validation Set EV, XVal EV, Mean EV:  0.9403016567230225 0.6014643788337708 0.6579372584819794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9403016567230225,\n",
       " 0.6014643788337708,\n",
       " 0.6579372584819794,\n",
       " 0.9398827370984433,\n",
       " 0.5912492331430992,\n",
       " -1235.512,\n",
       " -465.90454,\n",
       " -1107.244)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CrossEval3 test run 3 layers:\n",
    "#Comments=\"Params found with 20% training data and param_dist = {'layer1':[1200, 1300, 1400], 'layer2':[500,1000,1100,1200], 'layer3':[500,1000,1100,1200], 'lr':[1e-4,3e-3,5e-3,8e-3,1e-2], 'wd':[0.1,0.2,0.3,0.4], 'div_factor':[24,25,26], 'ps1':[0.09,0.095,0.1], 'ps2':[0.5,0.49,0.48], 'ps3':[0.5,0.49,0.48], 'emb_drop':[0.04,0.039,0.38]\"\n",
    "Comments=\"20% of data, optimizing for XV-MAE\"\n",
    "\n",
    "#xc={'layer1': 1300.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
    "xc={'layer1': 1400.0, 'layer2': 1200.0, 'layer3': 1000.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
    "CrossEval3(data, 100, mp=xc, verbose=True,\n",
    "           Comment=Comments,\n",
    "           metrics=mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:29:56.621883Z",
     "start_time": "2019-06-29T14:29:46.230288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XeYVNX5wPHvu71XloUtsJSld5ZuBUE0RtCAYiyYmGgSNYkmsUR/GjWmWBMTSzQWNHZsWFEROyJLZ6lLW7bALlvZ3s7vj7kLA2xn7swy+36eZ56dOffce88dhnnnnnvue8QYg1JKKWUnH083QCmllPfTYKOUUsp2GmyUUkrZToONUkop22mwUUopZTsNNkoppWynwUYppZTtNNgopZSynQYbpZRStvPzdAO6ih49epiUlBRPN0MppU4qq1evPmiMiWurngYbS0pKCunp6Z5uhlJKnVREZG976mk3mlJKKdtpsFFKKWU7DTZKKaVsp8FGKaWU7TTYKKWUsp0GG6WUUrbTYKOUUsp2GmyUUqqbyiut4q53MyitqrN9X3pTp1JKdTMHyqp5bHkmL3+/D4Nh2oAenDUs3tZ9arBRSqluIr+smse/2MmLK7NobDTMT0vi2jMHkhQdYvu+NdgopVQ3sCmnlHlPfEtdg+FH4xK5fnoqyTH2B5kmGmyUUqob+DrzINV1jXx64+kM7Bnm9v3rAAGllOoGcoqriAz290igAZuDjYjcICIZIrJJRF4WkSAReU5EdovIOusxxqorIvKIiGSKyAYRGee0nYUissN6LHQqHy8iG611HhERscpjROQTq/4nIhJt53EqpVRXl1NSRWJUsMf2b1uwEZFE4NdAmjFmBOALLLAW/8EYM8Z6rLPKzgFSrcfVwOPWdmKAO4FJwETgTqfg8bhVt2m92Vb5LcAyY0wqsMx6rZRS3VZOcRUJ3hhsLH5AsIj4ASFAbit15wDPG4fvgCgR6Q2cDXxijCkyxhQDnwCzrWURxpgVxhgDPA/MddrWIuv5IqdypZTqlnJLqkiK9sJgY4zJAR4AsoA8oNQY87G1+F6rq+xhEQm0yhKBfU6byLbKWivPbqYcIN4Yk2e1Iw/o2VwbReRqEUkXkfSCgoJOHqlSSnVtpVV1HKqp99putGgcZxj9gAQgVEQuA24FhgATgBjg5qZVmtmM6UR5uxljnjTGpBlj0uLi2pzVVCmlTko5xVUAXtuNdhaw2xhTYIypA94Ephpj8qyushrgWRzXYcBxZpLstH4Sjm631sqTmikHOGB1s2H9zXfpkSml1Ekkt8QRbBK9sRsNR/fZZBEJsUaJzQC2OAUBwXEtZZNVfwlwhTUqbTKObrc8YCkwS0SirbOlWcBSa9khEZlsbesK4B2nbTWNWlvoVK6UUt1OTlOw8eCZjW03dRpjVorIYmANUA+sBZ4EPhSROBzdYOuAX1irfACcC2QClcBPrO0Uicg9wCqr3t3GmCLr+S+B54Bg4EPrAfA34DURuQpH0Jtv02EqpVSXl1NSRYCfD7GhAR5rg60ZBIwxd+IYtuxsegt1DXBtC8ueAZ5ppjwdGNFMeSGOMymllOr2mu6x8fFp7lK3e2gGAaWU8nI5xZ69oRM02CillNfLKakiISrIo23QYKOUUl6spr6BgkM1JEa5L8NzczTYKKWUF8srqQY8O+wZNNgopZRXaxr2rN1oSimlbNMUbJK0G00ppZRdcoqrEIFekXpmo5RSyiY5JVX0DA8kwM+zX/cabJRSyovlenjStCYabJRSyovllFSRGO3Z6zWgwUYppbxWY6Mhr6Ta4yPRQIONUkp5rYPlNdQ2NJKk3WhKKaXskt0F5rFposFGKaW8VFeYobOJBhullPJSXWHStCYabJRSykvlllQREeRHeJC/p5uiwUYppbxVTnFVl+hCAw02SinltXJKqkjqAoMDQIONUkp5rZwukj0ANNgopZRXKquu41B1vXajKaWUsk/TsOeucI8NaLBRSimvlNuFhj2DBhullPJKXekeG9Bgo5RSXimnuIoAXx96hAV6uimABhullPJKOSVVJEQF4eMjnm4KoMFGKaW8kiPYdI0uNNBgo5RSXimnuOvcYwMabJRSyuvU1DeQf6imywx7Bg02SinldfaXVgNdZyQaaLBRSimvc/iGTg02Siml7JLThWbobKLBRimlvExmQTkAvSKDPNySIzTYKKWUF8kqrOT5b/dy1tCeBPr5ero5h2mwUUopL2GM4eY3NuDnI9wzd4Snm3MUDTZKKeUlXv5+Hyt2FfLHHwyld2TXuV4DGmyUUsor5JZU8ZcPtjBtYCwLJiR7ujnH0WCjlFInOWMMf3xrIw2Nhr9dOAqRrpEPzZkGG6WUOsm9uSaHz7cVcNPswSTHhHi6Oc2yNdiIyA0ikiEim0TkZREJEpF+IrJSRHaIyKsiEmDVDbReZ1rLU5y2c6tVvk1EznYqn22VZYrILU7lze5DKaW8Tf6hau5+bzNpfaNZOCXF081pkW3BRkQSgV8DacaYEYAvsAD4O/CwMSYVKAausla5Cig2xgwEHrbqISLDrPWGA7OBx0TEV0R8gUeBc4BhwCVWXVrZh1JKeZU/Lcmgqq6Bv88b1WWmE2iO3d1ofkCwiPgBIUAeMB1YbC1fBMy1ns+xXmMtnyGOjsc5wCvGmBpjzG4gE5hoPTKNMbuMMbXAK8Aca52W9qGUUl5j+bZ8Pti4n19PH8iAuDBPN6dVtgUbY0wO8ACQhSPIlAKrgRJjTL1VLRtItJ4nAvusdeut+rHO5ces01J5bCv7OIqIXC0i6SKSXlBQ0PmDVUopN6uua+DOdzLoHxfKz0/r7+nmtMnObrRoHGcl/YAEIBRHl9exTNMqLSxzVfnxhcY8aYxJM8akxcXFNVdFKaW6pMc/30lWUSX3zBnRpTIFtMTObrSzgN3GmAJjTB3wJjAViLK61QCSgFzreTaQDGAtjwSKnMuPWael8oOt7EMppU56uw9W8PgXOzl/dALTBvbwdHPaxc5gkwVMFpEQ6zrKDGAzsByYZ9VZCLxjPV9ivcZa/pkxxljlC6zRav2AVOB7YBWQao08C8AxiGCJtU5L+1BKqZOaMYY7l2QQ4OvD7T8Y6unmtJud12xW4rhIvwbYaO3rSeBm4EYRycRxfeVpa5WngVir/EbgFms7GcBrOALVR8C1xpgG65rMdcBSYAvwmlWXVvahlFIntQ837efL7QX8btYgekZ0nazObRHHiYBKS0sz6enpnm6GUkq1qLymnrMe/IKY0ACWXDcNP1/P35cvIquNMWlt1fNrq4JSSinPq2to5N73t7C/rJrHLhvXJQJNR2iwUUqpLswYw/Jt+dz7/hZ2FlTwk2kpjOsT7elmdZgGG6WU6qK27T/En9/fzFc7DtK/RyhPL0xj+pCenm5Wp2iwUUqpLsYYwz3vbeG5b3cTHuTPHecN47LJfQnwO7m6zpxpsFFKqS7m0y35PPPNbi5OS+bWc4cQFXLy5xLWYKOUUl1IQ6PhgaXb6NcjlHsvGHHSDQRoiXcchVJKeYkl63PYduAQN84c5DWBBjTYKKVUl1Fb38hDn2xnWO8IfjCyt6eb41IabJRSqot4dVUW+4qq+MPswV16bprO0GCjlFJdQGVtPY98lsnElBjOGOR9Weg12CilVBfw3Ld7KDhUw02zB+PIXexdNNgopZSHlVbW8cTnO5k+pCdpKTGebo4tNNgopZSH/efLnZRV1/P7WYM93RTbaLBRSikP2rb/EM9+s4fzRycwLCHC082xjQYbpZTykI3ZpSx4cgXhQX784WzvPasBDTZKKeUR6XuK+PFT3xES4Mfrv5hCckyIp5tkK01Xo5RSbvZN5kF+tiidXpFBvPizSSREBXu6SbbTYKOUUm702dYD/OJ/a+gXG8oLP5tIz/CTZ2rnE6HBRiml3GRLXhlXP7+aob0jeP6nE4kOPfmzObeXXrNRSik3Wb+vhPpGw78uGdutAg1osFFKKbcpqqwFID6ie3SdOdNgo5RSblJUXkuwvy/BAb6eborbabBRSik3KaqoJaabdZ810WCjlFJuUlSpwUYppZTN9MxGKaWU7QrLa4nVYKOUUspOemajlFLKVlW1DVTVNXS7+2uaaLBRSik3aLrHRrvRWiEiA0Qk0Hp+hoj8WkSi7G2aUkp5j6JyR7DRbrTWvQE0iMhA4GmgH/CSba1SSikv03Rmo8GmdY3GmHrgAuAfxpgbgN72NUsppbxLUUUNoMGmLXUicgmwEHjPKvO3p0lKKeV9CsubrtkEerglntHeYPMTYApwrzFmt4j0A/5nX7OUUsq7FFfW4usjRAR3z5ld2nXUxpjNwK8BRCQaCDfG/M3OhimllDcpqqglOiQAEfF0UzyivaPRPheRCBGJAdYDz4rIQ/Y2TSmlvEd3zh4A7e9GizTGlAEXAs8aY8YDZ9nXLKWU8i7dOXsAtD/Y+IlIb+AijgwQUEop1U7dOeMztD/Y3A0sBXYaY1aJSH9gR2sriMhgEVnn9CgTkd+KyJ9EJMep/FyndW4VkUwR2SYiZzuVz7bKMkXkFqfyfiKyUkR2iMirIhJglQdarzOt5Sntf0uUUsr19MymHYwxrxtjRhljfmm93mWM+VEb62wzxowxxowBxgOVwFvW4oeblhljPgAQkWHAAmA4MBt4TER8RcQXeBQ4BxgGXGLVBfi7ta1UoBi4yiq/Cig2xgwEHrbqKaWUR9Q3NFJSWafBpi0ikiQib4lIvogcEJE3RCSpA/uZgeOsaG8rdeYArxhjaowxu4FMYKL1yLQCXC3wCjBHHEM6pgOLrfUXAXOdtrXIer4YmCHddQiIUsrjSqrqAIgN02DTlmeBJUACkAi8a5W11wLgZafX14nIBhF5xhpKjbXdfU51sq2ylspjgRIrs4Fz+VHbspaXWvWPIiJXi0i6iKQXFBR04HCUUqr9iiocN3RGh2iwaUucMeZZY0y99XgOiGvPitZ1lPOB162ix4EBwBggD3iwqWozq5tOlLe2raMLjHnSGJNmjEmLi2vX4SilVIcdyR6gwaYtB0XksqZrKCJyGVDYznXPAdYYYw4AGGMOGGMajDGNwFM4usnAcWaS7LReEpDbSvlBIEpE/I4pP2pb1vJIoKid7VVKKZdqOrOJ0W60Nv0Ux7Dn/TjORubhSGHTHpfg1IVmDaFucgGwyXq+BFhgjSTrB6QC3wOrgFRr5FkAji65JcYYAyy32gKOvG3vOG1rofV8HvCZVV8ppdzucMbnbtyN1t50NVk4usIOE5HfAv9obT0RCQFmAtc4Fd8nImNwdGvtaVpmjMkQkdeAzUA9cK0xpsHaznU4hl77As8YYzKsbd0MvCIifwbW4pj+AOvvCyKSieOMZkF7jlMppezQNJdNd52lE9oZbFpwI20EG2NMJcdcmDfGXN5K/XuBe5sp/wD4oJnyXRzphnMurwbmt9Y2pZRyl6KKGiKC/PD37b6TI5/IketQYuA/X+zkime+93QzlFJdWFFlHbFh3XNqgSYnEmz0GghQXdfIVzsKKK+pb7uyUqpbKqqoITqke08B1mqwEZFDVpqZYx+HcNxz0+2NSorEGNicW+bppiiluqjC8lpiuumkaU1aDTbGmHBjTEQzj3BjTPecAegYIxIjAdiQXeLhliiluqriyu49vQCcWDeaAuLCA+kdGcTGnFJPN0Up1QUZYxwTp2mwUSdqZGIkG7M12Ciljneopp66BqNnNp5ugDcYmRjJroMVHKqu83RTlFJdTNM9Nt054zNosHGJkUmO6zabcnSQgFLqaIezB3TjVDWgwcYlRlqDBDbm6CABpdTRDp/ZdONUNaDBxiViwwJJjApmo57ZKKWOcTgJp3ajKVdwDBLQMxul1NGautG688RpoMHGZUYmRbKnsJLSKh0koJQ6oqiilkA/H4L9fT3dFI/SYOMiTddtMvR+G6WUk8Jyxw2d3X1meg02LtIUbDZosFFKOSmurO32I9FAg43LRIcGkBwTrDd3KqWOUlihedFAg41LjUyM9Hjams+2HuDi/6ygpr7Bo+1QSjkUVdQQ080zPoMGG5camRhFVlElJdboE094b0MeK3cXsXxrgcfaoJQ6okgzPgMabFxqVFLTzZ2eO7vZYHXjvb02x2NtUEo5VNc1UFHb0O2HPYMGG5cakdA03YBngk15TT07C8oJCfDls635lFbqMGylPKnY6uWI7ubZA0CDjUtFhvjTNzaETR46s9mYXYox8KszBlDb0Mh7G3M90g6llEOhJuE8TIONi41MjPTYmU3TBG6XTOxDas8w7UpTysOKNXvAYRpsXGxkYiQ5JVWH8yG504bsUhKjgokNC2Tu2ERW7SlmX1Gl29uhlHLQvGhHaLBxsZEeHCSwPruE0cmO/c8dmwjAW3p2o5THFGrG58M02LjYiKbpBtyclLOwvIbs4ipGJUUBkBgVzKR+Mby9NgdjjFvbopRyKKqoxddHiAzW+2w02LhYRJA//XuEuv26TVOanKbh1wAXjktk18EK1mtWA6U8oqiylugQf3x8undeNNBgY4sRiZFuH5G2MbsUkSM52gDOGdmbAD8f3lqT7da2KKUcisprddizRYONDUYnR5FbWs2egxVu2+eG7BL69wglPOjI6XpEkD8zh8bz7oY86hoa3dYWpZRDUUWtDg6waLCxwXmjeuPrI7z8fZZb9meMYX12KaOt6zXOLhibSFFFLV9u1/Q1XUFjo14/606KKmt12LNFg40N4iOCmDk0ntfS97klIeb+smoKDtUcdb2myemD44gO8ddRaV1A+p4iht35EVvydPrw7kLPbI7QYGOTyyb3pbiyjg837rd9X+v3WYMDko8/s/H39eGHoxP4ZPMByqo1fY0nvbs+l+q6RhZ9u8fTTVFu0NBoHHPZ6DUbQIONbaYOiCUlNoQXV+61fV8bskvw8xGG9Y5odvmF45KoqW/kgw15trdFNc8Yw7Kt+QC8sy5XA383UFpVhzF6Q2cTDTY28fERLp3Ul1V7itm6395ukw3ZpQzuFU5QC3Ocj06KZGDPMF5fraPSPGVHfjnZxVVcMrEPVXUNvKn/Fl7lwY+3MfOhL46aPLGoogaAmDCdXgA02Nhq3vgkAvx8ePE7+wYKGGPYkF1y+GbO5ogI88cnsXpvMTsLym1ry8lgV0E5Mx/6gh0HDrl1v8u2OM5qfjMjldHJUfxvZZbebOtFPtl8gB355fzo8W95fsUejDGaPeAYGmxsFB0awHkje/PW2hwqaupt2ceewkrKqusZ3czgAGcXjE3E10d4o5v/on4tPZsd+eU8ujzTrfv9bOsBhidE0CsyiMsm9SEzv5zvdhW5tQ3KHtV1DezIL+fHk/owbWAsd7yTwXUvryXLykuo3WgOGmxsdunkPpTX1PPOOnvS/Tdlem7tzAagZ0QQpw+K4801OTR44fDb6roG9pdWt1rHGMO763PxEXh3Qx7Zxe5JUlpSWcvqvcVMH9ITgB+OTiAy2J//fWf/9bzWGGNYvi1fpxA/QdsPHKKh0XDqwB48vXACN80ezEeb9vPHtzYCmvG5iQYbm43rE82QXuG8uHKvLd0m6/eVEuTvw6D4sDbrzh+fxP6yar7a4X333Fz/8lpmPfwF5a2cQa7JKianpIrfzRqMAM98vcctbftiewGNhsPBJsjfl/njk1iasZ/8stYDpJ3W7ivhJ8+u4m8fbvVYG7zBphzHNdkRiZH4+Ai/OmMgL/1sEtEhAQT6+WgGAYsGG5uJCJdO7ktGbhnr9rk+OeeG7BKGJ0Ti59v2P+X0oT2JCvFnsZd1pX2csd8a2l3famqeJetyCfTzYeHUFM4fncArq7IoqbR/KohlW/KJDQ046qbbSyf3pb7R8OqqfbbvvyVNF7Of+3YPK3cVeqwdJ7tNuaVEBPmRFB18uGxS/1iW/vY03vrVNAL89GsWNNi4xQVjEwkN8OXFla4dKFDf0Mim3NJmb+ZsTqCfL3PHJPLx5gNeM2V0ZW09d727mUHxYYxMjGTRiubPIOsbGnl/Yx4zhvYkLNCPn5/Wn8rahg51ZVXXNXDVc6u49c0N7V6nvqGRz7flc+aQnkclY+zXI5RTU3vw0vdZ1HsolVBGbilRIf4kR4fwh8UbqKy157qit8vILWN4QiQiRyfbjA4NYFhC87cjdEe2BRsRGSwi65weZSLyWxGJEZFPRGSH9Tfaqi8i8oiIZIrIBhEZ57SthVb9HSKy0Kl8vIhstNZ5RKx/7Zb24SlhgX7MGZvIu+tzXfpLekd+OdV1jc2mqWnJvPFJ1NY3smS9d2QU+OeyHeSUVHHvBSO5cmoKmfnlfLvz+F/pK3YVcrC8lvNHJwAwtHcEpw+K47lv91Bd1/Y1i/qGRq57aQ3Ltubz8vf7+K6dZwKr9xZTVl3PDKsLzdmlk/qSV1rNZ9b9N+6WkVvGyMRI7p83iqyiSv6u3WkdVtfQyJa8MkYkalBpi23BxhizzRgzxhgzBhgPVAJvAbcAy4wxqcAy6zXAOUCq9bgaeBwcgQO4E5gETATudAoej1t1m9abbZW3tA+PuXRSH2rqG106VfORwQHtO7MBGJ4QwZBe4V7RlbZt/yGe/mo388cnMSElhh+M6k1MaADPNXOH/rvrcwkL9OOMwUe+9K85vT8Hy2t5o42s2I2Nhpvf2MinW/K5/QdDSYgM4s/vb25XnrPPtubj7yucktrjuGVnDe1Jr4gg/ufiM972qK1vZPuBQwxPiGRS/1h+Mi2FRSv2sqKZQK1atrOgnNr6RoYntP//YHflrm60GcBOY8xeYA6wyCpfBMy1ns8BnjcO3wFRItIbOBv4xBhTZIwpBj4BZlvLIowxK4yj3+T5Y7bV3D48ZnhCJCMSI3gt3XVf8uv2lRAe5EdKbGi71xER5qclsz67lO1uvtfElRobDbe/vZGwID9uPXco4LjwfsnEZJZtOXDUdNg19Q18uGk/s4bHH3Xj65T+sYxKiuS/X+1ucYSeMYZ7P9jCG2uyuXHmIH52an9umj2ETTll7co3t2xrPpP6xR6VjbuJn68Pl0zsw5fbC9yaIRxgR/4h6hoMw61unpvOHkJKbAh/WLzetmH63ijj8OAAPbNpi7uCzQLgZet5vDEmD8D62/RTMxFwvlqabZW1Vp7dTHlr+ziKiFwtIukikl5QYP8IrYvSktmcV+aSuW6MMXyxrYAp/WM7PDHT3DEJ+PnISX12s3hNNqv2FHPrOUOOuo/h0kl9ERH+55Qm6IttBRyqrj/chdZERLjmtAHsPljBJ5ubz2H32Oc7efrr3Vw5NYXrpw8E4PzRCYxOiuT+pdtavc6RVVhJZn754VFozVkwMRk/H3FLWiNnGbmOL8mmYBMc4MsD80eTU1LFXz/c4ta2nMw25ZYS7O9Lvx5tjwbt7mwPNiISAJwPvN5W1WbKTCfK280Y86QxJs0YkxYXF9eRVTtlzuhEAvx8eC39xEcgbc4rI7e0mrOGxnd43diwQKYP6cmba3JOynluiitq+esHWxjfN5r545OPWpYQFcysYfG8umrf4WsxS9bnEhMawLSBx3dlzR7Riz4xITz+xS6MMRhjOFhew7p9Jfz7sx3cv3Qbc8ckcMd5ww5fAPbxEW4/bxj7y6p56svdLbbzs60HAJgxtOVgEx8RxNnDe/FaejZVte673yUjp5TQAN+jzorTUmL42Sn9+N93WXyTedBtbTmZZeSUMbR3OL46E2eb3HFmcw6wxhhzwHp9wOoCw/rbdHU0G3D+5kgCctsoT2qmvLV9eFRkiD+zh/fi7bU57boo3ZplW/IRgTNb+dXcmvlpyRwsr+EVN82540r3Ld1KWXU9914wotmzuiumpFBSWceSdblU1NTz6ZYDnDuyF/7NDA/39RF+fmo/1u8rYcaDXzDsjqWk/flT5j76DQ98vJ0zB8dx//zRx+1nQkoM547sxRNf7ORAC/fKLNuaz4C4UPq20c15+ZS+lFbV8e56e278bU5GbhlDe0ccd1y/mzWYfj1Cue2tjSf8GfV2jY2GzXlljEjU6zXt4Y5gcwlHutAAlgBNI8oWAu84lV9hjUqbDJRaXWBLgVkiEm0NDJgFLLWWHRKRydYotCuO2VZz+/C4iyckU1Zdz9KME5t64NMtBxiTHEVceOeS/E0f0pPTBsVx55IMPj7BtrjTun0lvLJqHz+dlsKQXs33k0/uH8Pg+HCe+3YPn245QHVdIz8cldBsXYB545M5e3g8qfFh/HhSH+784TCeuiKND39zKk8vnNBskAK4efYQGhoNDyzddtyy8pp6Vu4qarULrcmkfjEMig/j+e/2uCVfWmOjYUte2eEuNGdB/r7cM2cEeworeczNKX1ONnuLKimvqWeEDg5oF1uDjYiEADOBN52K/wbMFJEd1rK/WeUfALuATOAp4FcAxpgi4B5glfW42yoD+CXwX2udncCHbezD46b0jyUxKpjXT2CgwIGyajZkl3aqC62Jr4/w+KXjGJUUxXUvrz0pbuprbDTc8c4m4sIC+fWM1BbriQgLp6awOa+MBz/eTu/IICakxLRYPzjAl/9cnsZ/Lk/j/84bxk+m9WPmsPhmf/k76xsbypXTUli8JptNOaVU1NSzfl8Ji1dnc8c7m6htaGT6kLb/jUSEyyf3ZVNOx2/8zS+rJjO/YwM99hRWUFHb0OIIqlNSezB3TAKPf7GTzPzunbi1NU3XXvVemvaxNdgYYyqNMbHGmFKnskJjzAxjTKr1t8gqN8aYa40xA4wxI40x6U7rPGOMGWg9nnUqTzfGjLDWuc4aldbiProCHx9hfloSX2cePGrEVEc03ZfR2rWA9ggN9OPZKyfQJyaEny1KZ3Nu155B8tX0fWzILuWP5w5tdnSXs7ljE4gI8iOrqJLzRvXu8CCK9rr2zIFEBfsz/4kVDL9zKXMe/Ybfv76e99bnMaV/LGkp7bvF64JxSYQF+vHCivYPFDDG8PPn0/nhv75hdwdGszUNDmjtS/K2Hwwj2N+X29/eqNmpW5CRW4a/rzAoPtzTTTkpaAYBD5g3PgkROj0a7NPNB0iKDmawCz7k0aEBPP/TiYQF+XHFM9+TVeie5JQdVVJZy30fbWVivxjmjGm5S6xJSIAfF6XEo5xgAAAZqUlEQVQ5LvWdPzqxjdqdFxnsz33zRnP28Hh+N3MQT1w2nmW/O53Nd5/Ny1dPbrEL7lhhgX5cOC6R9zbkUVhe06511u4rYX12KVV1Dfz2lbXtHuzRni/JuPBAbjlnKN/tKuLNNd5xA7CrZeQ65pHSdDTto++SByRFh3DKwB4sXp3d4QzMVbUNfJ15kLOGxh+XHqOzEqKCeeGqidQ3NnLZ0yspONS+Lzt3euDjbZRV13PX+cPbfdy/nTmIp65IY2QHbnrtjJnD4vnHgrFcPyOV2SN6MSAurF256o51+eS+1DY08mo7Rysu+nYP4YF+3D9vFOuzS/nXsh3tWi8jt5TUnm1/SS6YkMz4vtHc+8EWiivszyF3MjHGsCmnlOG99XpNe2mw8ZCL0pLJKani250dG2L6TeZBauobT7gL7VgDe4bz7JUT2F9azeOf73Tptk/UppxSXlyZxeWT+zK0hamvmxMW6MfMYZ2/ruVuqfHhTO4fw4vfZbX5I+RAWTXvb8hjfloy89OSmTc+iX8vzyR9T+s9xsYYNue2L72Kj49w7wUjKK2q08zQx8gtraa4sk5v5uwADTYeMnNYPJHB/h3OKPDplgOEBfoxqV+sy9s0tk80k/rHdKkpCJoGBcSGBnDDzEGebo7trpiSQk5JVZv50l5cmUWDMVwxpS8Ad/5wGInRwdzw2joOVbecZPVAWQ2FFbXtTq8ypFcEPzulH6+m7+P73V3m0qfHZViDA4brsOd202DjIUH+vswdk8DSjP3tTs7Z2GhYtjWf0wfF2dZPfGpqD3bkl5NXWmXL9jvqjTXZrMkq4ebZQ4gMbn1QgDeYOSye+IhAXmglG3VNfQMvrczizME9SenhuIcnPMiff1w8hpziKv60ZHOL62bkWl+SHRhB9ZuzUkmMCubmNzyXGbqh0bD9wCFeS9/H7W9vZM6/v+bXL6/lYDuvb7naptwyfASGtjD8Xh1Pg40HXTQhmdr6Rt5o5wXYjTmlFByqcXkXmrNTUx2ZFL7a0TXuIH90eSZjkqP40biktit7AX9fH348sS9fbi9ocYTZBxvzOFhew5VTU44qH983huvOHMgba7J5f0Nes+tm5JYhQoe6I0MC/Lh//ih2H6zgrx+4vzvt5sUbGPWnpcx6+EtuWryBt9fmEujvy0cZ+zn74S89cp9YRk4pA+LCCA7wbbuyAjTYeNTwhEjG943muW9bTgTp7NMtB/AROHOwfcFmSK9weoQFdolgU13XwN6iSs4YHGfb0OWu6JKJyfj7Cre8seG4mUeNMTz7zR4GxDnmwznW9TNSGZ0cxR/f2tjsqLaM3FL6xYYSGujXoTZNHdCDq07pxwvf7eWL7e7rZi2uqOXV9H2M6xvNg/NH8+mNp7Phzlm8ds0U3rv+FHpFBnH1C6v5/evrW+0+dLWMXM0c0FEabDzsqlP6sa+oik+3HGiz7qdb8knrG0N0qH3TzIoIp6X24JvMg+1KoW+n7OJKjKFDWa29Qc+IIO6fN5r0vcVc+tR3R40EW7uvhA3ZpSycmtLsqDx/Xx8emDeKipp67vvo+MwGGbllnb4J8Q9nDya1Zxg3LV7vlhlOwTEfEDjuZ/rR+CQG9gw7/MNjUHw4b/1qGtedOZA312Qz+x9f8dqqfby1NptXV2Xxwoo9/PerXbzyfRZlLgxEBYdq2F9W3aGuSKXBxuNmDYsnMSqYp79uOaEjQE5JFVvyymztQmty6qAeFFXUsjnPszd57rXu+ekTG+LRdnjC3LGJPHHZeLbsP8TFT644nH+tabjzha10K6bGh3OVdVF/TVbx4fKSylqyi6s6PfdKkL8vD188hsLyWv7vnYxObaOj0vcW4+cjLU4QGODnw+/PHsziX07F31e46Y0N3PDqem5+YyP/904Gf35/C7e8uZGpf/2Me97b3OkbqZ0due6lZzYdocHGw/x8fbhyagrf7y5qdeqBZVuaMgjbP5S3KTvylx4elbbHCjZ9Y7pfsAHHYIHnfjKBnOIq5j+xgtV7iw4Pdw5roxvs+hmp9IoI4v/e3nS4i3bzMdMKdMaIxEh+MyOVd9fnssQNiUNX7y1ieGJkm9dGxvWJZukNp/HxDaex/Pdn8M0t01l121msv3MW7153CjOHxbPo2z2cfv9yfvXi6qOCcEe1JwODOp4Gmy7gognJhAT48kwLZzcNjYa31+bQr0coA+Ls71LqGR7EkF7hfLXds9dtsgorCA/0O2q+mu5m6oAevPjzyZRV1zH/iRVHDXduTVigH7efN5SM3DJesubKOXYOm8765RkDGNsnitvf2sj+0uYzXrtCTX0D67NLSevbvpQ/gX6+DIoPp1+PUBKjgokLDyQy2J+RSZE8fPEYvr55OtecPoCvdxzkwse+5bHPO5dodGN2KX1iQrrF6EhX0mDTBUQG+3NRWjLvbsglv5l09Q9/sp01WSVcc1p/l2UNaMtpg+JI31vksaGu4Miq2yc2xG3H3FWNSY7i1aunEBceyDkjeh0e7tyWH4zszbSBsdy/dBsHy2vIyC2lV0QQsWGdyxTexM/Xh4cuGkNdg2HOo19zzQvpPPTxNt7bkEtm/iHqXTRH0qacMmrrG5nQzvxybekVGcTNs4ew4tYZzBmTwH0fbeP+pVvbnfutrqGRP7+3mY8y9jNtoOvvc/N2HRuSomxz5dQUFq3Ywwvf7eV3swYfLv9oUx7/Xp7JggnJXDwhueUNuNipqT148stdrNxdZOvot9bsLaxkWAeG6Hqzwb3C+fKmM5Fm5wxsnohw1/kjOOefX/L3D7daI6hc83726xHKfy4fzyursti2/xCfbD5A03iS6BB/fjKtHwunppzQr//Vex03kY7v23LG7s4IDfTjoYvGEBLgy6PLd1JR08Ad5w1rdcRjTkkV1720hrVZJSyc0pc//mCoS9vUHWiw6SJSeoQyY0g8L67M4tozBxLk78v2A4e48bX1jEmO4q457c8J5goTUmII9PPhq+0HPRJs6hsayS6uZPaIXm7fd1cV6NfxezoG9gzjqlP688QXOxGBc0b2dll7ThsUx2mDHPdlVdc1sLOgnG37D/HBxjwe+mQ7T365i8un9OWqU/rRoxNnU+l7iukbG9LpOZta4+sj/OWCkYQE+PH017uprK3nrxeOanbGzeVb87nhtXXUNxj+/eOxnNfK3EiqZRpsupCfnpLCp08d4O21OZwzsjdXP59OaKAfT1w2vlNfNCciyN+Xif08l7omr7SaugbTbQcHuNL10wfyzroc8krtG64b5O/L8IRIhidEcuG4JDbnlvHo55k88cVOnv1mN5dP7stNs4e0Owu2MYbVe4s5fbB907WLCLf/YCihgX48smwHFTUNnD8mgfLqeg5V11FeU8/ewkpeX53N0N4RPHbpOPq1swtTHU+DTRcypX8sQ3tH8Mw3u1masZ+ckipe/vlkekUGeaQ9p6b24C8fbCWvtIrekcFu3XfTsOe2plRWbQsN9OPuOSO47a2NjOvjmusfbRmWEMGjPx5HZn45j32eyVNf7aa2vpG75oxo1/p7CisprKglzcVdaMcSEW6cOYjQAF/++uFW3t94dOaFQD8ffjypD3ecN4wgf80WcCI02HQhIsJPp6Xwh8Ub2H6gnD/PHUFaKzNM2s2RumYrX+84yPw0910vAthb5EjV0rcb3mNjh5nD4j2SAXtgzzAeumgMsaEBPPXVblLjw7lsctuj6VZZ2avbO/ncibrm9AHMGBpPdV0DEUH+hAX5ERbop3PVuJC+k13M+WMSSO0ZxsIpfbl0Uh+PtsWTqWv2FlYS4OdDrwjPnNUp17rlnKGcMTiOPy3JYMXOtqcgX72nmIggPwbGhbmhdQ4De4YxIjGSPrEhxIQGaKBxMX03u5hAP1+W/vY07pozwuNDfptS13ztgdQ1ewsr6BMT0q1yonkzXx/hkUvG0jc2hF++uLrNGWHT9xYxvm+0/vt7EQ02XVBX+g92SqpnUtfsLazUwQFeJiLIn6cXTsAYuGrRqhYTZxZX1LKzoMKjXcjK9TTYqFadYqWucWdXmjGGrKJKHRzghVJ6hPL4pePYdbCC37yyrtls503JN9ubOUCdHDTYqFb1jAhidFIkz6/YQ2mVe1K4F5TXUFnboIMDvNTUgT340w+H8dnWfB5ZtuO45el7i/H3FUYnN598U52cNNioNt01ZwT5h2q4+92WZ4B0paxunO25u7hscl8uHJfII5/t4JvMo8+aV+8tYnhCpA419jIabFSbxiRH8aszBvDGmmy3zIrYlO25u81j052ICH+eO4IBcWH85pW1h3MCdjT5pjp5aLBR7XL99FSG9o7gj29tpKjC3omzsgor8BFIjHLvjaTKvUIC/Hjs0nFU1DTw61fWUt/QeDj5prvur1Huo8FGtUuAnw8PXTSa0qo6bn97Y7sz5XbGnsJKEqKC9T6HbmBQfDj3zB3Bd7uK+OeyHaTvsSf5pvI8/d+s2m1o7whumDmIDzbut3XirL1FldqF1o3MG5/E/PFJ/Ht5Ji99n2Vb8k3lWRpsVIdcfWp/xvaJ4o53Mg5PVexqWYUVOjigm7l7zggG9Qxnb2El4/V6jVfSYKM6xM/Xhwfnj6amvoFfvbiGvNIql26/tKqO4so6UjTYdCvBAb48euk4eoQFMNMNU58r99Ngozqsf1wY980bzebcMmY9/CVvrsl22TWcw8OeY7QbrbsZ2DOMVbed5dI5d1TXocFGdcr5oxP48DenMjg+nBtfW881L6zmYHnNCW9Xsz13b57OB6jso8FGdVpKj1BevWYKfzx3CJ9vL2DWw1+y9ATvwzkyj40GG6W8iQYbdUJ8fYSrTxvA+9efQmJUML/832q2nEDSzr2FFcSFBxISoFMtKeVNNNgol0iND+eFqyYSGezPnUsyOn0NR7M9K+WdNNgol4kKCeCm2UP4fndRp+/D2Vuo2Z6V8kYabJRLXZSWzKikSP7ywRbKa+o7tG51XQP7y6r1eo1SXkiDjXIpXx/hrvOHc6Cshn81kz6+NVlFOjhAKW9la7ARkSgRWSwiW0Vki4hMEZE/iUiOiKyzHuc61b9VRDJFZJuInO1UPtsqyxSRW5zK+4nIShHZISKvikiAVR5ovc60lqfYeZzqaGP7RHNxWjJPf72bzPzy45bnllTx2OeZFBw6eqj0kZFo2o2mlLex+8zmn8BHxpghwGhgi1X+sDFmjPX4AEBEhgELgOHAbOAxEfEVEV/gUeAcYBhwiVUX4O/WtlKBYuAqq/wqoNgYMxB42Kqn3Oim2YMJCfDlT06DBcpr6rl/6VbOfOBz7vtoG/Oe+Paouej3Flr32OgAAaW8jm3BRkQigNOApwGMMbXGmJJWVpkDvGKMqTHG7AYygYnWI9MYs8sYUwu8AswRx91f04HF1vqLgLlO21pkPV8MzBC9W8ytYsMC+d2swXydeZD3N+bx4sq9nHH/ch5dvpPZI3rxxGXjKa2q40dPfMvmXMdQ6b2FlUQE+REV4u/h1iulXM3OM5v+QAHwrIisFZH/ikhT/8h1IrJBRJ4Rkaase4nAPqf1s62ylspjgRJjTP0x5Udty1peatVXbnTppD4M6RXOdS+t5ba3NtGvRyhvXzuNfy4Yy+wRvXj9min4+QgX/2cFK3cVsrfIMRJNfxco5X3sDDZ+wDjgcWPMWKACuAV4HBgAjAHygAet+s19w5hOlLe2raOIyNUiki4i6QUFBa0ciuoMP18f/v6jUUwdEMsTl43ntWumMMZpXvnU+HAW/3IqPSMCufyZ71mbVazZnpXyUnYGm2wg2xiz0nq9GBhnjDlgjGkwxjQCT+HoJmuqn+y0fhKQ20r5QSBKRPyOKT9qW9bySKDo2AYaY540xqQZY9Li4uJO6GBV80YnR/HSzycze0SvZs9YEqOCef0XUxnaO4JD1fWa7VkpL2VbsDHG7Af2ichgq2gGsFlEnFO6XgBssp4vARZYI8n6AanA98AqINUaeRaAYxDBEuO46rwcmGetvxB4x2lbC63n84DPjJ1TS6oTEhMawEs/m8TVp/VnzpjEtldQSp107E5AdT3wohUkdgE/AR4RkTE4urX2ANcAGGMyROQ1YDNQD1xrjGkAEJHrgKWAL/CMMSbD2v7NwCsi8mdgLdZgBOvvCyKSieOMZoHNx6lOUGigH388d6inm6GUsonoD36HtLQ0k56e7ulmKKXUSUVEVhtj0tqqpxkElFJK2U6DjVJKKdtpsFFKKWU7DTZKKaVsp8FGKaWU7TTYKKWUsp0GG6WUUrbT+2wsIlIA7MWR2qa0hWrNLTu2rLXXzs974Ei54wqttbkz9Vta3p7jP7astffDU+9BZ4+/pWX6GdDPQHf+DPQ1xrSd78sYow+nB/BkR5YdW9ba62Oep7ujzZ2p39Ly9hx/a8fcVd6Dzh6/fgb0M6CfgY7vp+mh3WjHe7eDy44ta+11a9s+ER3dblv1W1renuM/tqyt98dVOrLdzh5/S8v0M9B6mX4GvP8z0CbtRvMQEUk37Ujx4M26+3vQ3Y8f9D3oTsevZzae86SnG9AFdPf3oLsfP+h70G2OX89slFJK2U7PbJRSStlOg40LiMgzIpIvIpvarn3cuuNFZKOIZIrII+I0naWIXC8i20QkQ0Tuc22rXceO4xeRP4lIjoissx7nur7lrmPXZ8Ba/nsRMSLSw3Utdi2bPgP3iMgG69//YxFJcH3LXcem9+B+EdlqvQ9viUhUW9vqqjTYuMZzwOxOrvs4cDWOmUlTm7YjImcCc4BRxpjhwAMn3kzbPIeLj9/ysDFmjPX44MSaaLvnsOE9EJFkYCaQdYLts9tzuP747zfGjDLGjAHeA+440Uba7Dlc/x58AowwxowCtgO3nmAbPUaDjQsYY77EMSPoYSIyQEQ+EpHVIvKViAw5dj1riuwIY8wK47h49jww11r8S+Bvxpgaax/59h5F59l0/CcVG9+Dh4GbcMxs22XZcfzGmDKnqqF0z/fgY2NMvVX1OyDJ3qOwjwYb+zwJXG+MGQ/8HnismTqJQLbT62yrDGAQcKqIrBSRL0Rkgq2tdb0TPX6A66zug2dEJNq+ptrmhN4DETkfyDHGrLe7oTY54c+AiNwrIvuAS+n6ZzbNccX/gyY/BT50eQvdxM/TDfBGIhIGTAVed+p+D2yuajNlTb/e/IBoYDIwAXhNRPqbk2D4oIuO/3HgHuv1PcCDOP6znRRO9D0QkRDgNmCWPS20l4s+AxhjbgNuE5FbgeuAO13cVNu46j2wtnUbUA+86Mo2upMGG3v4ACVWX/NhIuILrLZeLsHxhep8WpwE5FrPs4E3reDyvYg04sijVGBnw13khI/fGHPAab2ncPTZn0xO9D0YAPQD1ltfVEnAGhGZaIzZb3PbXcEV/wecvQS8z0kUbHDReyAiC4HzgBknw4/NFrkqL093fwApwCan198C863nAoxuYb1VOM5eBMcp8rlW+S+Au63ng4B9WPdFdcWHDcff26nODcArnj5Gd78Hx9TZA/Tw9DG6+TOQ6lTnemCxp4/RA+/BbGAzEOfpYzvh98bTDfCGB/AykAfU4TgjuQrHr9KPgPXWh+WOFtZNAzYBO4F/NwUUIAD4n7VsDTDd08fp5uN/AdgIbMDx66+3u46nq7wHx9Tp0sHGps/AG1b5Bhz5uBI9fZweeA8ycfzQXGc9nvD0cXb2oRkElFJK2U5HoymllLKdBhullFK202CjlFLKdhpslFJK2U6DjVJKKdtpsFGqBSJS7ub9/VdEhrloWw1WtuRNIvJuW9mCRSRKRH7lin0r1Rwd+qxUC0Sk3BgT5sLt+ZkjSRVt5dx2EVkEbDfG3NtK/RTgPWPMCHe0T3U/emajVAeISJyIvCEiq6zHNKt8ooh8KyJrrb+DrfIrReR1EXkX+FhEzhCRz0VksTVPyYtOc5d8LiJp1vNyKwnlehH5TkTirfIB1utVInJ3O8++VnAkuWeYiCwTkTXW/ClzrDp/AwZYZ0P3W3X/YO1ng4jc5cK3UXVDGmyU6ph/4phnZwLwI+C/VvlW4DRjzFgc2Yn/4rTOFGChMWa69Xos8FtgGNAfmNbMfkKB74wxo4EvgZ877f+f1v6byyF2FCsP1wwcWRgAqoELjDHjgDOBB61gdwuw0zjmDvqDiMzCMa/KRGAMMF5ETmtrf0q1RBNxKtUxZwHDnLL4RohIOBAJLBKRVBwZe/2d1vnEGOM8z8n3xphsABFZhyOf1tfH7KeWI8lHV+OYQA0cgatpvpuXaHlSvWCnba/GMQkXOHJv/cUKHI04znjim1l/lvVYa70OwxF8vmxhf0q1SoONUh3jA0wxxlQ5F4rIv4DlxpgLrOsfnzstrjhmGzVOzxto/v9hnTlyQbWlOq2pMsaMEZFIHEHrWuARHPPCxAHjjTF1IrIHCGpmfQH+aoz5Twf3q1SztBtNqY75GMe8KgCISFP6+Eggx3p+pY37/w5H9x3AgrYqG2NKgV8DvxcRfxztzLcCzZlAX6vqISDcadWlwE+tOVkQkUQR6emiY1DdkAYbpVoWIiLZTo8bcXxxp1kXzTfjmAoC4D7gryLyDeBrY5t+C9woIt8DvYHStlYwxqzFkXV4AY7Jt9JEJB3HWc5Wq04h8I01VPp+Y8zHOLrpVojIRmAxRwcjpTpEhz4rdRKxZvCsMsYYEVkAXGKMmdPWekp5ml6zUerkMh74tzWCrISTaKps1b3pmY1SSinb6TUbpZRSttNgo5RSynYabJRSStlOg41SSinbabBRSillOw02SimlbPf/y4gxMt2UztIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:30:31.790894Z",
     "start_time": "2019-06-29T14:30:31.177859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:30:49.124886Z",
     "start_time": "2019-06-29T14:30:48.387843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:33:55.832565Z",
     "start_time": "2019-06-29T14:33:55.481545Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(175, 29)\n",
       "    (1): Embedding(8, 5)\n",
       "    (2): Embedding(4, 3)\n",
       "    (3): Embedding(13, 7)\n",
       "    (4): Embedding(32, 11)\n",
       "    (5): Embedding(3, 3)\n",
       "    (6): Embedding(3, 3)\n",
       "    (7): Embedding(5, 4)\n",
       "    (8): Embedding(3, 3)\n",
       "    (9): Embedding(12, 6)\n",
       "    (10): Embedding(54, 15)\n",
       "    (11): Embedding(367, 44)\n",
       "    (12): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.039)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=140, out_features=1300, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(1300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.1)\n",
       "    (4): Linear(in_features=1300, out_features=1000, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5)\n",
       "    (8): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.49)\n",
       "    (12): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Search Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T22:22:42.748559Z",
     "start_time": "2019-07-03T22:22:42.298534Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Pattern Serch def function:\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "def PatternSearch(param_dist, PercentToUse=20, alfa=2, verbose=False, metric=2,\n",
    "                  XevalMetric=explained_variance,xs={}):\n",
    "    \n",
    "    class Dimension():\n",
    "        def __init__(self, value):\n",
    "            #If value is a Tuple, divide the interval into \"length\" equaly spaced intervals:\n",
    "            if isinstance(value,tuple):\n",
    "               lower=value[0];upper=value[1];length=value[2]\n",
    "               value=[lower + x*(upper-lower)/(length-1) for x in range(length)]\n",
    "            self.value=value\n",
    "            self.value.sort()\n",
    "            self.min=self.value[0]\n",
    "            self.max=self.value[-1]\n",
    "            self.midptidx=int((len(self.value)/2)-0.5)\n",
    "            self.midpoint=self.value[self.midptidx]\n",
    "            self.Delta=(len(value)-1)-self.midptidx\n",
    "            self.BestValue=self.midpoint\n",
    "            self.CurrIndex=self.midptidx\n",
    "            self.BestIndex=self.midptidx\n",
    "        \n",
    "    Space={}\n",
    "    BestScore=[]; CurrScore=[]\n",
    "    for Dkey, Dval in param_dist.items():\n",
    "        Space[Dkey]=Dimension(Dval)\n",
    "        print(Dkey,\":\",Dval)\n",
    "    \n",
    "    #Episilon=0.001;\n",
    "    k=0.5; xc={}\n",
    "    global BestLearn\n",
    "    #print(Space)\n",
    "    if xs == {}:\n",
    "        #builds the first exploratory point by collecting the midpoint of each dimension:\n",
    "        for Dkey, Dval in Space.items():\n",
    "            xc[Dkey]=Dval.midpoint\n",
    "    else:\n",
    "        xc=xs\n",
    "    \n",
    "    print(xc)    \n",
    "    BestScore=CrossEval3(data,PercentToUse,mp=xc,verbose=verbose, metrics=XevalMetric)\n",
    "    BestScore=list(BestScore)\n",
    "    BestLearn=learn\n",
    "    #print(\"Type BestScore: \",type(BestScore))\n",
    "    cols=list(xc.keys())\n",
    "    cols.append('score')\n",
    "    global df\n",
    "    df=pd.DataFrame(columns=cols)\n",
    "    xc.update({'score':BestScore[metric]})\n",
    "    df=df.append(xc, ignore_index=True)\n",
    "    print(df)\n",
    "\n",
    "    Ndimensions=len(Space);  \n",
    "    BestIdx=0; InitialExploration=0\n",
    "\n",
    "    # i=exploratory moves iterations; k=Overall Iterations\n",
    "    i=0; Continue=0\n",
    "\n",
    "    while Continue<3:\n",
    "        #Exploratory Search:\n",
    "        while i < Ndimensions:\n",
    "            k+=1\n",
    "            for Direction in [1,-1]:\n",
    "                xn={}; xd={}\n",
    "                for CurDim in range(0,Ndimensions): #Build the vextor xn:\n",
    "                    if i == CurDim:\n",
    "                        NewIndex=list(Space.values())[CurDim].CurrIndex + Direction*list(Space.values())[CurDim].Delta\n",
    "                        #print(NewIndex)\n",
    "                        if NewIndex>len(list(Space.values())[CurDim].value)-1: NewIndex=len(list(Space.values())[CurDim].value)-1\n",
    "                        if NewIndex<0: NewIndex=0\n",
    "                        xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].value[NewIndex]\n",
    "                    else:\n",
    "                        xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].BestValue\n",
    "                if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                    print(Direction, xn)            \n",
    "                    CurrScore=CrossEval3(data, PercentToUse,mp=xn,verbose=verbose, metrics=XevalMetric)\n",
    "                    xd=xn.copy()\n",
    "                    xd.update({'score':CurrScore[metric]})\n",
    "                    df=df.append(xd, ignore_index=True)\n",
    "                    print(df)\n",
    "                    #print(CurrScore[metric])\n",
    "                    if CurrScore[metric] > BestScore[metric]: \n",
    "                        BestScore[metric]=CurrScore[metric]\n",
    "                        list(Space.values())[i].BestValue=list(Space.values())[i].value[NewIndex]\n",
    "                        list(Space.values())[i].BestIndex=NewIndex\n",
    "                        xc=xn.copy()\n",
    "                        #BestIdx=k\n",
    "                        BestLearn=learn\n",
    "                        break\n",
    "            list(Space.values())[i].CurrIndex=list(Space.values())[i].BestIndex \n",
    "            i+=1\n",
    "\n",
    "        #xc={}\n",
    "        #for Dkey, Dval in Space.items():\n",
    "        #    xc[Dkey]=Dval.BestValue\n",
    "        #print(xc)\n",
    "\n",
    "        #pattern move:\n",
    "        #BestIdx=int(BestIdx+0.5)\n",
    "        BestIdx=df['score'].idxmax()\n",
    "        pm=df.values[BestIdx]+(df.values[BestIdx]-df.values[InitialExploration])\n",
    "        pm=pm[0:(len(pm)-1)]\n",
    "        print(\"Theoretical Pattern Move: \",pm)\n",
    "\n",
    "        #picks the closest elements in the lists to the ideal point\n",
    "        n=0\n",
    "        for Dkey, Dval in Space.items():\n",
    "            xn[Dkey]=min(Dval.value, key=lambda x:abs(x-pm[n])) \n",
    "            n+=1\n",
    "\n",
    "        #Evaluates pattern move it it has not been evaluated already:\n",
    "        if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "            print(xn)\n",
    "            print(\"Executing Pattern Move...\")\n",
    "            k+=1\n",
    "            CurrScore=CrossEval3(data, PercentToUse, mp=xn,verbose=verbose, metrics=XevalMetric)\n",
    "            xd=xn.copy()\n",
    "            xd.update({'score':CurrScore[metric]})\n",
    "            df=df.append(xd, ignore_index=True)\n",
    "            print(df)\n",
    "            if CurrScore[metric] > BestScore[metric]:\n",
    "                BestScore[metric]=CurrScore[metric]\n",
    "                xc=xn.copy()\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(Space.values())[CurDim].BestIndex=list(Space.values())[CurDim].value.index(list(xc.values())[CurDim])\n",
    "                    list(Space.values())[CurDim].CurrIndex=list(Space.values())[CurDim].BestIndex\n",
    "                    list(Space.values())[CurDim].BestValue=list(xc.values())[CurDim]\n",
    "                #BestIdx=k\n",
    "                BestIdx=df['score'].idxmax()\n",
    "                InitialExploration=BestIdx\n",
    "                BestScore[metric]=CurrScore[metric]\n",
    "                BestLearn=learn\n",
    "            else:\n",
    "                #divide delta by 2\n",
    "                DeltaVector=[]\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "                    DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "        else:\n",
    "            #divide delta by 2\n",
    "            print(\"Closest point in the Space\",xn,\" has been evaluated\")\n",
    "            DeltaVector=[]\n",
    "            for CurDim in range(0,Ndimensions):\n",
    "                list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "                if list(Space.values())[CurDim].Delta==0: list(Space.values())[CurDim].Delta=1\n",
    "                DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "\n",
    "        print(\"Current Delta values for each dimension: \",DeltaVector)\n",
    "        i=0 \n",
    "        print(Ndimensions)\n",
    "        if DeltaVector==list(np.ones(len(Space))): Continue+=1\n",
    "\n",
    "    print(\"Best Parameters Found:\")\n",
    "    xc=dict(df.loc[df['score'].idxmax()])\n",
    "    del xc[\"score\"]\n",
    "    \n",
    "    print(\" \");print(xc);print(\" \")\n",
    "    print(df.loc[df['score'].idxmax()])\n",
    "    df.sort_values('score', axis=0, ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Search Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T00:28:18.170603Z",
     "start_time": "2019-06-27T23:31:59.282549Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [500, 1000, 1100]\n",
      "layer2 : [50, 100, 250, 500, 1000]\n",
      "layer3 : [50, 100, 250, 500, 1000]\n",
      "lr : [0.0001, 0.0005, 0.001, 0.003, 0.005, 0.01]\n",
      "wd : [0.2, 0.4, 0.5, 0.6]\n",
      "div_factor : [8, 10, 12, 15, 25]\n",
      "ps1 : [0.08, 0.09, 0.1]\n",
      "ps2 : [0.4, 0.5, 0.6]\n",
      "ps3 : [0.4, 0.5, 0.6]\n",
      "emb_drop : [0.03, 0.04, 0.05]\n",
      "{'layer1': 1000, 'layer2': 250, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 65.880639\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 {'layer1': 1100, 'layer2': 250, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 76.398907\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 117.776986\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 150.205060\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 147.894337\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 144.328819\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "5  -967.740784  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 148.981778\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "5  -967.740784  \n",
      "6  -929.789062  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 147.923317\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "5  -967.740784  \n",
      "6  -929.789062  \n",
      "7  -913.707275  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 146.982461\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "5  -967.740784  \n",
      "6  -929.789062  \n",
      "7  -913.707275  \n",
      "8  -919.638489  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.08, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 155.184901\n",
      "Validation data set shape: (20404, 18)\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1681.047485  \n",
      "1 -1619.853882  \n",
      "2 -1531.542603  \n",
      "3 -1293.934814  \n",
      "4  -944.028381  \n",
      "5  -967.740784  \n",
      "6  -929.789062  \n",
      "7  -913.707275  \n",
      "8  -919.638489  \n",
      "9  -965.174805  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.6, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 151.896275\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.4, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 153.040753\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 168.903661\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.4, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 156.473950\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.05}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 148.226478\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.03}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 170.063727\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1000, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 157.398002\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "-1 {'layer1': 1100, 'layer2': 500, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 111.729391\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 500, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 140.208019\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "18  -995.710388  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.003, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 154.887859\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "18  -995.710388  \n",
      "19 -1165.386841  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 151.485665\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "18  -995.710388  \n",
      "19 -1165.386841  \n",
      "20  -946.190430  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 15, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 146.813397\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "21  1100.0  1000.0  1000.0  0.010  0.2        15.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "18  -995.710388  \n",
      "19 -1165.386841  \n",
      "20  -946.190430  \n",
      "21  -988.473572  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 149.551554\n",
      "Validation data set shape: (20404, 18)\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "21  1100.0  1000.0  1000.0  0.010  0.2        15.0  0.09  0.5  0.5      0.04   \n",
      "22  1100.0  1000.0  1000.0  0.005  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1681.047485  \n",
      "1  -1619.853882  \n",
      "2  -1531.542603  \n",
      "3  -1293.934814  \n",
      "4   -944.028381  \n",
      "5   -967.740784  \n",
      "6   -929.789062  \n",
      "7   -913.707275  \n",
      "8   -919.638489  \n",
      "9   -965.174805  \n",
      "10  -961.620605  \n",
      "11  -956.899414  \n",
      "12  -948.803467  \n",
      "13  -944.471252  \n",
      "14  -963.932800  \n",
      "15  -946.919373  \n",
      "16  -967.798401  \n",
      "17  -942.263245  \n",
      "18  -995.710388  \n",
      "19 -1165.386841  \n",
      "20  -946.190430  \n",
      "21  -988.473572  \n",
      "22 -1073.419556  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Mdl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-c0dca64ec7ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m'emb_drop'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m }\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mPatternSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-b8b71301cab1>\u001b[0m in \u001b[0;36mPatternSearch\u001b[1;34m(param_dist, PercentToUse, alfa, verbose, metric)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mxc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mMdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mxc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Mdl' is not defined"
     ]
    }
   ],
   "source": [
    "#Optimizing for MAE at 20% of the data\n",
    "# xc={'layer1':1100,'layer2': 250,'layer3': 1000, 'lr':0.0005,'wd': 0.2,'div_factor': 15,\n",
    "#     'ps1':0.1,'ps2': 0.4,'ps3': 0.5,'emb_drop': 0.03}\n",
    "param_dist = {\n",
    "    'layer1':[500,1000,1100], \n",
    "    'layer2':[50,100,250,500,1000], \n",
    "    'layer3':[50,100,250,500,1000], \n",
    "    'lr':[1e-4,5e-4,1e-3,3e-3,5e-3,1e-2], \n",
    "    'wd':[0.2,0.4,0.5,0.6], \n",
    "    'div_factor':[8,10,12,15,25], \n",
    "    'ps1':[0.09,0.08,0.1],\n",
    "    'ps2':[0.5,0.4,0.6], \n",
    "    'ps3':[0.5,0.4,0.6], \n",
    "    'emb_drop':[0.04,0.03,0.05]\n",
    "}\n",
    "PatternSearch(param_dist, 20, alfa=1.4, metric=7, XevalMetric=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T03:09:54.584206Z",
     "start_time": "2019-06-28T02:16:34.586177Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [500, 1000, 1100]\n",
      "layer2 : [50, 100, 250, 500, 1000]\n",
      "layer3 : [50, 100, 250, 500, 1000]\n",
      "lr : [0.0001, 0.0005, 0.001, 0.003, 0.005, 0.01]\n",
      "wd : [0.2, 0.4, 0.5, 0.6]\n",
      "div_factor : [8, 10, 12, 15, 25]\n",
      "ps1 : [0.08, 0.09, 0.1]\n",
      "ps2 : [0.4, 0.5, 0.6]\n",
      "ps3 : [0.4, 0.5, 0.6]\n",
      "emb_drop : [0.03, 0.04, 0.05]\n",
      "{'layer1': 1000, 'layer2': 250, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 66.174785\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1 {'layer1': 1100, 'layer2': 250, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 71.820108\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 250, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 112.629442\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.001, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.410203\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 145.210306\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.686218\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "5  0.757915  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.679218\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "5  0.757915  \n",
      "6  0.768368  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 144.614272\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "5  0.757915  \n",
      "6  0.768368  \n",
      "7  0.774526  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 150.572612\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "5  0.757915  \n",
      "6  0.768368  \n",
      "7  0.774526  \n",
      "8  0.766804  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.08, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.911231\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1  1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2  1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3  1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4  1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5  1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6  1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "\n",
      "      score  \n",
      "0  0.435171  \n",
      "1  0.503410  \n",
      "2  0.526800  \n",
      "3  0.580557  \n",
      "4  0.763524  \n",
      "5  0.757915  \n",
      "6  0.768368  \n",
      "7  0.774526  \n",
      "8  0.766804  \n",
      "9  0.757370  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.6, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.721220\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.4, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 144.380258\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 145.324312\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.4, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.359200\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.05}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.853228\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.03}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.150188\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1000, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 142.774166\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "-1 {'layer1': 1100, 'layer2': 500, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 108.958232\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 500, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 127.345284\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "18  0.723951  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.003, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.715220\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "18  0.723951  \n",
      "19  0.627428  \n",
      "1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 147.701448\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "18  0.723951  \n",
      "19  0.627428  \n",
      "20  0.771663  \n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 15, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.686219\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "21  1100.0  1000.0  1000.0  0.010  0.2        15.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "18  0.723951  \n",
      "19  0.627428  \n",
      "20  0.771663  \n",
      "21  0.742027  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 143.446204\n",
      "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
      "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
      "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
      "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
      "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
      "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
      "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
      "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
      "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
      "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
      "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
      "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
      "21  1100.0  1000.0  1000.0  0.010  0.2        15.0  0.09  0.5  0.5      0.04   \n",
      "22  1100.0  1000.0  1000.0  0.005  0.2        25.0  0.09  0.5  0.5      0.04   \n",
      "\n",
      "       score  \n",
      "0   0.435171  \n",
      "1   0.503410  \n",
      "2   0.526800  \n",
      "3   0.580557  \n",
      "4   0.763524  \n",
      "5   0.757915  \n",
      "6   0.768368  \n",
      "7   0.774526  \n",
      "8   0.766804  \n",
      "9   0.757370  \n",
      "10  0.754634  \n",
      "11  0.759425  \n",
      "12  0.760545  \n",
      "13  0.765773  \n",
      "14  0.760452  \n",
      "15  0.767382  \n",
      "16  0.759708  \n",
      "17  0.766072  \n",
      "18  0.723951  \n",
      "19  0.627428  \n",
      "20  0.771663  \n",
      "21  0.742027  \n",
      "22  0.704894  \n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [1.20e+03 1.75e+03 1.75e+03 1.90e-02 0.00e+00 3.80e+01 9.00e-02 5.00e-01 5.00e-01 4.00e-02]\n",
      "Closest point in the Space {'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 1100.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.01, 'wd': 0.2, 'div_factor': 25.0, 'ps1': 0.09, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      " \n",
      "layer1        1100.000000\n",
      "layer2        1000.000000\n",
      "layer3        1000.000000\n",
      "lr               0.010000\n",
      "wd               0.200000\n",
      "div_factor      25.000000\n",
      "ps1              0.090000\n",
      "ps2              0.500000\n",
      "ps3              0.500000\n",
      "emb_drop         0.040000\n",
      "score            0.774526\n",
      "Name: 7, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>layer3</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>ps3</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.774526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.771663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.768368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.767382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.766804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.766072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.765773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.763524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.760545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.760452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.759708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.759425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.757915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.757370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.754634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.742027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.723951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.704894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.627428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.580557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.503410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.435171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
       "7   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "20  1100.0  1000.0  1000.0  0.010  0.4        25.0  0.09  0.5  0.5      0.04   \n",
       "6   1100.0  1000.0  1000.0  0.010  0.2        12.0  0.09  0.5  0.5      0.04   \n",
       "15  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.03   \n",
       "8   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.10  0.5  0.5      0.04   \n",
       "17  1100.0   500.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "13  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.4      0.04   \n",
       "4   1100.0  1000.0  1000.0  0.010  0.4        12.0  0.09  0.5  0.5      0.04   \n",
       "12  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.6      0.04   \n",
       "14  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.05   \n",
       "16  1000.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "11  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.4  0.5      0.04   \n",
       "5   1100.0  1000.0  1000.0  0.010  0.6        12.0  0.09  0.5  0.5      0.04   \n",
       "9   1100.0  1000.0  1000.0  0.010  0.2        25.0  0.08  0.5  0.5      0.04   \n",
       "10  1100.0  1000.0  1000.0  0.010  0.2        25.0  0.09  0.6  0.5      0.04   \n",
       "21  1100.0  1000.0  1000.0  0.010  0.2        15.0  0.09  0.5  0.5      0.04   \n",
       "18  1100.0  1000.0   500.0  0.010  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "22  1100.0  1000.0  1000.0  0.005  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "19  1100.0  1000.0  1000.0  0.003  0.2        25.0  0.09  0.5  0.5      0.04   \n",
       "3   1100.0  1000.0  1000.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
       "2   1100.0  1000.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
       "1   1100.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
       "0   1000.0   250.0   250.0  0.001  0.4        12.0  0.09  0.5  0.5      0.04   \n",
       "\n",
       "       score  \n",
       "7   0.774526  \n",
       "20  0.771663  \n",
       "6   0.768368  \n",
       "15  0.767382  \n",
       "8   0.766804  \n",
       "17  0.766072  \n",
       "13  0.765773  \n",
       "4   0.763524  \n",
       "12  0.760545  \n",
       "14  0.760452  \n",
       "16  0.759708  \n",
       "11  0.759425  \n",
       "5   0.757915  \n",
       "9   0.757370  \n",
       "10  0.754634  \n",
       "21  0.742027  \n",
       "18  0.723951  \n",
       "22  0.704894  \n",
       "19  0.627428  \n",
       "3   0.580557  \n",
       "2   0.526800  \n",
       "1   0.503410  \n",
       "0   0.435171  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same parameters but optimizing for explained variance at 20%\n",
    "PatternSearch(param_dist, 20, alfa=1.4, metric=2, XevalMetric=explained_variance )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T09:22:41.602458Z",
     "start_time": "2019-06-28T04:02:59.424300Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [1050, 1100, 1200, 1300]\n",
      "layer2 : [900, 1000, 1100]\n",
      "layer3 : [900, 1000, 1100]\n",
      "lr : [0.005, 0.008, 0.01]\n",
      "wd : [0.1, 0.2, 0.3, 0.4]\n",
      "div_factor : [8, 10, 12, 15, 25]\n",
      "ps1 : [0.09, 0.095, 0.1]\n",
      "ps2 : [0.49, 0.5]\n",
      "ps3 : [0.49, 0.5]\n",
      "emb_drop : [0.039, 0.04]\n",
      "{'layer1': 1100, 'layer2': 1000, 'layer3': 1000, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 718.358088\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 767.052873\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 821.554990\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "-1 {'layer1': 1300, 'layer2': 900, 'layer3': 1000, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 720.823229\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1100, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 799.766744\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 900, 'lr': 0.008, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 745.941665\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5  1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "5     0.039 -1058.536499  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 776.497413\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5  1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6  1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "5     0.039 -1058.536499  \n",
      "6     0.039 -1027.472778  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.2, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 774.080275\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5  1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6  1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7  1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "5     0.039 -1058.536499  \n",
      "6     0.039 -1027.472778  \n",
      "7     0.039 -1017.297852  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.4, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 774.134278\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5  1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6  1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7  1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8  1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "5     0.039 -1058.536499  \n",
      "6     0.039 -1027.472778  \n",
      "7     0.039 -1017.297852  \n",
      "8     0.039 -1029.416626  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 778.317517\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0  1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1  1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2  1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3  1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4  1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5  1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6  1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7  1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8  1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9  1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "\n",
      "   emb_drop        score  \n",
      "0     0.039 -1046.003784  \n",
      "1     0.039 -1022.560730  \n",
      "2     0.039 -1059.312866  \n",
      "3     0.039 -1061.715698  \n",
      "4     0.039 -1061.216675  \n",
      "5     0.039 -1058.536499  \n",
      "6     0.039 -1027.472778  \n",
      "7     0.039 -1017.297852  \n",
      "8     0.039 -1029.416626  \n",
      "9     0.039 -1007.365051  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 774.908322\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 777.512471\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 806.434125\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 773.385235\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 771.244113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.0e+03 1.0e+03 2.0e-03 0.0e+00 1.2e+01 9.5e-02 4.9e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1200, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 751.786000\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 817.812776\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "-1 {'layer1': 1300, 'layer2': 900, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 721.929292\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1100, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 798.285659\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 900, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 744.981611\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "19     0.039 -1036.380493  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.008, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 784.275858\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "20  1300.0  1000.0  1000.0  0.008  0.1        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "19     0.039 -1036.380493  \n",
      "20     0.039 -1010.895325  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 775.532358\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "20  1300.0  1000.0  1000.0  0.008  0.1        25.0  0.100  0.50  0.49   \n",
      "21  1300.0  1000.0  1000.0  0.005  0.2        25.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "19     0.039 -1036.380493  \n",
      "20     0.039 -1010.895325  \n",
      "21     0.039 -1015.800537  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 15, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 777.251456\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "20  1300.0  1000.0  1000.0  0.008  0.1        25.0  0.100  0.50  0.49   \n",
      "21  1300.0  1000.0  1000.0  0.005  0.2        25.0  0.100  0.50  0.49   \n",
      "22  1300.0  1000.0  1000.0  0.005  0.1        15.0  0.100  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "19     0.039 -1036.380493  \n",
      "20     0.039 -1010.895325  \n",
      "21     0.039 -1015.800537  \n",
      "22     0.039 -1004.157959  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 774.614305\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
      "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
      "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
      "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
      "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
      "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
      "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "20  1300.0  1000.0  1000.0  0.008  0.1        25.0  0.100  0.50  0.49   \n",
      "21  1300.0  1000.0  1000.0  0.005  0.2        25.0  0.100  0.50  0.49   \n",
      "22  1300.0  1000.0  1000.0  0.005  0.1        15.0  0.100  0.50  0.49   \n",
      "23  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1046.003784  \n",
      "1      0.039 -1022.560730  \n",
      "2      0.039 -1059.312866  \n",
      "3      0.039 -1061.715698  \n",
      "4      0.039 -1061.216675  \n",
      "5      0.039 -1058.536499  \n",
      "6      0.039 -1027.472778  \n",
      "7      0.039 -1017.297852  \n",
      "8      0.039 -1029.416626  \n",
      "9      0.039 -1007.365051  \n",
      "10     0.039 -1003.385986  \n",
      "11     0.039 -1002.580872  \n",
      "12     0.039 -1001.814148  \n",
      "13     0.039 -1007.947754  \n",
      "14     0.040 -1008.739258  \n",
      "15     0.039 -1028.187378  \n",
      "16     0.039 -1048.557251  \n",
      "17     0.039 -1035.454956  \n",
      "18     0.039 -1039.615601  \n",
      "19     0.039 -1036.380493  \n",
      "20     0.039 -1010.895325  \n",
      "21     0.039 -1015.800537  \n",
      "22     0.039 -1004.157959  \n",
      "23     0.039 -1010.372803  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.0e+03 1.0e+03 2.0e-03 0.0e+00 1.2e+01 9.5e-02 4.9e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [1.5e+03 1.0e+03 1.0e+03 2.0e-03 0.0e+00 1.2e+01 9.5e-02 4.9e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 12, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 1300.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      " \n",
      "layer1        1300.000000\n",
      "layer2        1000.000000\n",
      "layer3        1000.000000\n",
      "lr               0.005000\n",
      "wd               0.100000\n",
      "div_factor      25.000000\n",
      "ps1              0.100000\n",
      "ps2              0.500000\n",
      "ps3              0.490000\n",
      "emb_drop         0.039000\n",
      "score        -1001.814148\n",
      "Name: 12, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>layer3</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>ps3</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1001.814148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1002.580872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1003.385986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1004.157959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1007.365051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1007.947754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1008.739258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1010.372803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1010.895325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1015.800537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1017.297852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1022.560730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1027.472778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1028.187378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1029.416626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1035.454956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1036.380493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1039.615601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1046.003784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1048.557251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1058.536499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1059.312866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1061.216675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1061.715698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
       "12  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "11  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.49  0.49   \n",
       "10  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.49  0.49   \n",
       "22  1300.0  1000.0  1000.0  0.005  0.1        15.0  0.100  0.50  0.49   \n",
       "9   1300.0  1000.0  1000.0  0.005  0.1        12.0  0.095  0.49  0.49   \n",
       "13  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.50   \n",
       "14  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "23  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.095  0.50  0.49   \n",
       "20  1300.0  1000.0  1000.0  0.008  0.1        25.0  0.100  0.50  0.49   \n",
       "21  1300.0  1000.0  1000.0  0.005  0.2        25.0  0.100  0.50  0.49   \n",
       "7   1300.0  1000.0  1000.0  0.005  0.2        12.0  0.095  0.49  0.49   \n",
       "1   1300.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "6   1300.0  1000.0  1000.0  0.010  0.2        12.0  0.095  0.49  0.49   \n",
       "15  1200.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "8   1300.0  1000.0  1000.0  0.005  0.4        12.0  0.095  0.49  0.49   \n",
       "17  1300.0   900.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "19  1300.0  1000.0   900.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "18  1300.0  1000.0  1100.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "0   1100.0  1000.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "16  1300.0  1100.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "5   1300.0  1000.0   900.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "2   1300.0  1100.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "4   1300.0  1000.0  1100.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "3   1300.0   900.0  1000.0  0.008  0.2        12.0  0.095  0.49  0.49   \n",
       "\n",
       "    emb_drop        score  \n",
       "12     0.039 -1001.814148  \n",
       "11     0.039 -1002.580872  \n",
       "10     0.039 -1003.385986  \n",
       "22     0.039 -1004.157959  \n",
       "9      0.039 -1007.365051  \n",
       "13     0.039 -1007.947754  \n",
       "14     0.040 -1008.739258  \n",
       "23     0.039 -1010.372803  \n",
       "20     0.039 -1010.895325  \n",
       "21     0.039 -1015.800537  \n",
       "7      0.039 -1017.297852  \n",
       "1      0.039 -1022.560730  \n",
       "6      0.039 -1027.472778  \n",
       "15     0.039 -1028.187378  \n",
       "8      0.039 -1029.416626  \n",
       "17     0.039 -1035.454956  \n",
       "19     0.039 -1036.380493  \n",
       "18     0.039 -1039.615601  \n",
       "0      0.039 -1046.003784  \n",
       "16     0.039 -1048.557251  \n",
       "5      0.039 -1058.536499  \n",
       "2      0.039 -1059.312866  \n",
       "4      0.039 -1061.216675  \n",
       "3      0.039 -1061.715698  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the learnings from the previous run to reset the parameters and run at 100%\n",
    "param_dist = {\n",
    "    'layer1':[1050, 1100,1200, 1300], \n",
    "    'layer2':[900,1000,1100], \n",
    "    'layer3':[900,1000,1100], \n",
    "    'lr':[5e-3,8e-3,1e-2], \n",
    "    'wd':[0.1,0.2,0.3,0.4], \n",
    "    'div_factor':[8,10,12,15,25], \n",
    "    'ps1':[0.09,0.095,0.1],\n",
    "    'ps2':[0.5,0.49], \n",
    "    'ps3':[0.5,0.49], \n",
    "    'emb_drop':[0.04,0.039]\n",
    "}\n",
    "PatternSearch(param_dist, 100, alfa=1.4, metric=7, XevalMetric=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T04:08:39.578758Z",
     "start_time": "2019-06-28T20:56:51.211679Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [1200, 1300, 1400]\n",
      "layer2 : [700, 1000, 1100, 1200]\n",
      "layer3 : [600, 1000, 1100, 1200]\n",
      "lr : [0.005, 0.008, 0.009, 0.01, 0.1, 1]\n",
      "wd : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "div_factor : [25, 26]\n",
      "ps1 : [0.094, 0.095, 0.096, 0.1]\n",
      "ps2 : [0.49, 0.5, 0.51, 0.6]\n",
      "ps3 : [0.49, 0.5, 0.51, 0.6]\n",
      "emb_drop : [0.039, 0.04, 0.041]\n",
      "{'layer1': 1300.0, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 756.415265\n",
      "   layer1  layer2  layer3     lr   wd  div_factor  ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.1  0.5  0.49     0.039   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 {'layer1': 1400, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 798.460669\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "-1 {'layer1': 1200, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 744.533585\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 851.733716\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "-1 {'layer1': 1300, 'layer2': 700, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 649.944175\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1200, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 849.281576\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "5  1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "5 -1062.984009  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 600, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 699.220993\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "5  1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "6  1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "5 -1062.984009  \n",
      "6 -1093.201050  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 1, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 1099.065863\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "5  1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "6  1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "7  1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "5 -1062.984009  \n",
      "6 -1093.201050  \n",
      "7 -3699.194336  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 770.313060\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "5  1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "6  1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "7  1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "8  1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "5 -1062.984009  \n",
      "6 -1093.201050  \n",
      "7 -3699.194336  \n",
      "8 -1022.783691  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 774.639307\n",
      "   layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "2  1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "3  1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "4  1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "5  1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "6  1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "7  1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "8  1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50     0.040   \n",
      "9  1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1001.814148  \n",
      "1 -1058.283813  \n",
      "2 -1077.550293  \n",
      "3 -1056.231689  \n",
      "4 -1071.444336  \n",
      "5 -1062.984009  \n",
      "6 -1093.201050  \n",
      "7 -3699.194336  \n",
      "8 -1022.783691  \n",
      "9 -1078.152954  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 769.307002\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 26, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 770.238055\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 764.636735\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.5  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 768.562959\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.5  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.094, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 763.645678\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.5  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.5  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.6, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 776.251399\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.5  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.5  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.5  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.5  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.5  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.5  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.5  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.5  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.6  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.49, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 772.280172\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 768.610962\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 773.481241\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.041}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 768.354947\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 766.456839\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "Theoretical Pattern Move:  [1.3e+03 1.0e+03 1.0e+03 5.0e-03 1.0e-01 2.5e+01 1.0e-01 5.0e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 2, 2, 1, 1, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 818.397810\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1100, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 796.616564\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.1, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 776.559417\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.5, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 775.382349\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.096, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 769.080989\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.51, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 769.420008\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 771.949153\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "27     0.040 -1042.207031  \n",
      "Theoretical Pattern Move:  [1.3e+03 1.0e+03 1.0e+03 5.0e-03 1.0e-01 2.5e+01 1.0e-01 5.0e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.01, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 769.328003\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
      "28  1300.0  1000.0  1000.0  0.010  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "27     0.040 -1042.207031  \n",
      "28     0.040 -1045.663208  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.008, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 773.177223\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
      "28  1300.0  1000.0  1000.0  0.010  0.3        25.0  0.095  0.50  0.50   \n",
      "29  1300.0  1000.0  1000.0  0.008  0.3        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "27     0.040 -1042.207031  \n",
      "28     0.040 -1045.663208  \n",
      "29     0.040 -1034.734985  \n",
      "1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 765.576789\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
      "28  1300.0  1000.0  1000.0  0.010  0.3        25.0  0.095  0.50  0.50   \n",
      "29  1300.0  1000.0  1000.0  0.008  0.3        25.0  0.095  0.50  0.50   \n",
      "30  1300.0  1000.0  1000.0  0.009  0.4        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "27     0.040 -1042.207031  \n",
      "28     0.040 -1045.663208  \n",
      "29     0.040 -1034.734985  \n",
      "30     0.040 -1051.262085  \n",
      "-1 {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.009, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.095, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 790.811436\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
      "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
      "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
      "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
      "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
      "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
      "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
      "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
      "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
      "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
      "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
      "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
      "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
      "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
      "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
      "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
      "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
      "28  1300.0  1000.0  1000.0  0.010  0.3        25.0  0.095  0.50  0.50   \n",
      "29  1300.0  1000.0  1000.0  0.008  0.3        25.0  0.095  0.50  0.50   \n",
      "30  1300.0  1000.0  1000.0  0.009  0.4        25.0  0.095  0.50  0.50   \n",
      "31  1300.0  1000.0  1000.0  0.009  0.2        25.0  0.095  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1001.814148  \n",
      "1      0.040 -1058.283813  \n",
      "2      0.040 -1077.550293  \n",
      "3      0.040 -1056.231689  \n",
      "4      0.040 -1071.444336  \n",
      "5      0.040 -1062.984009  \n",
      "6      0.040 -1093.201050  \n",
      "7      0.040 -3699.194336  \n",
      "8      0.040 -1022.783691  \n",
      "9      0.040 -1078.152954  \n",
      "10     0.040 -1010.175476  \n",
      "11     0.040 -1043.532593  \n",
      "12     0.040 -1038.338623  \n",
      "13     0.040 -1041.815063  \n",
      "14     0.040 -1036.502808  \n",
      "15     0.040 -1042.157104  \n",
      "16     0.040 -1040.960205  \n",
      "17     0.040 -1037.550415  \n",
      "18     0.040 -1038.385132  \n",
      "19     0.041 -1038.078979  \n",
      "20     0.039 -1037.886353  \n",
      "21     0.040 -1092.670288  \n",
      "22     0.040 -1093.523315  \n",
      "23     0.040 -1288.345093  \n",
      "24     0.040 -1069.146606  \n",
      "25     0.040 -1041.887817  \n",
      "26     0.040 -1043.802856  \n",
      "27     0.040 -1042.207031  \n",
      "28     0.040 -1045.663208  \n",
      "29     0.040 -1034.734985  \n",
      "30     0.040 -1051.262085  \n",
      "31     0.040 -1026.899170  \n",
      "Theoretical Pattern Move:  [1.3e+03 1.0e+03 1.0e+03 5.0e-03 1.0e-01 2.5e+01 1.0e-01 5.0e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [1.3e+03 1.0e+03 1.0e+03 5.0e-03 1.0e-01 2.5e+01 1.0e-01 5.0e-01 4.9e-01 3.9e-02]\n",
      "Closest point in the Space {'layer1': 1300, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 1300.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      " \n",
      "layer1        1300.000000\n",
      "layer2        1000.000000\n",
      "layer3        1000.000000\n",
      "lr               0.005000\n",
      "wd               0.100000\n",
      "div_factor      25.000000\n",
      "ps1              0.100000\n",
      "ps2              0.500000\n",
      "ps3              0.490000\n",
      "emb_drop         0.039000\n",
      "score        -1001.814148\n",
      "Name: 0, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>layer3</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>ps3</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1001.814148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1010.175476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1022.783691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1026.899170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1034.734985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1036.502808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1037.550415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1037.886353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1038.078979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1038.338623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1038.385132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1040.960205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1041.815063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1041.887817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1042.157104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1042.207031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1043.532593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1043.802856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1045.663208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1051.262085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1056.231689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1058.283813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1062.984009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1069.146606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1071.444336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1077.550293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1078.152954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1092.670288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1093.201050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1093.523315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1288.345093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-3699.194336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
       "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "10  1300.0  1000.0  1000.0  0.009  0.1        25.0  0.095  0.50  0.50   \n",
       "8   1300.0  1000.0  1000.0  0.005  0.3        25.0  0.095  0.50  0.50   \n",
       "31  1300.0  1000.0  1000.0  0.009  0.2        25.0  0.095  0.50  0.50   \n",
       "29  1300.0  1000.0  1000.0  0.008  0.3        25.0  0.095  0.50  0.50   \n",
       "14  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.094  0.50  0.50   \n",
       "17  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.60   \n",
       "20  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "19  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "12  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "18  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.49   \n",
       "16  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.49  0.50   \n",
       "13  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.100  0.50  0.50   \n",
       "25  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.096  0.50  0.50   \n",
       "15  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.60  0.50   \n",
       "27  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.51   \n",
       "11  1300.0  1000.0  1000.0  0.009  0.3        26.0  0.095  0.50  0.50   \n",
       "26  1300.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.51  0.50   \n",
       "28  1300.0  1000.0  1000.0  0.010  0.3        25.0  0.095  0.50  0.50   \n",
       "30  1300.0  1000.0  1000.0  0.009  0.4        25.0  0.095  0.50  0.50   \n",
       "3   1300.0  1200.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "1   1400.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "5   1300.0  1000.0  1200.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "24  1300.0  1000.0  1000.0  0.009  0.5        25.0  0.095  0.50  0.50   \n",
       "4   1300.0   700.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "2   1200.0  1000.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "9   1300.0  1000.0  1000.0  0.009  0.6        25.0  0.095  0.50  0.50   \n",
       "21  1300.0  1100.0  1000.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "6   1300.0  1000.0   600.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "22  1300.0  1000.0  1100.0  0.009  0.3        25.0  0.095  0.50  0.50   \n",
       "23  1300.0  1000.0  1000.0  0.100  0.3        25.0  0.095  0.50  0.50   \n",
       "7   1300.0  1000.0  1000.0  1.000  0.3        25.0  0.095  0.50  0.50   \n",
       "\n",
       "    emb_drop        score  \n",
       "0      0.039 -1001.814148  \n",
       "10     0.040 -1010.175476  \n",
       "8      0.040 -1022.783691  \n",
       "31     0.040 -1026.899170  \n",
       "29     0.040 -1034.734985  \n",
       "14     0.040 -1036.502808  \n",
       "17     0.040 -1037.550415  \n",
       "20     0.039 -1037.886353  \n",
       "19     0.041 -1038.078979  \n",
       "12     0.040 -1038.338623  \n",
       "18     0.040 -1038.385132  \n",
       "16     0.040 -1040.960205  \n",
       "13     0.040 -1041.815063  \n",
       "25     0.040 -1041.887817  \n",
       "15     0.040 -1042.157104  \n",
       "27     0.040 -1042.207031  \n",
       "11     0.040 -1043.532593  \n",
       "26     0.040 -1043.802856  \n",
       "28     0.040 -1045.663208  \n",
       "30     0.040 -1051.262085  \n",
       "3      0.040 -1056.231689  \n",
       "1      0.040 -1058.283813  \n",
       "5      0.040 -1062.984009  \n",
       "24     0.040 -1069.146606  \n",
       "4      0.040 -1071.444336  \n",
       "2      0.040 -1077.550293  \n",
       "9      0.040 -1078.152954  \n",
       "21     0.040 -1092.670288  \n",
       "6      0.040 -1093.201050  \n",
       "22     0.040 -1093.523315  \n",
       "23     0.040 -1288.345093  \n",
       "7      0.040 -3699.194336  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'layer1':[1200, 1300, 1400], \n",
    "    'layer2':[700,1000,1100,1200], \n",
    "    'layer3':[600,1000,1100,1200], \n",
    "    'lr':[5e-3,8e-3,9e-3,1e-2,1e-1,1], \n",
    "    'wd':[0.1,0.2,0.3,0.4,0.5,0.6], \n",
    "    'div_factor':[25,26], \n",
    "    'ps1':[0.094,0.095,0.096,0.1],\n",
    "    'ps2':[0.5,0.49,0.51,0.6], \n",
    "    'ps3':[0.5,0.49,0.51,0.6], \n",
    "    'emb_drop':[0.04,0.039,0.041]\n",
    "}\n",
    "xs={'layer1': 1300.0, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
    "PatternSearch(param_dist, 100, alfa=1.4, metric=7, XevalMetric=mae,xs=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T13:33:34.647445Z",
     "start_time": "2019-06-29T13:33:20.052610Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BestLearn.lr_find()\n",
    "BestLearn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T13:28:37.514450Z",
     "start_time": "2019-06-29T13:28:36.937417Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BestLearn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:31:23.991880Z",
     "start_time": "2019-06-29T14:31:23.627859Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(175, 29)\n",
       "    (1): Embedding(8, 5)\n",
       "    (2): Embedding(4, 3)\n",
       "    (3): Embedding(13, 7)\n",
       "    (4): Embedding(32, 11)\n",
       "    (5): Embedding(3, 3)\n",
       "    (6): Embedding(3, 3)\n",
       "    (7): Embedding(5, 4)\n",
       "    (8): Embedding(3, 3)\n",
       "    (9): Embedding(12, 6)\n",
       "    (10): Embedding(54, 15)\n",
       "    (11): Embedding(367, 44)\n",
       "    (12): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.04)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=140, out_features=1400, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(1400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.095)\n",
       "    (4): Linear(in_features=1400, out_features=1000, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5)\n",
       "    (8): Linear(in_features=1000, out_features=1200, bias=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.5)\n",
       "    (12): Linear(in_features=1200, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestLearn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T19:43:59.180615Z",
     "start_time": "2019-06-29T17:51:32.793744Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [1200, 1300, 1400]\n",
      "layer2 : [700, 1000, 1100, 1200]\n",
      "layer3 : [600, 1000, 1100, 1200]\n",
      "lr : [0.004, 0.005, 0.006, 0.008, 0.009, 0.01, 0.1, 1]\n",
      "wd : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "div_factor : [25, 26]\n",
      "ps1 : [0.098, 0.099, 0.1, 0.11, 0.12, 0.13, 0.15, 0.5]\n",
      "ps2 : [0.49, 0.5, 0.51, 0.6]\n",
      "ps3 : [0.49, 0.5, 0.51, 0.6]\n",
      "emb_drop : [0.039, 0.04, 0.041]\n",
      "{'layer1': 1300.0, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 145.780338\n",
      "   layer1  layer2  layer3     lr   wd  div_factor  ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.1  0.5  0.49     0.039   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 {'layer1': 1400, 'layer2': 1000, 'layer3': 1000, 'lr': 0.008, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 161.670247\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.250081\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1200, 'lr': 0.008, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 188.855802\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 600, 'lr': 0.008, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 160.897203\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 1, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 196.407234\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "5  1400.0  1200.0  1000.0  1.000  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "5 -3518.319092  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.157133\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "5  1400.0  1200.0  1000.0  1.000  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "6  1400.0  1200.0  1000.0  0.004  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "5 -3518.319092  \n",
      "6 -1162.151855  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.414147\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "5  1400.0  1200.0  1000.0  1.000  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "6  1400.0  1200.0  1000.0  0.004  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "7  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "5 -3518.319092  \n",
      "6 -1162.151855  \n",
      "7 -1014.118347  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 26, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 178.213193\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "5  1400.0  1200.0  1000.0  1.000  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "6  1400.0  1200.0  1000.0  0.004  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "7  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.50     0.040   \n",
      "8  1400.0  1200.0  1000.0  0.008  0.6        26.0  0.11  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "5 -3518.319092  \n",
      "6 -1162.151855  \n",
      "7 -1014.118347  \n",
      "8 -1057.133423  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.5, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.400090\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2   ps3  emb_drop  \\\n",
      "0  1300.0  1000.0  1000.0  0.005  0.1        25.0  0.10  0.5  0.49     0.039   \n",
      "1  1400.0  1000.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "2  1400.0  1200.0  1000.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "3  1400.0  1200.0  1200.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "4  1400.0  1200.0   600.0  0.008  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "5  1400.0  1200.0  1000.0  1.000  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "6  1400.0  1200.0  1000.0  0.004  0.3        25.0  0.11  0.5  0.50     0.040   \n",
      "7  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.50     0.040   \n",
      "8  1400.0  1200.0  1000.0  0.008  0.6        26.0  0.11  0.5  0.50     0.040   \n",
      "9  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.50  0.5  0.50     0.040   \n",
      "\n",
      "         score  \n",
      "0 -1199.050049  \n",
      "1 -1082.969604  \n",
      "2 -1058.308838  \n",
      "3 -1092.204590  \n",
      "4 -1089.924561  \n",
      "5 -3518.319092  \n",
      "6 -1162.151855  \n",
      "7 -1014.118347  \n",
      "8 -1057.133423  \n",
      "9 -1070.859619  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 178.636218\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.5  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.5  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.5  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.5  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.5  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.5  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.6, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 179.955293\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1  ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.5  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.5  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.5  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.5  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.5  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.5  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.5  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.5  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.6  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.49, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 178.683220\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.300141\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 187.543727\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.041}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 180.626331\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.847115\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.4e+03 1.0e+03 1.1e-02 1.1e+00 2.5e+01 1.2e-01 5.0e-01 5.1e-01 4.1e-02]\n",
      "{'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.041}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 178.851230\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "Current Delta values for each dimension:  [1, 1, 1, 3, 2, 1, 3, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 171.839829\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "-1 {'layer1': 1400, 'layer2': 1100, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 168.262624\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1100, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 183.929520\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 600, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 157.198991\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.1, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.351144\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.621160\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 182.886461\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.15, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.202136\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.51, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.822114\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 178.107187\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.4e+03 1.0e+03 1.1e-02 1.1e+00 2.5e+01 1.2e-01 5.0e-01 5.1e-01 4.1e-02]\n",
      "Closest point in the Space {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.041}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 2, 1, 1, 2, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.889117\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.005, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.654161\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.5, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.745166\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.13, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 179.547270\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.835114\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "32     0.040 -1045.252686  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.4e+03 1.0e+03 1.1e-02 1.1e+00 2.5e+01 1.2e-01 5.0e-01 5.1e-01 4.1e-02]\n",
      "Closest point in the Space {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.041}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.009, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 180.593329\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.009  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "32     0.040 -1045.252686  \n",
      "33     0.040 -1019.364258  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 180.401318\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.009  0.6        25.0  0.110  0.50  0.50   \n",
      "34  1400.0  1200.0  1000.0  0.006  0.6        25.0  0.110  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "32     0.040 -1045.252686  \n",
      "33     0.040 -1019.364258  \n",
      "34     0.040 -1087.018799  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 177.155133\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.009  0.6        25.0  0.110  0.50  0.50   \n",
      "34  1400.0  1200.0  1000.0  0.006  0.6        25.0  0.110  0.50  0.50   \n",
      "35  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.120  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "32     0.040 -1045.252686  \n",
      "33     0.040 -1019.364258  \n",
      "34     0.040 -1087.018799  \n",
      "35     0.040 -1040.851196  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (20110, 18)\n",
      "3355 3351\n",
      "6706 3351\n",
      "10057 3351\n",
      "13408 3351\n",
      "16759 3351\n",
      "Cross Validation Time: 176.561099\n",
      "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
      "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
      "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
      "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
      "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
      "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
      "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
      "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
      "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
      "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
      "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
      "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
      "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
      "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
      "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
      "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.009  0.6        25.0  0.110  0.50  0.50   \n",
      "34  1400.0  1200.0  1000.0  0.006  0.6        25.0  0.110  0.50  0.50   \n",
      "35  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.120  0.50  0.50   \n",
      "36  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.100  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.039 -1199.050049  \n",
      "1      0.040 -1082.969604  \n",
      "2      0.040 -1058.308838  \n",
      "3      0.040 -1092.204590  \n",
      "4      0.040 -1089.924561  \n",
      "5      0.040 -3518.319092  \n",
      "6      0.040 -1162.151855  \n",
      "7      0.040 -1014.118347  \n",
      "8      0.040 -1057.133423  \n",
      "9      0.040 -1070.859619  \n",
      "10     0.040 -1027.032715  \n",
      "11     0.040 -1069.779541  \n",
      "12     0.040 -1033.777954  \n",
      "13     0.040 -1049.304565  \n",
      "14     0.040 -1048.075439  \n",
      "15     0.041 -1047.473511  \n",
      "16     0.039 -1047.645142  \n",
      "17     0.041 -1031.762939  \n",
      "18     0.040 -1060.008667  \n",
      "19     0.040 -1162.284180  \n",
      "20     0.040 -1108.947021  \n",
      "21     0.040 -1075.474976  \n",
      "22     0.040 -1027.766357  \n",
      "23     0.040 -1142.082397  \n",
      "24     0.040 -1033.807129  \n",
      "25     0.040 -1047.138916  \n",
      "26     0.040 -1043.223267  \n",
      "27     0.040 -1028.684937  \n",
      "28     0.040 -1033.517090  \n",
      "29     0.040 -1095.113647  \n",
      "30     0.040 -1038.399658  \n",
      "31     0.040 -1066.539795  \n",
      "32     0.040 -1045.252686  \n",
      "33     0.040 -1019.364258  \n",
      "34     0.040 -1087.018799  \n",
      "35     0.040 -1040.851196  \n",
      "36     0.040 -1046.676147  \n",
      "Theoretical Pattern Move:  [1.5e+03 1.4e+03 1.0e+03 1.1e-02 1.1e+00 2.5e+01 1.2e-01 5.0e-01 5.1e-01 4.1e-02]\n",
      "Closest point in the Space {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.041}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [1.5e+03 1.4e+03 1.0e+03 1.1e-02 1.1e+00 2.5e+01 1.2e-01 5.0e-01 5.1e-01 4.1e-02]\n",
      "Closest point in the Space {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.6, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.041}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 1400.0, 'layer2': 1200.0, 'layer3': 1000.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      " \n",
      "layer1        1400.000000\n",
      "layer2        1200.000000\n",
      "layer3        1000.000000\n",
      "lr               0.008000\n",
      "wd               0.600000\n",
      "div_factor      25.000000\n",
      "ps1              0.110000\n",
      "ps2              0.500000\n",
      "ps3              0.500000\n",
      "emb_drop         0.040000\n",
      "score        -1014.118347\n",
      "Name: 7, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>layer3</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>ps3</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1014.118347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1019.364258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1027.032715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1027.766357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1028.684937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1031.762939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1033.517090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1033.777954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1033.807129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1038.399658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1040.851196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1043.223267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1045.252686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1046.676147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1047.138916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1047.473511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1047.645142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1048.075439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1049.304565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1057.133423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1058.308838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1060.008667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1066.539795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1069.779541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1070.859619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1075.474976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1082.969604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1087.018799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1089.924561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1092.204590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1095.113647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1108.947021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1142.082397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1162.151855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1162.284180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1199.050049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-3518.319092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  layer3     lr   wd  div_factor    ps1   ps2   ps3  \\\n",
       "7   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "33  1400.0  1200.0  1000.0  0.009  0.6        25.0  0.110  0.50  0.50   \n",
       "10  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.098  0.50  0.50   \n",
       "22  1400.0  1200.0  1000.0  0.100  0.6        25.0  0.110  0.50  0.50   \n",
       "27  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.51   \n",
       "17  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.120  0.50  0.51   \n",
       "28  1400.0  1200.0  1000.0  0.010  0.6        25.0  0.110  0.50  0.50   \n",
       "12  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.49  0.50   \n",
       "24  1400.0  1200.0  1000.0  0.008  0.4        25.0  0.110  0.50  0.50   \n",
       "30  1400.0  1200.0  1000.0  0.008  0.5        25.0  0.110  0.50  0.50   \n",
       "35  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.120  0.50  0.50   \n",
       "26  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.51  0.50   \n",
       "32  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.099  0.50  0.50   \n",
       "36  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.100  0.50  0.50   \n",
       "25  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.150  0.50  0.50   \n",
       "15  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "16  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "14  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.49   \n",
       "13  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.60   \n",
       "8   1400.0  1200.0  1000.0  0.008  0.6        26.0  0.110  0.50  0.50   \n",
       "2   1400.0  1200.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
       "18  1300.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "31  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.130  0.50  0.50   \n",
       "11  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.110  0.60  0.50   \n",
       "9   1400.0  1200.0  1000.0  0.008  0.6        25.0  0.500  0.50  0.50   \n",
       "21  1400.0  1200.0   600.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "1   1400.0  1000.0  1000.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
       "34  1400.0  1200.0  1000.0  0.006  0.6        25.0  0.110  0.50  0.50   \n",
       "4   1400.0  1200.0   600.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
       "3   1400.0  1200.0  1200.0  0.008  0.3        25.0  0.110  0.50  0.50   \n",
       "29  1400.0  1200.0  1000.0  0.005  0.6        25.0  0.110  0.50  0.50   \n",
       "20  1400.0  1200.0  1100.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "23  1400.0  1200.0  1000.0  0.004  0.6        25.0  0.110  0.50  0.50   \n",
       "6   1400.0  1200.0  1000.0  0.004  0.3        25.0  0.110  0.50  0.50   \n",
       "19  1400.0  1100.0  1000.0  0.008  0.6        25.0  0.110  0.50  0.50   \n",
       "0   1300.0  1000.0  1000.0  0.005  0.1        25.0  0.100  0.50  0.49   \n",
       "5   1400.0  1200.0  1000.0  1.000  0.3        25.0  0.110  0.50  0.50   \n",
       "\n",
       "    emb_drop        score  \n",
       "7      0.040 -1014.118347  \n",
       "33     0.040 -1019.364258  \n",
       "10     0.040 -1027.032715  \n",
       "22     0.040 -1027.766357  \n",
       "27     0.040 -1028.684937  \n",
       "17     0.041 -1031.762939  \n",
       "28     0.040 -1033.517090  \n",
       "12     0.040 -1033.777954  \n",
       "24     0.040 -1033.807129  \n",
       "30     0.040 -1038.399658  \n",
       "35     0.040 -1040.851196  \n",
       "26     0.040 -1043.223267  \n",
       "32     0.040 -1045.252686  \n",
       "36     0.040 -1046.676147  \n",
       "25     0.040 -1047.138916  \n",
       "15     0.041 -1047.473511  \n",
       "16     0.039 -1047.645142  \n",
       "14     0.040 -1048.075439  \n",
       "13     0.040 -1049.304565  \n",
       "8      0.040 -1057.133423  \n",
       "2      0.040 -1058.308838  \n",
       "18     0.040 -1060.008667  \n",
       "31     0.040 -1066.539795  \n",
       "11     0.040 -1069.779541  \n",
       "9      0.040 -1070.859619  \n",
       "21     0.040 -1075.474976  \n",
       "1      0.040 -1082.969604  \n",
       "34     0.040 -1087.018799  \n",
       "4      0.040 -1089.924561  \n",
       "3      0.040 -1092.204590  \n",
       "29     0.040 -1095.113647  \n",
       "20     0.040 -1108.947021  \n",
       "23     0.040 -1142.082397  \n",
       "6      0.040 -1162.151855  \n",
       "19     0.040 -1162.284180  \n",
       "0      0.039 -1199.050049  \n",
       "5      0.040 -3518.319092  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'layer1': 1300.0, 'layer2': 1000.0, 'layer3': 1000.0, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
    "#-1074\n",
    "param_dist = {\n",
    "    'layer1':[1200, 1300, 1400], \n",
    "    'layer2':[700,1000,1100,1200], \n",
    "    'layer3':[600,1000,1100,1200], \n",
    "    'lr':[4e-3,5e-3,6e-3,8e-3,9e-3,1e-2,1e-1,1], \n",
    "    'wd':[0.1,0.2,0.3,0.4,0.5,0.6], \n",
    "    'div_factor':[25,26], \n",
    "    'ps1':[0.098,0.099,0.1,0.11,0.12,0.13,0.15,0.5],\n",
    "    'ps2':[0.5,0.49,0.51,0.6], \n",
    "    'ps3':[0.5,0.49,0.51,0.6], \n",
    "    'emb_drop':[0.04,0.039,0.041]\n",
    "}\n",
    "\n",
    "xs={'layer1': 1300.0, 'layer2': 1000, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
    "PatternSearch(param_dist, 20, alfa=1.4, metric=5, XevalMetric=mae,xs=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-30T12:18:48.613862Z",
     "start_time": "2019-06-29T22:12:05.301872Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [1200, 1300, 1400]\n",
      "layer2 : [700, 1000, 1100, 1200]\n",
      "layer3 : [600, 1000, 1100, 1200]\n",
      "lr : [0.0001, 0.004, 0.005, 0.006, 0.008, 0.009, 0.01, 0.1]\n",
      "wd : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
      "div_factor : [25, 26]\n",
      "ps1 : [0.098, 0.099, 0.1, 0.11, 0.12, 0.13, 0.15, 0.5]\n",
      "ps2 : [0.49, 0.5, 0.51, 0.6]\n",
      "ps3 : [0.49, 0.5, 0.51, 0.6]\n",
      "emb_drop : [0.039, 0.04, 0.041]\n",
      "{'layer1': 1400.0, 'layer2': 1200.0, 'layer3': 1000.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 894.918186\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 {'layer1': 1400, 'layer2': 1000, 'layer3': 1000, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 811.821434\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 895.902243\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1200, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 950.050339\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 600, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 801.278831\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.1, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 907.244891\n",
      "   layer1  layer2  layer3     lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.008  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.006  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5  1400.0  1200.0  1000.0  0.100  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "5 -1221.768311  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 900.949531\n",
      "   layer1  layer2  layer3      lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5  1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "6  1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "5 -1221.768311  \n",
      "6 -1833.396118  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.8, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 909.131999\n",
      "   layer1  layer2  layer3      lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5  1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "6  1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "7  1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "5 -1221.768311  \n",
      "6 -1833.396118  \n",
      "7 -1251.711670  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 902.896643\n",
      "   layer1  layer2  layer3      lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5  1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "6  1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "7  1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.11  0.5  0.5      0.04   \n",
      "8  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "5 -1221.768311  \n",
      "6 -1833.396118  \n",
      "7 -1251.711670  \n",
      "8 -1149.849121  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 902.831639\n",
      "   layer1  layer2  layer3      lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0  1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1  1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2  1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3  1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4  1400.0  1200.0   600.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5  1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "6  1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "7  1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.11  0.5  0.5      0.04   \n",
      "8  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.11  0.5  0.5      0.04   \n",
      "9  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.11  0.5  0.5      0.04   \n",
      "\n",
      "         score  \n",
      "0 -1235.511963  \n",
      "1 -1183.905518  \n",
      "2 -1181.590088  \n",
      "3 -1182.547607  \n",
      "4 -1210.813354  \n",
      "5 -1221.768311  \n",
      "6 -1833.396118  \n",
      "7 -1251.711670  \n",
      "8 -1149.849121  \n",
      "9 -1160.443726  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.5, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 904.265721\n",
      "    layer1  layer2  layer3      lr   wd  div_factor   ps1  ps2  ps3  emb_drop  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.11  0.5  0.5      0.04   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.11  0.5  0.5      0.04   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.11  0.5  0.5      0.04   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.11  0.5  0.5      0.04   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.11  0.5  0.5      0.04   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.50  0.5  0.5      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1235.511963  \n",
      "1  -1183.905518  \n",
      "2  -1181.590088  \n",
      "3  -1182.547607  \n",
      "4  -1210.813354  \n",
      "5  -1221.768311  \n",
      "6  -1833.396118  \n",
      "7  -1251.711670  \n",
      "8  -1149.849121  \n",
      "9  -1160.443726  \n",
      "10 -1153.510010  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 889.863897\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1  ps2  ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.5  0.5   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.5  0.5   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.5  0.5   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.5  0.5   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.5  0.5   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.5  0.5   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.5  0.5   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.5  0.5   \n",
      "\n",
      "    emb_drop        score  \n",
      "0       0.04 -1235.511963  \n",
      "1       0.04 -1183.905518  \n",
      "2       0.04 -1181.590088  \n",
      "3       0.04 -1182.547607  \n",
      "4       0.04 -1210.813354  \n",
      "5       0.04 -1221.768311  \n",
      "6       0.04 -1833.396118  \n",
      "7       0.04 -1251.711670  \n",
      "8       0.04 -1149.849121  \n",
      "9       0.04 -1160.443726  \n",
      "10      0.04 -1153.510010  \n",
      "11      0.04 -1146.624146  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.6, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 895.296208\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1  ps2  ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.5  0.5   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.5  0.5   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.5  0.5   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.5  0.5   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.5  0.5   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.5  0.5   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.5  0.5   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.5  0.5   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.5  0.5   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.6  0.5   \n",
      "\n",
      "    emb_drop        score  \n",
      "0       0.04 -1235.511963  \n",
      "1       0.04 -1183.905518  \n",
      "2       0.04 -1181.590088  \n",
      "3       0.04 -1182.547607  \n",
      "4       0.04 -1210.813354  \n",
      "5       0.04 -1221.768311  \n",
      "6       0.04 -1833.396118  \n",
      "7       0.04 -1251.711670  \n",
      "8       0.04 -1149.849121  \n",
      "9       0.04 -1160.443726  \n",
      "10      0.04 -1153.510010  \n",
      "11      0.04 -1146.624146  \n",
      "12      0.04 -1154.172363  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.49, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 913.033222\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2  ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.5   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.5   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.5   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.5   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.5   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.5   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.5   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.5   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.5   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.5   \n",
      "\n",
      "    emb_drop        score  \n",
      "0       0.04 -1235.511963  \n",
      "1       0.04 -1183.905518  \n",
      "2       0.04 -1181.590088  \n",
      "3       0.04 -1182.547607  \n",
      "4       0.04 -1210.813354  \n",
      "5       0.04 -1221.768311  \n",
      "6       0.04 -1833.396118  \n",
      "7       0.04 -1251.711670  \n",
      "8       0.04 -1149.849121  \n",
      "9       0.04 -1160.443726  \n",
      "10      0.04 -1153.510010  \n",
      "11      0.04 -1146.624146  \n",
      "12      0.04 -1154.172363  \n",
      "13      0.04 -1151.929321  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 897.342325\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2  ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.5   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.5   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.5   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.5   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.5   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.5   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.5   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.5   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.5   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.5   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.5   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.6   \n",
      "\n",
      "    emb_drop        score  \n",
      "0       0.04 -1235.511963  \n",
      "1       0.04 -1183.905518  \n",
      "2       0.04 -1181.590088  \n",
      "3       0.04 -1182.547607  \n",
      "4       0.04 -1210.813354  \n",
      "5       0.04 -1221.768311  \n",
      "6       0.04 -1833.396118  \n",
      "7       0.04 -1251.711670  \n",
      "8       0.04 -1149.849121  \n",
      "9       0.04 -1160.443726  \n",
      "10      0.04 -1153.510010  \n",
      "11      0.04 -1146.624146  \n",
      "12      0.04 -1154.172363  \n",
      "13      0.04 -1151.929321  \n",
      "14      0.04 -1149.558105  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 902.961646\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0       0.04 -1235.511963  \n",
      "1       0.04 -1183.905518  \n",
      "2       0.04 -1181.590088  \n",
      "3       0.04 -1182.547607  \n",
      "4       0.04 -1210.813354  \n",
      "5       0.04 -1221.768311  \n",
      "6       0.04 -1833.396118  \n",
      "7       0.04 -1251.711670  \n",
      "8       0.04 -1149.849121  \n",
      "9       0.04 -1160.443726  \n",
      "10      0.04 -1153.510010  \n",
      "11      0.04 -1146.624146  \n",
      "12      0.04 -1154.172363  \n",
      "13      0.04 -1151.929321  \n",
      "14      0.04 -1149.558105  \n",
      "15      0.04 -1146.179321  \n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.041}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 888.361811\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "-1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 895.729233\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "Theoretical Pattern Move:  [ 1.4e+03  1.2e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  8.6e-02  5.0e-01  4.8e-01  3.8e-02]\n",
      "{'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 899.180430\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "Current Delta values for each dimension:  [1, 1, 1, 3, 3, 1, 3, 1, 1, 1]\n",
      "10\n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 887.705774\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "-1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 836.084821\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 925.250921\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 600, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 742.712687\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.01, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 853.618824\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 851.974730\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 859.297149\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 850.811664\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 854.437871\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.51, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 847.863495\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.49, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 854.944900\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 852.559764\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 855.194915\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "Theoretical Pattern Move:  [ 1.2e+03  1.2e+03  1.0e+03  4.0e-03 -4.0e-01  2.7e+01  8.6e-02  5.0e-01  5.0e-01  3.8e-02]\n",
      "{'layer1': 1200, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 839.504017\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "Current Delta values for each dimension:  [1, 1, 1, 2, 2, 1, 2, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 892.409043\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "-1 {'layer1': 1200, 'layer2': 1200, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 823.765116\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "-1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 806.980156\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 866.925585\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 600, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 741.028384\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.009, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 849.020561\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 892.878070\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.3, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 851.840722\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 855.630939\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.1, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 855.771947\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.51, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 853.417813\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.49, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 852.280748\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.51, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 856.445986\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.49, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 850.804663\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 850.752660\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "Theoretical Pattern Move:  [ 1.2e+03  1.2e+03  1.0e+03  0.0e+00 -4.0e-01  2.7e+01  8.6e-02  5.0e-01  5.0e-01  3.8e-02]\n",
      "{'layer1': 1200, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 825.637224\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "1 {'layer1': 1400, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 884.952616\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "-1 {'layer1': 1300, 'layer2': 1100, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 804.917038\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1100, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 870.930815\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 600, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 784.477870\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "52     0.039 -1168.148560  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.005, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 860.045192\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "53  1300.0  1200.0  1000.0  0.0050  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "52     0.039 -1168.148560  \n",
      "53     0.039 -1140.973877  \n",
      "-1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 853.103795\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "53  1300.0  1200.0  1000.0  0.0050  0.1        26.0  0.098  0.50  0.50   \n",
      "54  1300.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "52     0.039 -1168.148560  \n",
      "53     0.039 -1140.973877  \n",
      "54     0.039 -2171.517578  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.2, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 849.586594\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "53  1300.0  1200.0  1000.0  0.0050  0.1        26.0  0.098  0.50  0.50   \n",
      "54  1300.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "55  1300.0  1200.0  1000.0  0.0040  0.2        26.0  0.098  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "52     0.039 -1168.148560  \n",
      "53     0.039 -1140.973877  \n",
      "54     0.039 -2171.517578  \n",
      "55     0.039 -1141.483154  \n",
      "1 {'layer1': 1300, 'layer2': 1200, 'layer3': 1000, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.099, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 858.323093\n",
      "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
      "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
      "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
      "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
      "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
      "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
      "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
      "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
      "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
      "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
      "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
      "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
      "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
      "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
      "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
      "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
      "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
      "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
      "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
      "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
      "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
      "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
      "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
      "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
      "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
      "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
      "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
      "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
      "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
      "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
      "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
      "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
      "53  1300.0  1200.0  1000.0  0.0050  0.1        26.0  0.098  0.50  0.50   \n",
      "54  1300.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
      "55  1300.0  1200.0  1000.0  0.0040  0.2        26.0  0.098  0.50  0.50   \n",
      "56  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.099  0.50  0.50   \n",
      "\n",
      "    emb_drop        score  \n",
      "0      0.040 -1235.511963  \n",
      "1      0.040 -1183.905518  \n",
      "2      0.040 -1181.590088  \n",
      "3      0.040 -1182.547607  \n",
      "4      0.040 -1210.813354  \n",
      "5      0.040 -1221.768311  \n",
      "6      0.040 -1833.396118  \n",
      "7      0.040 -1251.711670  \n",
      "8      0.040 -1149.849121  \n",
      "9      0.040 -1160.443726  \n",
      "10     0.040 -1153.510010  \n",
      "11     0.040 -1146.624146  \n",
      "12     0.040 -1154.172363  \n",
      "13     0.040 -1151.929321  \n",
      "14     0.040 -1149.558105  \n",
      "15     0.040 -1146.179321  \n",
      "16     0.041 -1146.312134  \n",
      "17     0.039 -1145.300049  \n",
      "18     0.039 -1145.315063  \n",
      "19     0.039 -1141.213135  \n",
      "20     0.039 -1168.520874  \n",
      "21     0.039 -1164.845947  \n",
      "22     0.039 -1181.017334  \n",
      "23     0.039 -1153.959961  \n",
      "24     0.039 -2026.009766  \n",
      "25     0.039 -1175.904541  \n",
      "26     0.039 -1137.673828  \n",
      "27     0.039 -1139.781982  \n",
      "28     0.039 -1156.843506  \n",
      "29     0.039 -1142.578125  \n",
      "30     0.039 -1132.548706  \n",
      "31     0.040 -1138.993774  \n",
      "32     0.039 -1148.864990  \n",
      "33     0.039 -1140.949585  \n",
      "34     0.039 -1153.373779  \n",
      "35     0.039 -1169.913696  \n",
      "36     0.039 -1172.640869  \n",
      "37     0.039 -1170.830811  \n",
      "38     0.039 -1143.406250  \n",
      "39     0.039 -1128.493652  \n",
      "40     0.039 -1148.329102  \n",
      "41     0.039 -1144.076904  \n",
      "42     0.039 -1132.333252  \n",
      "43     0.039 -1141.690063  \n",
      "44     0.039 -1147.592163  \n",
      "45     0.039 -1137.516357  \n",
      "46     0.039 -1148.930908  \n",
      "47     0.040 -1141.891602  \n",
      "48     0.039 -1959.820923  \n",
      "49     0.039 -1146.831421  \n",
      "50     0.039 -1166.829834  \n",
      "51     0.039 -1159.933350  \n",
      "52     0.039 -1168.148560  \n",
      "53     0.039 -1140.973877  \n",
      "54     0.039 -2171.517578  \n",
      "55     0.039 -1141.483154  \n",
      "56     0.039 -1153.551514  \n",
      "Theoretical Pattern Move:  [ 1.2e+03  1.2e+03  1.0e+03  0.0e+00 -4.0e-01  2.7e+01  8.6e-02  5.0e-01  5.0e-01  3.8e-02]\n",
      "Closest point in the Space {'layer1': 1200, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Theoretical Pattern Move:  [ 1.2e+03  1.2e+03  1.0e+03  0.0e+00 -4.0e-01  2.7e+01  8.6e-02  5.0e-01  5.0e-01  3.8e-02]\n",
      "Closest point in the Space {'layer1': 1200, 'layer2': 1200, 'layer3': 1000, 'lr': 0.0001, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "10\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 1300.0, 'layer2': 1200.0, 'layer3': 1000.0, 'lr': 0.004, 'wd': 0.1, 'div_factor': 26.0, 'ps1': 0.098, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.039}\n",
      " \n",
      "layer1        1300.000000\n",
      "layer2        1200.000000\n",
      "layer3        1000.000000\n",
      "lr               0.004000\n",
      "wd               0.100000\n",
      "div_factor      26.000000\n",
      "ps1              0.098000\n",
      "ps2              0.500000\n",
      "ps3              0.500000\n",
      "emb_drop         0.039000\n",
      "score        -1128.493652\n",
      "Name: 39, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>layer3</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>ps3</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1128.493652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1132.333252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1132.548706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1137.516357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1137.673828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1138.993774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1139.781982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1140.949585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1140.973877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1141.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1141.483154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1141.690063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1141.891602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1142.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1143.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1144.076904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1145.300049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1145.315063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1146.179321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1146.312134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1146.624146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1146.831421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1147.592163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1148.329102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1148.864990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1148.930908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1149.558105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1149.849121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1151.929321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1153.373779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1153.510010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1153.551514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1153.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1154.172363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1156.843506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1159.933350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1160.443726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1164.845947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1166.829834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1168.148560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1168.520874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1169.913696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1170.830811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1172.640869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1175.904541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1181.017334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1181.590088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1182.547607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1183.905518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1210.813354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1221.768311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1235.511963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1251.711670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1833.396118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1200.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1959.820923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-2026.009766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-2171.517578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2  layer3      lr   wd  div_factor    ps1   ps2   ps3  \\\n",
       "39  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "42  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.100  0.50  0.50   \n",
       "30  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "45  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.51   \n",
       "26  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.49   \n",
       "31  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "27  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.49   \n",
       "33  1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "53  1300.0  1200.0  1000.0  0.0050  0.1        26.0  0.098  0.50  0.50   \n",
       "19  1300.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "55  1300.0  1200.0  1000.0  0.0040  0.2        26.0  0.098  0.50  0.50   \n",
       "43  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.51  0.50   \n",
       "47  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "29  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.49  0.49   \n",
       "38  1300.0  1200.0  1000.0  0.0090  0.1        26.0  0.098  0.50  0.50   \n",
       "41  1300.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.50   \n",
       "17  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "18  1400.0  1200.0  1000.0  0.0040  0.1        25.0  0.098  0.50  0.49   \n",
       "15  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "16  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "11  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.50   \n",
       "49  1400.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "44  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.49  0.50   \n",
       "40  1300.0  1200.0  1000.0  0.0040  0.3        26.0  0.098  0.50  0.50   \n",
       "32  1200.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "46  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.49   \n",
       "14  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.60   \n",
       "8   1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.110  0.50  0.50   \n",
       "13  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.49  0.50   \n",
       "34  1200.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "10  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.500  0.50  0.50   \n",
       "56  1300.0  1200.0  1000.0  0.0040  0.1        26.0  0.099  0.50  0.50   \n",
       "23  1300.0  1200.0  1000.0  0.0100  0.1        25.0  0.098  0.50  0.49   \n",
       "12  1400.0  1200.0  1000.0  0.0060  0.1        25.0  0.098  0.60  0.50   \n",
       "28  1300.0  1200.0  1000.0  0.0060  0.1        26.0  0.098  0.51  0.49   \n",
       "51  1300.0  1200.0  1100.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "9   1400.0  1200.0  1000.0  0.0060  0.1        26.0  0.110  0.50  0.50   \n",
       "21  1300.0  1200.0  1100.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "50  1300.0  1100.0  1000.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "52  1300.0  1200.0   600.0  0.0040  0.1        26.0  0.098  0.50  0.50   \n",
       "20  1300.0  1100.0  1000.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "35  1300.0  1100.0  1000.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "37  1300.0  1200.0   600.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "36  1300.0  1200.0  1100.0  0.0060  0.1        26.0  0.098  0.50  0.50   \n",
       "25  1300.0  1200.0  1000.0  0.0060  0.4        25.0  0.098  0.50  0.49   \n",
       "22  1300.0  1200.0   600.0  0.0060  0.1        25.0  0.098  0.50  0.49   \n",
       "2   1400.0  1200.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
       "3   1400.0  1200.0  1200.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
       "1   1400.0  1000.0  1000.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
       "4   1400.0  1200.0   600.0  0.0060  0.4        25.0  0.110  0.50  0.50   \n",
       "5   1400.0  1200.0  1000.0  0.1000  0.4        25.0  0.110  0.50  0.50   \n",
       "0   1400.0  1200.0  1000.0  0.0080  0.6        25.0  0.110  0.50  0.50   \n",
       "7   1400.0  1200.0  1000.0  0.0060  0.8        25.0  0.110  0.50  0.50   \n",
       "6   1400.0  1200.0  1000.0  0.0001  0.4        25.0  0.110  0.50  0.50   \n",
       "48  1200.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
       "24  1300.0  1200.0  1000.0  0.0001  0.1        25.0  0.098  0.50  0.49   \n",
       "54  1300.0  1200.0  1000.0  0.0001  0.1        26.0  0.098  0.50  0.50   \n",
       "\n",
       "    emb_drop        score  \n",
       "39     0.039 -1128.493652  \n",
       "42     0.039 -1132.333252  \n",
       "30     0.039 -1132.548706  \n",
       "45     0.039 -1137.516357  \n",
       "26     0.039 -1137.673828  \n",
       "31     0.040 -1138.993774  \n",
       "27     0.039 -1139.781982  \n",
       "33     0.039 -1140.949585  \n",
       "53     0.039 -1140.973877  \n",
       "19     0.039 -1141.213135  \n",
       "55     0.039 -1141.483154  \n",
       "43     0.039 -1141.690063  \n",
       "47     0.040 -1141.891602  \n",
       "29     0.039 -1142.578125  \n",
       "38     0.039 -1143.406250  \n",
       "41     0.039 -1144.076904  \n",
       "17     0.039 -1145.300049  \n",
       "18     0.039 -1145.315063  \n",
       "15     0.040 -1146.179321  \n",
       "16     0.041 -1146.312134  \n",
       "11     0.040 -1146.624146  \n",
       "49     0.039 -1146.831421  \n",
       "44     0.039 -1147.592163  \n",
       "40     0.039 -1148.329102  \n",
       "32     0.039 -1148.864990  \n",
       "46     0.039 -1148.930908  \n",
       "14     0.040 -1149.558105  \n",
       "8      0.040 -1149.849121  \n",
       "13     0.040 -1151.929321  \n",
       "34     0.039 -1153.373779  \n",
       "10     0.040 -1153.510010  \n",
       "56     0.039 -1153.551514  \n",
       "23     0.039 -1153.959961  \n",
       "12     0.040 -1154.172363  \n",
       "28     0.039 -1156.843506  \n",
       "51     0.039 -1159.933350  \n",
       "9      0.040 -1160.443726  \n",
       "21     0.039 -1164.845947  \n",
       "50     0.039 -1166.829834  \n",
       "52     0.039 -1168.148560  \n",
       "20     0.039 -1168.520874  \n",
       "35     0.039 -1169.913696  \n",
       "37     0.039 -1170.830811  \n",
       "36     0.039 -1172.640869  \n",
       "25     0.039 -1175.904541  \n",
       "22     0.039 -1181.017334  \n",
       "2      0.040 -1181.590088  \n",
       "3      0.040 -1182.547607  \n",
       "1      0.040 -1183.905518  \n",
       "4      0.040 -1210.813354  \n",
       "5      0.040 -1221.768311  \n",
       "0      0.040 -1235.511963  \n",
       "7      0.040 -1251.711670  \n",
       "6      0.040 -1833.396118  \n",
       "48     0.039 -1959.820923  \n",
       "24     0.039 -2026.009766  \n",
       "54     0.039 -2171.517578  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'layer1':[1200, 1300, 1400], \n",
    "    'layer2':[700,1000,1100,1200], \n",
    "    'layer3':[600,1000,1100,1200], \n",
    "    'lr':[1e-4,4e-3,5e-3,6e-3,8e-3,9e-3,1e-2,1e-1], \n",
    "    'wd':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], \n",
    "    'div_factor':[25,26], \n",
    "    'ps1':[0.098,0.099,0.1,0.11,0.12,0.13,0.15,0.5],\n",
    "    'ps2':[0.5,0.49,0.51,0.6], \n",
    "    'ps3':[0.5,0.49,0.51,0.6], \n",
    "    'emb_drop':[0.04,0.039,0.041]\n",
    "}\n",
    "\n",
    "xs={'layer1': 1400.0, 'layer2': 1200.0, 'layer3': 1000.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'ps3': 0.5, 'emb_drop': 0.04}\n",
    "PatternSearch(param_dist, 100, alfa=1.4, metric=5, XevalMetric=mae,xs=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T21:47:57.995091Z",
     "start_time": "2019-06-29T21:47:45.630384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "BestLearn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T21:48:18.901287Z",
     "start_time": "2019-06-29T21:48:17.979234Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BestLearn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T12:37:29.038966Z",
     "start_time": "2019-07-03T22:52:57.346348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1 : [1000, 1200, 1300, 1400, 2000]\n",
      "layer2 : [500, 700, 1000, 1100, 1200, 1600, 2000]\n",
      "lr : [0.0001, 0.001, 0.003, 0.005, 0.006, 0.008, 0.009, 0.01, 0.1]\n",
      "wd : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
      "div_factor : [25, 26]\n",
      "ps1 : [0.001, 0.098, 0.099, 0.1, 0.11, 0.12, 0.13, 0.15, 0.5, 0.8, 0.9]\n",
      "ps2 : [0.01, 0.49, 0.5, 0.51, 0.6]\n",
      "emb_drop : [0.03, 0.039, 0.04, 0.041, 0.05]\n",
      "{'layer1': 1400.0, 'layer2': 1200.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 620.579495\n",
      "   layer1  layer2     lr   wd  div_factor   ps1  ps2  emb_drop       score\n",
      "0  1400.0  1200.0  0.008  0.6        25.0  0.11  0.5      0.04 -1075.94165\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 772.828203\n",
      "   layer1  layer2     lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.008  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.006  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "1 {'layer1': 2000, 'layer2': 2000, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 1092.185469\n",
      "   layer1  layer2     lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.008  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.006  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.006  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "-1 {'layer1': 2000, 'layer2': 500, 'lr': 0.006, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 544.178125\n",
      "   layer1  layer2     lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.008  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.006  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.006  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.006  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.1, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 12658.277929\n",
      "   layer1  layer2     lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.008  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.006  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.006  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.006  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.100  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.0001, 'wd': 0.4, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 811.360407\n",
      "   layer1  layer2      lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.0080  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.1000  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "5  2000.0  1100.0  0.0001  0.4        25.0  0.12  0.5      0.04 -1630.354492\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.8, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 810.367350\n",
      "   layer1  layer2      lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.0080  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.1000  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "5  2000.0  1100.0  0.0001  0.4        25.0  0.12  0.5      0.04 -1630.354492\n",
      "6  2000.0  1100.0  0.0060  0.8        25.0  0.12  0.5      0.04 -1093.173950\n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 758.909407\n",
      "   layer1  layer2      lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.0080  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.1000  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "5  2000.0  1100.0  0.0001  0.4        25.0  0.12  0.5      0.04 -1630.354492\n",
      "6  2000.0  1100.0  0.0060  0.8        25.0  0.12  0.5      0.04 -1093.173950\n",
      "7  2000.0  1100.0  0.0060  0.1        25.0  0.12  0.5      0.04 -1014.079102\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 768.362030\n",
      "   layer1  layer2      lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.0080  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.1000  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "5  2000.0  1100.0  0.0001  0.4        25.0  0.12  0.5      0.04 -1630.354492\n",
      "6  2000.0  1100.0  0.0060  0.8        25.0  0.12  0.5      0.04 -1093.173950\n",
      "7  2000.0  1100.0  0.0060  0.1        25.0  0.12  0.5      0.04 -1014.079102\n",
      "8  2000.0  1100.0  0.0060  0.1        26.0  0.12  0.5      0.04 -1022.683594\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.9, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 801.279831\n",
      "   layer1  layer2      lr   wd  div_factor   ps1  ps2  emb_drop        score\n",
      "0  1400.0  1200.0  0.0080  0.6        25.0  0.11  0.5      0.04 -1075.941650\n",
      "1  2000.0  1100.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1051.209839\n",
      "2  2000.0  2000.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1082.475830\n",
      "3  2000.0   500.0  0.0060  0.4        25.0  0.12  0.5      0.04 -1068.104858\n",
      "4  2000.0  1100.0  0.1000  0.4        25.0  0.12  0.5      0.04 -1132.706909\n",
      "5  2000.0  1100.0  0.0001  0.4        25.0  0.12  0.5      0.04 -1630.354492\n",
      "6  2000.0  1100.0  0.0060  0.8        25.0  0.12  0.5      0.04 -1093.173950\n",
      "7  2000.0  1100.0  0.0060  0.1        25.0  0.12  0.5      0.04 -1014.079102\n",
      "8  2000.0  1100.0  0.0060  0.1        26.0  0.12  0.5      0.04 -1022.683594\n",
      "9  2000.0  1100.0  0.0060  0.1        25.0  0.90  0.5      0.04 -1070.824829\n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.001, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 762.589617\n",
      "    layer1  layer2      lr   wd  div_factor    ps1  ps2  emb_drop        score\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.5      0.04 -1075.941650\n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1051.209839\n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1082.475830\n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1068.104858\n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.5      0.04 -1132.706909\n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.5      0.04 -1630.354492\n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.5      0.04 -1093.173950\n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.5      0.04 -1014.079102\n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.5      0.04 -1022.683594\n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.5      0.04 -1070.824829\n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.5      0.04 -1018.992920\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.6, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 759.171422\n",
      "    layer1  layer2      lr   wd  div_factor    ps1  ps2  emb_drop        score\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.5      0.04 -1075.941650\n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1051.209839\n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1082.475830\n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.5      0.04 -1068.104858\n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.5      0.04 -1132.706909\n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.5      0.04 -1630.354492\n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.5      0.04 -1093.173950\n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.5      0.04 -1014.079102\n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.5      0.04 -1022.683594\n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.5      0.04 -1070.824829\n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.5      0.04 -1018.992920\n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.6      0.04 -1023.623779\n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.01, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 761.061531\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.05}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 757.432323\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.03}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 759.446438\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  1.3e-01  5.0e-01  4.0e-02]\n",
      "{'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.13, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 721.387261\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "Current Delta values for each dimension:  [1, 2, 3, 3, 1, 4, 1, 1]\n",
      "8\n",
      "-1 {'layer1': 1400, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 601.493403\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "1 {'layer1': 2000, 'layer2': 1600, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 954.377587\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "-1 {'layer1': 2000, 'layer2': 700, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 612.518034\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.01, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 758.347375\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.001, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 765.446781\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.8, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 762.962639\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50      0.04   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 797.447611\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50      0.04   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50      0.04   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.51, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 760.027471\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50      0.04   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50      0.04   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50      0.04   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 761.789572\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50      0.04   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50      0.04   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50      0.04   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50      0.04   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50      0.04   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50      0.04   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50      0.04   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50      0.04   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60      0.04   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01      0.04   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.05   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.03   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50      0.04   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50      0.04   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50      0.04   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50      0.04   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50      0.04   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50      0.04   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51      0.04   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49      0.04   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.041}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 765.834803\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 764.929751\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  1.3e-01  5.0e-01  4.0e-02]\n",
      "Closest point in the Space {'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.13, 'ps2': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 2, 2, 1, 3, 1, 1]\n",
      "8\n",
      "1 {'layer1': 2000, 'layer2': 1200, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 803.250943\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "-1 {'layer1': 2000, 'layer2': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 715.025897\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.009, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 765.918808\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 763.003641\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.3, 'div_factor': 25, 'ps1': 0.12, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 769.577017\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.5, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 759.591446\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 789.350148\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.51, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 749.507869\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.49, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 748.562815\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.041}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 751.837003\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.039}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 748.353803\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  8.8e-02  5.0e-01  4.0e-02]\n",
      "{'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Executing Pattern Move...\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 713.215793\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 2, 1, 1]\n",
      "8\n",
      "-1 {'layer1': 1400, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 594.173985\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "1 {'layer1': 2000, 'layer2': 1200, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 796.591562\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "-1 {'layer1': 2000, 'layer2': 1000, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 715.003896\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.008, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 758.894406\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "-1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.005, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 755.144192\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "43 -1019.806152  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.2, 'div_factor': 25, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 752.975068\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
      "44  2000.0  1100.0  0.0060  0.2        25.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "43 -1019.806152  \n",
      "44 -1024.884399  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 26, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 763.768685\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
      "44  2000.0  1100.0  0.0060  0.2        25.0  0.099  0.50     0.040   \n",
      "45  2000.0  1100.0  0.0060  0.1        26.0  0.099  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "43 -1019.806152  \n",
      "44 -1024.884399  \n",
      "45 -1017.966125  \n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.11, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n",
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 754.418150\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
      "44  2000.0  1100.0  0.0060  0.2        25.0  0.099  0.50     0.040   \n",
      "45  2000.0  1100.0  0.0060  0.1        26.0  0.099  0.50     0.040   \n",
      "46  2000.0  1100.0  0.0060  0.1        25.0  0.110  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "43 -1019.806152  \n",
      "44 -1024.884399  \n",
      "45 -1017.966125  \n",
      "46 -1021.327820  \n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  8.8e-02  5.0e-01  4.0e-02]\n",
      "Closest point in the Space {'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "8\n",
      "1 {'layer1': 2000, 'layer2': 1100, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.1, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      "Current Model:  Fast.ai Tabular\n",
      "Training data set shape: (100554, 18)\n",
      "16759 16759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33518 16759\n",
      "50277 16759\n",
      "67036 16759\n",
      "83795 16759\n",
      "Cross Validation Time: 760.711510\n",
      "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
      "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
      "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
      "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
      "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
      "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
      "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
      "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
      "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
      "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
      "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
      "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
      "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
      "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
      "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
      "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
      "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
      "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
      "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
      "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
      "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
      "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
      "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
      "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
      "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
      "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
      "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
      "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
      "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
      "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
      "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
      "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
      "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
      "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
      "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
      "44  2000.0  1100.0  0.0060  0.2        25.0  0.099  0.50     0.040   \n",
      "45  2000.0  1100.0  0.0060  0.1        26.0  0.099  0.50     0.040   \n",
      "46  2000.0  1100.0  0.0060  0.1        25.0  0.110  0.50     0.040   \n",
      "47  2000.0  1100.0  0.0060  0.1        25.0  0.100  0.50     0.040   \n",
      "\n",
      "          score  \n",
      "0  -1075.941650  \n",
      "1  -1051.209839  \n",
      "2  -1082.475830  \n",
      "3  -1068.104858  \n",
      "4  -1132.706909  \n",
      "5  -1630.354492  \n",
      "6  -1093.173950  \n",
      "7  -1014.079102  \n",
      "8  -1022.683594  \n",
      "9  -1070.824829  \n",
      "10 -1018.992920  \n",
      "11 -1023.623779  \n",
      "12 -1025.400757  \n",
      "13 -1015.726074  \n",
      "14 -1018.601746  \n",
      "15 -1046.517334  \n",
      "16 -1026.614136  \n",
      "17 -1047.360718  \n",
      "18 -1029.865601  \n",
      "19 -1018.761169  \n",
      "20 -1103.043091  \n",
      "21 -1041.244141  \n",
      "22 -1020.553711  \n",
      "23 -1018.606995  \n",
      "24 -1018.138000  \n",
      "25 -1020.141113  \n",
      "26 -1016.209656  \n",
      "27 -1021.015930  \n",
      "28 -1036.740479  \n",
      "29 -1023.992249  \n",
      "30 -1039.564331  \n",
      "31 -1035.260620  \n",
      "32 -1021.668213  \n",
      "33 -1012.205872  \n",
      "34 -1020.187500  \n",
      "35 -1021.362061  \n",
      "36 -1023.370422  \n",
      "37 -1021.698914  \n",
      "38 -1040.247925  \n",
      "39 -1020.637268  \n",
      "40 -1019.271484  \n",
      "41 -1032.003174  \n",
      "42 -1022.110840  \n",
      "43 -1019.806152  \n",
      "44 -1024.884399  \n",
      "45 -1017.966125  \n",
      "46 -1021.327820  \n",
      "47 -1017.615967  \n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  8.8e-02  5.0e-01  4.0e-02]\n",
      "Closest point in the Space {'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "8\n",
      "Theoretical Pattern Move:  [ 2.6e+03  1.0e+03  4.0e-03 -4.0e-01  2.5e+01  8.8e-02  5.0e-01  4.0e-02]\n",
      "Closest point in the Space {'layer1': 2000, 'layer2': 1000, 'lr': 0.003, 'wd': 0.1, 'div_factor': 25, 'ps1': 0.098, 'ps2': 0.5, 'emb_drop': 0.04}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "8\n",
      "Best Parameters Found:\n",
      " \n",
      "{'layer1': 2000.0, 'layer2': 1100.0, 'lr': 0.006, 'wd': 0.1, 'div_factor': 25.0, 'ps1': 0.099, 'ps2': 0.5, 'emb_drop': 0.04}\n",
      " \n",
      "layer1        2000.000000\n",
      "layer2        1100.000000\n",
      "lr               0.006000\n",
      "wd               0.100000\n",
      "div_factor      25.000000\n",
      "ps1              0.099000\n",
      "ps2              0.500000\n",
      "emb_drop         0.040000\n",
      "score        -1012.205872\n",
      "Name: 33, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer1</th>\n",
       "      <th>layer2</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>ps1</th>\n",
       "      <th>ps2</th>\n",
       "      <th>emb_drop</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1012.205872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1014.079102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-1015.726074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1016.209656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1017.615967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1017.966125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1018.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-1018.601746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1018.606995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1018.761169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1018.992920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1019.271484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1019.806152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1020.141113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1020.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1020.553711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1020.637268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1021.015930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1021.327820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1021.362061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1021.668213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-1021.698914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1022.110840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1022.683594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-1023.370422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1023.623779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1023.992249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1024.884399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1025.400757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1026.614136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1029.865601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1032.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1035.260620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1036.740479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1039.564331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1040.247925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1041.244141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1046.517334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1047.360718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1051.209839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1068.104858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1070.824829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1075.941650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1082.475830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1093.173950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1103.043091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1132.706909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1630.354492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer1  layer2      lr   wd  div_factor    ps1   ps2  emb_drop  \\\n",
       "33  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
       "7   2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "13  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.050   \n",
       "26  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.039   \n",
       "47  2000.0  1100.0  0.0060  0.1        25.0  0.100  0.50     0.040   \n",
       "45  2000.0  1100.0  0.0060  0.1        26.0  0.099  0.50     0.040   \n",
       "24  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.49     0.040   \n",
       "14  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.030   \n",
       "23  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.51     0.040   \n",
       "19  2000.0  1100.0  0.0100  0.1        25.0  0.120  0.50     0.040   \n",
       "10  2000.0  1100.0  0.0060  0.1        25.0  0.001  0.50     0.040   \n",
       "40  2000.0  1200.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
       "43  2000.0  1100.0  0.0050  0.1        25.0  0.099  0.50     0.040   \n",
       "25  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.041   \n",
       "34  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.51     0.040   \n",
       "22  2000.0  1100.0  0.0060  0.1        25.0  0.098  0.50     0.040   \n",
       "39  1400.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
       "27  2000.0  1200.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "46  2000.0  1100.0  0.0060  0.1        25.0  0.110  0.50     0.040   \n",
       "35  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.49     0.040   \n",
       "32  2000.0  1100.0  0.0060  0.1        25.0  0.500  0.50     0.040   \n",
       "37  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.039   \n",
       "42  2000.0  1100.0  0.0080  0.1        25.0  0.099  0.50     0.040   \n",
       "8   2000.0  1100.0  0.0060  0.1        26.0  0.120  0.50     0.040   \n",
       "36  2000.0  1100.0  0.0060  0.1        25.0  0.099  0.50     0.041   \n",
       "11  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.60     0.040   \n",
       "29  2000.0  1100.0  0.0090  0.1        25.0  0.120  0.50     0.040   \n",
       "44  2000.0  1100.0  0.0060  0.2        25.0  0.099  0.50     0.040   \n",
       "12  2000.0  1100.0  0.0060  0.1        25.0  0.120  0.01     0.040   \n",
       "16  1400.0  1100.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "18  2000.0   700.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "41  2000.0  1000.0  0.0060  0.1        25.0  0.099  0.50     0.040   \n",
       "31  2000.0  1100.0  0.0060  0.3        25.0  0.120  0.50     0.040   \n",
       "28  2000.0  1000.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "30  2000.0  1100.0  0.0030  0.1        25.0  0.120  0.50     0.040   \n",
       "38  2000.0  1000.0  0.0030  0.1        25.0  0.098  0.50     0.040   \n",
       "21  2000.0  1100.0  0.0060  0.1        25.0  0.800  0.50     0.040   \n",
       "15  2000.0  1000.0  0.0030  0.1        25.0  0.130  0.50     0.040   \n",
       "17  2000.0  1600.0  0.0060  0.1        25.0  0.120  0.50     0.040   \n",
       "1   2000.0  1100.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
       "3   2000.0   500.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
       "9   2000.0  1100.0  0.0060  0.1        25.0  0.900  0.50     0.040   \n",
       "0   1400.0  1200.0  0.0080  0.6        25.0  0.110  0.50     0.040   \n",
       "2   2000.0  2000.0  0.0060  0.4        25.0  0.120  0.50     0.040   \n",
       "6   2000.0  1100.0  0.0060  0.8        25.0  0.120  0.50     0.040   \n",
       "20  2000.0  1100.0  0.0010  0.1        25.0  0.120  0.50     0.040   \n",
       "4   2000.0  1100.0  0.1000  0.4        25.0  0.120  0.50     0.040   \n",
       "5   2000.0  1100.0  0.0001  0.4        25.0  0.120  0.50     0.040   \n",
       "\n",
       "          score  \n",
       "33 -1012.205872  \n",
       "7  -1014.079102  \n",
       "13 -1015.726074  \n",
       "26 -1016.209656  \n",
       "47 -1017.615967  \n",
       "45 -1017.966125  \n",
       "24 -1018.138000  \n",
       "14 -1018.601746  \n",
       "23 -1018.606995  \n",
       "19 -1018.761169  \n",
       "10 -1018.992920  \n",
       "40 -1019.271484  \n",
       "43 -1019.806152  \n",
       "25 -1020.141113  \n",
       "34 -1020.187500  \n",
       "22 -1020.553711  \n",
       "39 -1020.637268  \n",
       "27 -1021.015930  \n",
       "46 -1021.327820  \n",
       "35 -1021.362061  \n",
       "32 -1021.668213  \n",
       "37 -1021.698914  \n",
       "42 -1022.110840  \n",
       "8  -1022.683594  \n",
       "36 -1023.370422  \n",
       "11 -1023.623779  \n",
       "29 -1023.992249  \n",
       "44 -1024.884399  \n",
       "12 -1025.400757  \n",
       "16 -1026.614136  \n",
       "18 -1029.865601  \n",
       "41 -1032.003174  \n",
       "31 -1035.260620  \n",
       "28 -1036.740479  \n",
       "30 -1039.564331  \n",
       "38 -1040.247925  \n",
       "21 -1041.244141  \n",
       "15 -1046.517334  \n",
       "17 -1047.360718  \n",
       "1  -1051.209839  \n",
       "3  -1068.104858  \n",
       "9  -1070.824829  \n",
       "0  -1075.941650  \n",
       "2  -1082.475830  \n",
       "6  -1093.173950  \n",
       "20 -1103.043091  \n",
       "4  -1132.706909  \n",
       "5  -1630.354492  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Two Layer tests:\n",
    " \n",
    "param_dist = {\n",
    "    'layer1':[1000,1200, 1300, 1400, 2000], \n",
    "    'layer2':[500,700,1000,1100,1200,1600, 2000], \n",
    "    'lr':[1e-4,1e-3,3e-3,5e-3,6e-3,8e-3,9e-3,1e-2,1e-1], \n",
    "    'wd':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], \n",
    "    'div_factor':[25,26], \n",
    "    'ps1':[0.001,0.8,0.9,0.098,0.099,0.1,0.11,0.12,0.13,0.15,0.5],\n",
    "    'ps2':[0.01,0.5,0.49,0.51,0.6], \n",
    "    'emb_drop':[0.03,0.04,0.039,0.041,0.05]\n",
    "}\n",
    "\n",
    "xs={'layer1': 1400.0, 'layer2': 1200.0, 'lr': 0.008, 'wd': 0.6, 'div_factor': 25.0, 'ps1': 0.11, 'ps2': 0.5, 'emb_drop': 0.04}\n",
    "PatternSearch(param_dist, 100, alfa=1.4, metric=7, XevalMetric=mae,xs=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T12:42:40.292769Z",
     "start_time": "2019-07-04T12:42:39.573728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "331.209px",
    "left": "729.302px",
    "right": "20px",
    "top": "111px",
    "width": "393.332px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
