{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param Opt w/ XGBoost and H20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training & Validation Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:04:49.938238Z",
     "start_time": "2019-07-30T18:04:39.550644Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load Training and Validation sets:\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T18:04:51.125306Z",
     "start_time": "2019-07-30T18:04:49.946239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitPoint:  100553\n",
      "Training Set shape (100553, 18)\n",
      "Validation Set shape (20405, 18)\n",
      " \n",
      " \n",
      "Train Data Type Descriptions:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100553 entries, 0 to 100552\n",
      "Data columns (total 18 columns):\n",
      "StoreID                 100553 non-null int32\n",
      "IsHoliday               100553 non-null int32\n",
      "IsOpen                  100553 non-null int32\n",
      "HasPromotions           100553 non-null int32\n",
      "StoreType               100553 non-null int8\n",
      "AssortmentType          100553 non-null int8\n",
      "NearestCompetitor       100553 non-null int32\n",
      "Region                  100553 non-null int32\n",
      "NumberOfSales           100553 non-null int32\n",
      "Region_AreaKM2          100553 non-null int32\n",
      "Region_GDP              100553 non-null int32\n",
      "Region_PopulationK      100553 non-null int32\n",
      "Year                    100553 non-null int32\n",
      "Month (number)          100553 non-null int32\n",
      "Week                    100553 non-null int32\n",
      "Day of year             100553 non-null int32\n",
      "Day of month            100553 non-null int32\n",
      "Day of week (number)    100553 non-null int32\n",
      "dtypes: int32(16), int8(2)\n",
      "memory usage: 7.1 MB\n",
      "None\n",
      " \n",
      " \n",
      "Feature Columns:\n",
      "Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n",
      "Win\n",
      "7\n",
      "3.6.7\n",
      "sklearn\n",
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "trainBench = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")\n",
    "#testBench = pd.read_csv(\"c:/Benchmarking/testBench.csv\")\n",
    "validBench = pd.read_csv(\"c:/Benchmarking/validBench.csv\")\n",
    "\n",
    "SplitPoint=len(trainBench.index) #SplitPoint = trainBench.shape[0] #trainBench.count(axis=0)\n",
    "print(\"SplitPoint: \",SplitPoint)\n",
    "\n",
    "df = pd.concat([trainBench,validBench],axis=0)\n",
    "del trainBench, validBench\n",
    "gc.collect()\n",
    "\n",
    "df = df.drop(\"ID\", axis=1)\n",
    "Int64columns = df.select_dtypes(['int64']).columns\n",
    "#Int64columns\n",
    "df[Int64columns] = df[Int64columns].astype(np.int32)\n",
    "#df['Date']=pd.to_datetime(dict(year=df['Year'], month=df['Month (number)'], day=df['Day of month']))\n",
    "#numerical enconde Assortment type & Store Type\n",
    "cat_columns = df.select_dtypes(['object']).columns\n",
    "df[cat_columns] = df[cat_columns].astype('category')\n",
    "#print(\"Categorical Columns:\")\n",
    "#print(df.select_dtypes(['category']).columns)\n",
    "#print(\" \")\n",
    "#print(\" \")\n",
    "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "#print(\"Categorical Columns:\")\n",
    "#print(df.select_dtypes(['category']).columns)\n",
    "#print(\" \")\n",
    "#print(\" \")\n",
    "\n",
    "trainBench, validBench = df.iloc[:SplitPoint, :], df.iloc[SplitPoint:, :]\n",
    "print(\"Training Set shape\",trainBench.shape)\n",
    "print(\"Validation Set shape\",validBench.shape)\n",
    "del df\n",
    "gc.collect()\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Train Data Type Descriptions:\")\n",
    "TrainDataTypes=trainBench.dtypes\n",
    "#print(TrainDataTypes)\n",
    "print(trainBench.info())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#print(validBench.info())\n",
    "\n",
    "# Split Features and Responses\n",
    "mask = trainBench.columns.difference(['NumberOfSales'])\n",
    "trainDataset_X = trainBench[mask]\n",
    "#print(\" \")\n",
    "#print(\" \")\n",
    "print(\"Feature Columns:\")\n",
    "print(mask)\n",
    "trainDataset_y = pd.DataFrame(trainBench['NumberOfSales'])\n",
    "#print(\"Head of Validation Data:\")\n",
    "#print(validBench.head(3))\n",
    "#print(\" \")\n",
    "#print(mask)\n",
    "validBench_X = validBench[mask]\n",
    "validBench_y = pd.DataFrame(validBench['NumberOfSales'])\n",
    "#CatCols=[trainBench.columns.get_loc(c) for c in trainBench.select_dtypes(['category']).columns if c in trainBench]\n",
    "del trainBench, validBench\n",
    "gc\n",
    "import platform\n",
    "import sys\n",
    "OpSys=platform.system()[:3]\n",
    "print(OpSys)\n",
    "OpSysVer=platform.release()\n",
    "print(OpSysVer)\n",
    "LangVer=sys.version[:5]\n",
    "print(LangVer)\n",
    "import sklearn\n",
    "Lib='sklearn'\n",
    "print(Lib)\n",
    "LibVer= sklearn.__version__\n",
    "print(LibVer)\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "workbook_name = 'C:\\\\Benchmarking\\\\AlgoPerf.xlsx'\n",
    "def xlsADD(row):\n",
    "    wb = load_workbook(workbook_name)\n",
    "    page = wb.active\n",
    "    page.append(row)\n",
    "    wb.save(filename=workbook_name) \n",
    "\n",
    "def InsertHeader():\n",
    "    Result=('OpSys','OpVer', \n",
    "            'Lang', \n",
    "            'LangVer', \n",
    "            'Lib', \n",
    "            'Lib.Ver', \n",
    "            'Algo',\n",
    "            'M_FitTime', \n",
    "            'XVR_FitTime',\n",
    "            'XVR_ScorTime',\n",
    "            'XV_Time',\n",
    "            'XV_EV',\n",
    "            'XV_MAE', \n",
    "            'XV_MSE', \n",
    "            'XV_RMSE', \n",
    "            'XV_R2',\n",
    "            'TS_PredTime',\n",
    "            'TS_EV',\n",
    "            'TS_MAE', \n",
    "            'TS_MSE', \n",
    "            'TS_RMSE', \n",
    "            'TS_R2',\n",
    "            'MeanEV',\n",
    "            'MdlParam', \n",
    "            'FeatImp',\n",
    "            'TdTypes',\n",
    "            'Comments')\n",
    "    #print(Result)\n",
    "    xlsADD(Result)\n",
    "    \n",
    "def InsertValues():\n",
    "    Result=(OpSys, \n",
    "            OpSysVer, \n",
    "            'Python', \n",
    "            LangVer, \n",
    "            Lib, \n",
    "            LibVer,\n",
    "            Algo,\n",
    "            M_FitTime, \n",
    "            XVR_FT,\n",
    "            ST,\n",
    "            XValidTime,\n",
    "            EV,\n",
    "            MAE, \n",
    "            MSE, \n",
    "            RMSE, \n",
    "            R2,\n",
    "            PredTime,\n",
    "            EVv,\n",
    "            MAEv, \n",
    "            MSEv, \n",
    "            RMSEv, \n",
    "            R2v,\n",
    "            EVtot.mean(),\n",
    "            str(Params), \n",
    "            str(d),\n",
    "            str(TrainDataTypes),\n",
    "            Comments)\n",
    "    #print(Result)\n",
    "    xlsADD(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validate and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T12:27:41.747473Z",
     "start_time": "2019-05-15T12:27:41.704470Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#Cross Validate and Score 1\n",
    "params={}\n",
    "def CrossEval(SplitPercent=20, MdlParams={}, verbose=False):\n",
    "    #**************Insert comments about this run here:\n",
    "    global Comments\n",
    "    #Comments='n_estimators=250, max_features=n_features-15, max_depth = 15, '+ str(SplitPercent)+'% of the training data'\n",
    "    global Algo\n",
    "    Algo='ExtraTreesRegressor'\n",
    "    Nrows,Ncols=trainDataset_X.shape\n",
    "    SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "    X, y = trainDataset_X.iloc[:SplitPoint, :], trainDataset_y.iloc[:SplitPoint]\n",
    "    #if verbose: \n",
    "    print(\"Training data set shape:\",X.shape)\n",
    "    start=time.time()\n",
    "    forest = ExtraTreesRegressor(n_estimators=240, \n",
    "                                 max_features = int(X.columns.size - 15),\n",
    "                                 max_depth = 16,\n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=0)\n",
    "    if MdlParams != {}:\n",
    "        forest.set_params(**MdlParams)\n",
    "    \n",
    "    forest.fit(X, y)\n",
    "    global M_FitTime, XValidTime, PredTime\n",
    "    M_FitTime = time.time() - start\n",
    "    if verbose:print(\"Measured Fit Time: \", M_FitTime)\n",
    "\n",
    "    UseTScv=True\n",
    "    if UseTScv:\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        if verbose:print(\"Using Time Series Cross Validation\")\n",
    "        start=time.time()\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "    else:\n",
    "        from sklearn.model_selection import KFold\n",
    "        if verbose:print(\"Using KFold Cross Validation\")\n",
    "        start=time.time()\n",
    "        kfolds = KFold(n_splits=5,shuffle=False,random_state=0)\n",
    "        scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=kfolds, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )\n",
    "\n",
    "    XValidTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Model Parameters: \")\n",
    "    global Params\n",
    "    Params=forest.get_params(deep=True)\n",
    "    if verbose:print(Params)\n",
    "    if verbose:print(\" \"); \n",
    "    global EV, MAE, MSE, RMSE, R2,XVR_FT, ST \n",
    "    if verbose:print(\"Cross Validation Performance: \")\n",
    "    #if verbose:\n",
    "    print(\"Cross Validation Time: %0.6f\" % (XValidTime))\n",
    "    EV=scores['test_explained_variance'].mean()\n",
    "    if verbose:print(scores['test_explained_variance'])\n",
    "    if verbose:print(\"EV: %0.6f\" % (EV))\n",
    "    #MAE is less sensitive to outliers, The contant value that minimizes the MAE is the median of the target values\n",
    "    if verbose:print(-1*scores['test_neg_mean_absolute_error'])\n",
    "    MAE=-1*scores['test_neg_mean_absolute_error'].mean()\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAE))\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    if verbose:print(-1*scores['test_neg_mean_squared_error'])\n",
    "    MSE=-1*scores['test_neg_mean_squared_error'].mean()\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSE))\n",
    "    if verbose:print(np.sqrt(-1*scores['test_neg_mean_squared_error']))\n",
    "    RMSE=np.sqrt(-1*scores['test_neg_mean_squared_error'].mean())\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSE))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    if verbose:print(\"XV R2 Actuals:\",scores['test_r2'])\n",
    "    R2=scores['test_r2'].mean()\n",
    "    if verbose:print(\"Cross Validation R2: %0.6f\" % (R2))\n",
    "    if verbose:print(\"XVR_fit_time Actuals: \", (scores['fit_time']))\n",
    "    XVR_FT=scores['fit_time'].mean()\n",
    "    if verbose:print(\"XVR_fit_time: %0.6f\" % (XVR_FT))\n",
    "    if verbose:print(\"score_time Actuals: \", (scores['score_time']))      \n",
    "    ST=scores['score_time'].mean()\n",
    "    if verbose:print(\"score_time: %0.6f\" % (ST))\n",
    "\n",
    "    Comments=str(Params)\n",
    "    \n",
    "    # Score Validation Set: \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import explained_variance_score\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    start=time.time()\n",
    "    y_pred=forest.predict(validBench_X)\n",
    "    PredTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    global EVv, MAEv, MSEv, RMSEv, R2v, EVtot\n",
    "    if verbose:print(\"Prediction Time: \", PredTime)\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation data set shape:\",validBench_X.shape)\n",
    "    EVv=explained_variance_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set Explained Variance (EV): %0.2f\" % (EVv))\n",
    "    #MAE is less sensitive to outliers, The constant value that minimizes the MAE is the median of the target values\n",
    "    MAEv=mean_absolute_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAEv))\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    MSEv=mean_squared_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSEv))\n",
    "    RMSEv=np.sqrt(MSEv)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSEv))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    R2v=r2_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set R2: %0.6f\" % (R2v))\n",
    "    EVtot=scores['test_explained_variance'].copy()\n",
    "    EVtot = np.append(EVtot,EVv)\n",
    "    EVtotMean=EVtot.mean()\n",
    "    if verbose:print(\"Total Mean EV: \",EVtot,EVtotMean)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Feature Importances:\")\n",
    "    global d\n",
    "    d=[]\n",
    "    \n",
    "    for f in range(X.shape[1]):\n",
    "        if verbose:print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "        d.append({\"%d. feat: %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]])})\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation Set EV, XVal EV, Mean EV: \",EVv, EV, EVtotMean)\n",
    "    #InsertHeader()\n",
    "    #InsertValues()\n",
    "    return EVv, EV, EVtotMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T18:39:14.435269Z",
     "start_time": "2019-06-05T18:39:14.372265Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#Cross Validate and Score 2: Can pass the model as a parameter\n",
    "params={}\n",
    "def CrossEval2(SplitPercent=20, Mdl=ExtraTreesRegressor(), MdlParams={}, verbose=False, AlgName=\"\"):\n",
    "    #**************Insert comments about this run here:\n",
    "    global Comments\n",
    "    #Comments='n_estimators=250, max_features=n_features-15, max_depth = 15, '+ str(SplitPercent)+'% of the training data'\n",
    "    global Algo\n",
    "    if AlgName !=\"\":\n",
    "        Algo=AlgName\n",
    "    else:\n",
    "        Algo= type(Mdl).__name__\n",
    "    print(\"Current Model: \", Algo)\n",
    "    if verbose: print(\"Current Model: \", Algo)\n",
    "    Nrows,Ncols=trainDataset_X.shape\n",
    "    SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "    X, y = trainDataset_X.iloc[:SplitPoint, :], trainDataset_y.iloc[:SplitPoint]\n",
    "    #if verbose: \n",
    "    print(\"Training data set shape:\",X.shape)\n",
    "    start=time.time()\n",
    "    \n",
    "    forest = Mdl\n",
    "    \n",
    "    if MdlParams != {}:\n",
    "        forest.set_params(**MdlParams)\n",
    "    else:\n",
    "        forest.set_params(n_estimators=240, \n",
    "                          max_features = int(X.columns.size - 15),\n",
    "                          max_depth = 16,\n",
    "                          n_jobs=-1, \n",
    "                          random_state=0)\n",
    "    \n",
    "    forest.fit(X, y)\n",
    "    global M_FitTime, XValidTime, PredTime\n",
    "    M_FitTime = time.time() - start\n",
    "    if verbose:print(\"Measured Fit Time: \", M_FitTime)\n",
    "\n",
    "    UseTScv=True\n",
    "    if UseTScv:\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        if verbose:print(\"Using Time Series Cross Validation\")\n",
    "        start=time.time()\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "    else:\n",
    "        from sklearn.model_selection import KFold\n",
    "        if verbose:print(\"Using KFold Cross Validation\")\n",
    "        start=time.time()\n",
    "        kfolds = KFold(n_splits=5,shuffle=False,random_state=0)\n",
    "        scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=kfolds, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )\n",
    "\n",
    "    XValidTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Model Parameters: \")\n",
    "    global Params\n",
    "    Params=forest.get_params(deep=True)\n",
    "    if verbose:print(Params)\n",
    "    if verbose:print(\" \"); \n",
    "    global EV, MAE, MSE, RMSE, R2,XVR_FT, ST \n",
    "    if verbose:print(\"Cross Validation Performance: \")\n",
    "    #if verbose:\n",
    "    print(\"Cross Validation Time: %0.6f\" % (XValidTime))\n",
    "    EV=scores['test_explained_variance'].mean()\n",
    "    if verbose:print(scores['test_explained_variance'])\n",
    "    if verbose:print(\"EV: %0.6f\" % (EV))\n",
    "    #MAE is less sensitive to outliers, The contant value that minimizes the MAE is the median of the target values\n",
    "    if verbose:print(-1*scores['test_neg_mean_absolute_error'])\n",
    "    MAE=-1*scores['test_neg_mean_absolute_error'].mean()\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAE))\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    if verbose:print(-1*scores['test_neg_mean_squared_error'])\n",
    "    MSE=-1*scores['test_neg_mean_squared_error'].mean()\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSE))\n",
    "    if verbose:print(np.sqrt(-1*scores['test_neg_mean_squared_error']))\n",
    "    RMSE=np.sqrt(-1*scores['test_neg_mean_squared_error'].mean())\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSE))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    if verbose:print(\"XV R2 Actuals:\",scores['test_r2'])\n",
    "    R2=scores['test_r2'].mean()\n",
    "    if verbose:print(\"Cross Validation R2: %0.6f\" % (R2))\n",
    "    if verbose:print(\"XVR_fit_time Actuals: \", (scores['fit_time']))\n",
    "    XVR_FT=scores['fit_time'].mean()\n",
    "    if verbose:print(\"XVR_fit_time: %0.6f\" % (XVR_FT))\n",
    "    if verbose:print(\"score_time Actuals: \", (scores['score_time']))      \n",
    "    ST=scores['score_time'].mean()\n",
    "    if verbose:print(\"score_time: %0.6f\" % (ST))\n",
    "\n",
    "    Comments=str(Params)\n",
    "    \n",
    "    # Score Validation Set: \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import explained_variance_score\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    start=time.time()\n",
    "    y_pred=forest.predict(validBench_X)\n",
    "    PredTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    global EVv, MAEv, MSEv, RMSEv, R2v, EVtot\n",
    "    if verbose:print(\"Prediction Time: \", PredTime)\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation data set shape:\",validBench_X.shape)\n",
    "    EVv=explained_variance_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set Explained Variance (EV): %0.2f\" % (EVv))\n",
    "    #MAE is less sensitive to outliers, The constant value that minimizes the MAE is the median of the target values\n",
    "    MAEv=mean_absolute_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAEv))\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    MSEv=mean_squared_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSEv))\n",
    "    RMSEv=np.sqrt(MSEv)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSEv))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    R2v=r2_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set R2: %0.6f\" % (R2v))\n",
    "    EVtot=scores['test_explained_variance'].copy()\n",
    "    EVtot = np.append(EVtot,EVv)\n",
    "    EVtotMean=EVtot.mean()\n",
    "    if verbose:print(\"Total Mean EV: \",EVtot,EVtotMean)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Feature Importances:\")\n",
    "    global d\n",
    "    d=[]\n",
    "    \n",
    "    for f in range(X.shape[1]):\n",
    "        if verbose:print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "        d.append({\"%d. feat: %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]])})\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation Set EV, XVal EV, Mean EV: \",EVv, EV, EVtotMean)\n",
    "    #InsertHeader()\n",
    "    #InsertValues()\n",
    "    return EVv, EV, EVtotMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T18:45:11.581782Z",
     "start_time": "2019-06-05T18:44:41.998818Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  ExtraTreesRegressor\n",
      "Training data set shape: (100553, 17)\n",
      "Measured Fit Time:  3.0296638011932373\n",
      "Using Time Series Cross Validation\n",
      " \n",
      "Model Parameters: \n",
      "{'bootstrap': False, 'criterion': 'mse', 'max_depth': 16, 'max_features': 2, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 240, 'n_jobs': -1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 26.107286\n",
      "[0.6893979  0.55408824 0.64868223 0.67215276 0.39283517]\n",
      "EV: 0.591431\n",
      "[ 965.77104548 1290.12103312 1130.37150079 1032.344633   1251.99564222]\n",
      "MAE: 1134.120771\n",
      "[2031006.19284856 3981787.24555979 2449460.21873086 2362433.8414521\n",
      " 6041839.27572217]\n",
      "MSE: 3373305.354863\n",
      "[1425.1337456  1995.44161668 1565.07514795 1537.02109337 2458.01531234]\n",
      "RMSE: 1836.656025\n",
      "XV R2 Actuals: [0.66915499 0.55369165 0.64824639 0.67212614 0.39255823]\n",
      "Cross Validation R2: 0.587155\n",
      "XVR_fit_time Actuals:  [1.3370378  1.91405511 2.58560872 3.13410234 3.73361611]\n",
      "XVR_fit_time: 2.540884\n",
      "score_time Actuals:  [0.83251977 0.82851434 0.82952595 0.82701731 0.82901788]\n",
      "score_time: 0.829319\n",
      " \n",
      " \n",
      "Prediction Time:  0.2080073356628418\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1057.398489\n",
      "MSE: 1995088.740405\n",
      "RMSE: 1412.476103\n",
      "Validation Set R2: 0.667064\n",
      "Total Mean EV:  [0.6893979  0.55408824 0.64868223 0.67215276 0.39283517 0.68034527] 0.6062502600317962\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.365634)\n",
      "2. feature 4 HasPromotions (0.149547)\n",
      "3. feature 2 Day of week (number) (0.104564)\n",
      "4. feature 13 StoreID (0.049110)\n",
      "5. feature 11 Region_GDP (0.043111)\n",
      "6. feature 8 NearestCompetitor (0.042815)\n",
      "7. feature 14 StoreType (0.041804)\n",
      "8. feature 5 IsHoliday (0.039832)\n",
      "9. feature 0 AssortmentType (0.036843)\n",
      "10. feature 10 Region_AreaKM2 (0.026902)\n",
      "11. feature 12 Region_PopulationK (0.023678)\n",
      "12. feature 9 Region (0.020062)\n",
      "13. feature 3 Day of year (0.015112)\n",
      "14. feature 1 Day of month (0.014028)\n",
      "15. feature 15 Week (0.012853)\n",
      "16. feature 7 Month (number) (0.010592)\n",
      "17. feature 16 Year (0.003513)\n",
      " \n",
      "Validation Set EV, XVal EV, Mean EV:  0.6803452671668492 0.5914312586047856 0.6062502600317962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6803452671668492, 0.5914312586047856, 0.6062502600317962)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CrossEval2 Test:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "MdlParams={}\n",
    "Mdl=ExtraTreesRegressor\n",
    "CrossEval2(100, Mdl=Mdl, MdlParams=MdlParams,verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:52:30.751298Z",
     "start_time": "2019-06-22T14:52:30.670293Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Cross Validate and Score 3\n",
    "params={}\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "def CrossEval3(trainDataset_X, trainDataset_y,\n",
    "               validBench_X, validBench_y,\n",
    "               SplitPercent=100, Mdl=ExtraTreesRegressor, MdlParams={}, verbose=False,\n",
    "               AlgName=\"\", Comment=\"\"):\n",
    "    global Algo\n",
    "    if AlgName !=\"\":\n",
    "        Algo=AlgName\n",
    "    else:\n",
    "        Algo= type(Mdl).__name__\n",
    "    print(\"Current Model: \", Algo)\n",
    "    Nrows,_ =trainDataset_X.shape\n",
    "    SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "    #print(SplitPoint)\n",
    "    Pandas = False\n",
    "    if isinstance(trainDataset_X,pd.core.frame.DataFrame): Pandas=True\n",
    "    print(\"Pandas Value:\",Pandas)\n",
    "    if Pandas:\n",
    "        X, y = trainDataset_X.iloc[:SplitPoint, :], trainDataset_y.iloc[:SplitPoint]\n",
    "    elif isinstance(trainDataset_X,h2o.H2OFrame):\n",
    "        X, y = trainDataset_X[:SplitPoint, :], trainDataset_y[:SplitPoint, :]\n",
    "        X=X.concat(y, axis=1)\n",
    "        print(\"Y column: \",y.names)\n",
    "        print(\"X columns: \",X.names)\n",
    "    del(trainDataset_X,trainDataset_y)\n",
    "    \n",
    "    #if verbose: \n",
    "    print(\"Training data set shape:\",X.shape)\n",
    "    \n",
    "    forest = Mdl\n",
    "    \n",
    "    if MdlParams != {}:\n",
    "        forest.set_params(**MdlParams)\n",
    "    elif Pandas:\n",
    "        forest.set_params(n_estimators=245, \n",
    "                          max_features = int(X.columns.size - 15),\n",
    "                          max_depth = 16,\n",
    "                          n_jobs=-1, \n",
    "                          random_state=0)\n",
    "    #print(X.shape,y.shape)\n",
    "    \n",
    "    global M_FitTime, XValidTime, PredTime\n",
    "    global EV, MAE, MSE, RMSE, R2, XVR_FT, ST \n",
    "    \n",
    "    EVar=[]; MAEar=[]; MSEar=[]; R2ar=[]; RMSEar=[]\n",
    "    \n",
    "    start=time.time()\n",
    "    if Pandas: \n",
    "        y=y.values.ravel()\n",
    "        forest.fit(X, y)\n",
    "        M_FitTime = time.time() - start\n",
    "        UseTScv=True\n",
    "        start=time.time()\n",
    "        if UseTScv:\n",
    "            if verbose:print(\"Using Time Series Cross Validation\")\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            scores = cross_validate(forest, X, y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "        else:\n",
    "            if verbose:print(\"Using KFold Cross Validation\")\n",
    "            kfolds = KFold(n_splits=5,shuffle=False,random_state=0)\n",
    "            scores = cross_validate(forest, X, y, cv=kfolds, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )\n",
    "        XValidTime = time.time() - start\n",
    "        EVar=scores['test_explained_variance']\n",
    "        MAEar=-1*scores['test_neg_mean_absolute_error']\n",
    "        MSEar=-1*scores['test_neg_mean_squared_error']\n",
    "        RMSEar=np.sqrt(-1*scores['test_neg_mean_squared_error'])\n",
    "        R2ar=scores['test_r2']\n",
    "        XVR_FTar=scores['fit_time']\n",
    "        STar=scores['score_time']\n",
    "    else: #For H2O you have to do time series cross validation manually\n",
    "        forest.set_params(nfolds=0)\n",
    "        M_FitTime=0\n",
    "        start=time.time()        \n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "        Xcols=list(set(X.names)-set('NumberOfSales'))\n",
    "        Ycol='NumberOfSales'\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            train = X[min(train_index):max(train_index),:]\n",
    "            test = X[min(test_index):max(test_index),:]\n",
    "            print(len(train),len(test))\n",
    "            forest.train(x=Xcols,y=Ycol,\n",
    "                         training_frame=train,validation_frame=test,verbose=False)            \n",
    "            y_pred=forest.predict(test[Xcols])\n",
    "            EVar.append(explained_variance_score(test[Ycol].as_data_frame(), y_pred.as_data_frame()))\n",
    "            MAEar.append(mean_absolute_error(test[Ycol].as_data_frame(), y_pred.as_data_frame()))\n",
    "            MSEar.append(mean_squared_error(test[Ycol].as_data_frame(), y_pred.as_data_frame()))\n",
    "            R2ar.append(r2_score(test[Ycol].as_data_frame(), y_pred.as_data_frame()))\n",
    "        \n",
    "        df = pd.DataFrame(forest.get_params())\n",
    "        print(df)\n",
    "        RMSEar=np.sqrt(np.array(MSEar))\n",
    "        XVR_FTar=[0, 0, 0, 0, 0]\n",
    "        STar=[0, 0, 0, 0, 0]\n",
    "        XValidTime = time.time() - start \n",
    "    \n",
    "    EV = np.array(EVar).mean()\n",
    "    MAE=np.array(MAEar).mean()\n",
    "    MSE=np.array(MSEar).mean()\n",
    "    RMSE=np.array(RMSEar).mean()\n",
    "    R2=np.array(R2ar).mean()\n",
    "    XVR_FT=np.array(XVR_FTar).sum()\n",
    "    ST=np.array(STar).sum()\n",
    "    \n",
    "    if verbose:print(\"Measured Fit Time: \", M_FitTime)\n",
    "        \n",
    "    if verbose:print(\" \")\n",
    "\n",
    "    global Params\n",
    "    Params=forest.get_params(deep=True)\n",
    "    if verbose and Pandas:\n",
    "        print(\"Model Parameters: \")\n",
    "        print(Params)\n",
    "    if verbose:print(\" \"); \n",
    "\n",
    "    if verbose:print(\"Cross Validation Performance: \")\n",
    "    print(\"Cross Validation Time: %0.6f\" % (XValidTime))\n",
    "    if verbose:print(\"EV: %0.6f\" % (EV))\n",
    "    if verbose:print(\"EV for each Fold:\",EVar)\n",
    "    #MAE is less sensitive to outliers, The contant value that minimizes the MAE is the median of the target values\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAE))\n",
    "    if verbose:print(\"MAE for each Fold:\",MAEar)\n",
    "    #MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSE))\n",
    "    if verbose:print(\"MSE for each Fold:\",MSEar)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSE))\n",
    "    if verbose:print(\"RMSE for each Fold:\",RMSEar)\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    if verbose:print(\"XV R2 Actuals:\",R2ar)\n",
    "    if verbose:print(\"Cross Validation R2: %0.6f\" % (R2))\n",
    "    if verbose:print(\"XVR_fit_time Actuals: \", (XVR_FTar))\n",
    "    if verbose:print(\"XVR_fit_time: %0.6f\" % (XVR_FT))\n",
    "    if verbose:print(\"score_time Actuals: \", (STar))      \n",
    "    if verbose:print(\"score_time: %0.6f\" % (ST))\n",
    "\n",
    "    \n",
    "    \n",
    "    #**************Insert comments about this run here:\n",
    "    global Comments    \n",
    "    if Comment==\"\":\n",
    "        Comments=str(Params)\n",
    "    else:\n",
    "        Comments=Comment\n",
    "    #Comments='n_estimators=250, max_features=n_features-15, max_depth = 15, '+ str(SplitPercent)+'% of the training data'\n",
    "        \n",
    "    # Score Validation Set: \n",
    "    start=time.time()\n",
    "    if Pandas:\n",
    "        y_pred=forest.predict(validBench_X)\n",
    "    else:\n",
    "        y_pred=forest.predict(validBench_X).as_data_frame()\n",
    "        validBench_y=validBench_y.as_data_frame()\n",
    "    PredTime = time.time() - start\n",
    "\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\" \")\n",
    "    global EVv, MAEv, MSEv, RMSEv, R2v, EVtot\n",
    "    if verbose:print(\"Prediction Time: \", PredTime)\n",
    "    if verbose:print(\" \")\n",
    "    if verbose:print(\"Validation data set shape:\",validBench_X.shape)\n",
    "    EVv=explained_variance_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set Explained Variance (EV): %0.2f\" % (EVv))\n",
    "    #MAE is less sensitive to outliers, The constant value that minimizes the MAE is the median of the target values\n",
    "    MAEv=mean_absolute_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MAE: %0.6f\" % (MAEv))\n",
    "    #MAE considers outliers, The constant value that minimizes the MSE is the mean of the target values\n",
    "    #If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "    #It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "    MSEv=mean_squared_error(validBench_y, y_pred)\n",
    "    if verbose:print(\"MSE: %0.6f\" % (MSEv))\n",
    "    RMSEv=np.sqrt(MSEv)\n",
    "    if verbose:print(\"RMSE: %0.6f\" % (RMSEv))\n",
    "    #Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "    R2v=r2_score(validBench_y, y_pred)\n",
    "    if verbose:print(\"Validation Set R2: %0.6f\" % (R2v))\n",
    "    EVtot=EVar.copy()\n",
    "    EVtot = np.append(EVtot,EVv)\n",
    "    EVtotMean=EVtot.mean()\n",
    "    if verbose:print(\"Total Mean EV: \",EVtot,EVtotMean)\n",
    "    global d\n",
    "    if Pandas and Algo!=\"XGBRegressor\":\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                     axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        # Print the feature ranking\n",
    "        if verbose:print(\" \")\n",
    "        if verbose:print(\" \")\n",
    "        if verbose:print(\"Feature Importances:\")\n",
    "\n",
    "        d=[]\n",
    "\n",
    "        for f in range(X.shape[1]):\n",
    "            if verbose:print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]]))\n",
    "            d.append({\"%d. feat: %d %s (%f)\" % (f + 1, indices[f], X.columns[indices[f]], importances[indices[f]])})\n",
    "        if verbose:print(\" \")\n",
    "    else:\n",
    "        d=\"\"\n",
    "    if verbose:print(\"Validation Set EV, XVal EV, Mean EV: \",EVv, EV, EVtotMean)\n",
    "\n",
    "    #InsertHeader()\n",
    "    InsertValues()\n",
    "    return EVv, EV, EVtotMean, R2v, R2, -MAE, -MAEv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEval3 Test with XGBRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T15:44:44.530894Z",
     "start_time": "2019-06-22T15:44:21.697052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  7.624358177185059\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.588277\n",
      "EV: 0.623928\n",
      "EV for each Fold: [0.68190691 0.56076369 0.67496651 0.65102564 0.55097968]\n",
      "MAE: 1170.456117\n",
      "MAE for each Fold: [1058.7341904  1298.82717459 1154.30082154 1081.80776398 1258.61063623]\n",
      "MSE: 3138426.957409\n",
      "MSE for each Fold: [2331326.57835202 3923153.33696417 2329060.63802474 2517511.5149839\n",
      " 4591082.71871968]\n",
      "RMSE: 1752.607481\n",
      "RMSE for each Fold: [1526.8682256  1980.69516508 1526.12602298 1586.66679394 2142.68119857]\n",
      "XV R2 Actuals: [0.62023367 0.56026378 0.66553632 0.65060345 0.53841615]\n",
      "Cross Validation R2: 0.607011\n",
      "XVR_fit_time Actuals:  [1.40706754 1.36320734 2.26226711 2.85566759 3.63522458]\n",
      "XVR_fit_time: 11.523434\n",
      "score_time Actuals:  [0.1530087  0.16020298 0.15820289 0.19320059 0.19320512]\n",
      "score_time: 0.857820\n",
      " \n",
      " \n",
      "Prediction Time:  0.10920000076293945\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 1000.375395\n",
      "MSE: 1928647.830256\n",
      "RMSE: 1388.757657\n",
      "Validation Set R2: 0.678151\n",
      "Total Mean EV:  [0.68190691 0.56076369 0.67496651 0.65102564 0.55097968 0.68559582] 0.6342063760826037\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6855958186900012 0.6239284875611242 0.6342063760826037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6855958186900012,\n",
       " 0.6239284875611242,\n",
       " 0.6342063760826037,\n",
       " 0.6781513177647969,\n",
       " 0.6070106735659544,\n",
       " -1170.4561173464306,\n",
       " -1000.3753948798661)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CrossEval3 Test:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "MdlParams= {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
    "\n",
    "Mdl=xgb.XGBRegressor(random_state=0,n_jobs=-1,)\n",
    "CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "           SplitPercent=100, Mdl=Mdl, MdlParams=MdlParams,verbose=True,AlgName=\"XGBRegressor\",\n",
    "           Comment=\"Best Params found Maximazing Avg. EV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T15:01:41.335830Z",
     "start_time": "2019-06-22T14:53:10.051808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth : [10, 11, 12]\n",
      "n_estimators : [51, 52, 53, 54, 57, 58]\n",
      "learning_rate : [0.097, 0.098, 0.099, 0.1, 0.101]\n",
      "colsample_bylevel : [0.99, 1]\n",
      "colsample_bytree : [0.5, 0.55, 1]\n",
      "subsample : [0.8, 0.9, 1]\n",
      "reg_lambda : [0.1, 0.5, 1, 1.1, 1.5, 2, 5]\n",
      "{'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.099, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 18.162286\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1 {'max_depth': 12, 'n_estimators': 53, 'learning_rate': 0.099, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.852896\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "1 {'max_depth': 12, 'n_estimators': 58, 'learning_rate': 0.099, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 17.977232\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "-1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.170629\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.101, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.346229\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "-1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 17.605866\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "5       12.0          51.0          0.097               0.99   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "5              0.55        0.9         1.1  0.616429  \n",
      "1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.55, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 18.479433\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "5       12.0          51.0          0.097               0.99   \n",
      "6       12.0          51.0          0.097               1.00   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "5              0.55        0.9         1.1  0.616429  \n",
      "6              0.55        0.9         1.1  0.621570  \n",
      "1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 23.347642\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "5       12.0          51.0          0.097               0.99   \n",
      "6       12.0          51.0          0.097               1.00   \n",
      "7       12.0          51.0          0.097               1.00   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "5              0.55        0.9         1.1  0.616429  \n",
      "6              0.55        0.9         1.1  0.621570  \n",
      "7              1.00        0.9         1.1  0.500737  \n",
      "-1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 0.9, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.573230\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "5       12.0          51.0          0.097               0.99   \n",
      "6       12.0          51.0          0.097               1.00   \n",
      "7       12.0          51.0          0.097               1.00   \n",
      "8       12.0          51.0          0.097               1.00   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "5              0.55        0.9         1.1  0.616429  \n",
      "6              0.55        0.9         1.1  0.621570  \n",
      "7              1.00        0.9         1.1  0.500737  \n",
      "8              0.50        0.9         1.1  0.623808  \n",
      "1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 1.1}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 15.299230\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0       11.0          53.0          0.099               0.99   \n",
      "1       12.0          53.0          0.099               0.99   \n",
      "2       12.0          58.0          0.099               0.99   \n",
      "3       12.0          51.0          0.099               0.99   \n",
      "4       12.0          51.0          0.101               0.99   \n",
      "5       12.0          51.0          0.097               0.99   \n",
      "6       12.0          51.0          0.097               1.00   \n",
      "7       12.0          51.0          0.097               1.00   \n",
      "8       12.0          51.0          0.097               1.00   \n",
      "9       12.0          51.0          0.097               1.00   \n",
      "\n",
      "   colsample_bytree  subsample  reg_lambda     score  \n",
      "0              0.55        0.9         1.1  0.605803  \n",
      "1              0.55        0.9         1.1  0.614355  \n",
      "2              0.55        0.9         1.1  0.614221  \n",
      "3              0.55        0.9         1.1  0.614618  \n",
      "4              0.55        0.9         1.1  0.613663  \n",
      "5              0.55        0.9         1.1  0.616429  \n",
      "6              0.55        0.9         1.1  0.621570  \n",
      "7              1.00        0.9         1.1  0.500737  \n",
      "8              0.50        0.9         1.1  0.623808  \n",
      "9              0.50        1.0         1.1  0.629106  \n",
      "1 {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 15.474848\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "Theoretical Pattern Move:  [13.    49.     0.095  1.01   0.45   0.9    1.1  ]\n",
      "Closest point in the Space {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 0.9, 'reg_lambda': 1.1}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 2, 1, 1, 1, 1, 2]\n",
      "7\n",
      "-1 {'max_depth': 11, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 13.816025\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 14.501426\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.098, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 14.716826\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "-1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 0.99, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 13.726425\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.55, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.446429\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "-1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 0.9, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 16.531050\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "-1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 1.5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 14.609626\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "Theoretical Pattern Move:  [13.    49.     0.095  1.01   0.45   1.1    8.9  ]\n",
      "Closest point in the Space {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1]\n",
      "7\n",
      "1 {'max_depth': 12, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 16.292429\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "18       12.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "18              0.50        1.0         5.0  0.632831  \n",
      "-1 {'max_depth': 10, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 13.747624\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "18       12.0          53.0          0.097               1.00   \n",
      "19       10.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "18              0.50        1.0         5.0  0.632831  \n",
      "19              0.50        1.0         5.0  0.631785  \n",
      "1 {'max_depth': 11, 'n_estimators': 54, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 14.750026\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "18       12.0          53.0          0.097               1.00   \n",
      "19       10.0          53.0          0.097               1.00   \n",
      "20       11.0          54.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "18              0.50        1.0         5.0  0.632831  \n",
      "19              0.50        1.0         5.0  0.631785  \n",
      "20              0.50        1.0         5.0  0.634158  \n",
      "-1 {'max_depth': 11, 'n_estimators': 52, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Time: 14.156225\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "18       12.0          53.0          0.097               1.00   \n",
      "19       10.0          53.0          0.097               1.00   \n",
      "20       11.0          54.0          0.097               1.00   \n",
      "21       11.0          52.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "18              0.50        1.0         5.0  0.632831  \n",
      "19              0.50        1.0         5.0  0.631785  \n",
      "20              0.50        1.0         5.0  0.634158  \n",
      "21              0.50        1.0         5.0  0.633689  \n",
      "-1 {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 2}\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 15.656862\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        11.0          53.0          0.099               0.99   \n",
      "1        12.0          53.0          0.099               0.99   \n",
      "2        12.0          58.0          0.099               0.99   \n",
      "3        12.0          51.0          0.099               0.99   \n",
      "4        12.0          51.0          0.101               0.99   \n",
      "5        12.0          51.0          0.097               0.99   \n",
      "6        12.0          51.0          0.097               1.00   \n",
      "7        12.0          51.0          0.097               1.00   \n",
      "8        12.0          51.0          0.097               1.00   \n",
      "9        12.0          51.0          0.097               1.00   \n",
      "10       12.0          51.0          0.097               1.00   \n",
      "11       11.0          51.0          0.097               1.00   \n",
      "12       11.0          53.0          0.097               1.00   \n",
      "13       11.0          53.0          0.098               1.00   \n",
      "14       11.0          53.0          0.097               0.99   \n",
      "15       11.0          53.0          0.097               1.00   \n",
      "16       11.0          53.0          0.097               1.00   \n",
      "17       11.0          53.0          0.097               1.00   \n",
      "18       12.0          53.0          0.097               1.00   \n",
      "19       10.0          53.0          0.097               1.00   \n",
      "20       11.0          54.0          0.097               1.00   \n",
      "21       11.0          52.0          0.097               1.00   \n",
      "22       11.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  subsample  reg_lambda     score  \n",
      "0               0.55        0.9         1.1  0.605803  \n",
      "1               0.55        0.9         1.1  0.614355  \n",
      "2               0.55        0.9         1.1  0.614221  \n",
      "3               0.55        0.9         1.1  0.614618  \n",
      "4               0.55        0.9         1.1  0.613663  \n",
      "5               0.55        0.9         1.1  0.616429  \n",
      "6               0.55        0.9         1.1  0.621570  \n",
      "7               1.00        0.9         1.1  0.500737  \n",
      "8               0.50        0.9         1.1  0.623808  \n",
      "9               0.50        1.0         1.1  0.629106  \n",
      "10              0.50        1.0         5.0  0.632173  \n",
      "11              0.50        1.0         5.0  0.633401  \n",
      "12              0.50        1.0         5.0  0.634206  \n",
      "13              0.50        1.0         5.0  0.633075  \n",
      "14              0.50        1.0         5.0  0.621080  \n",
      "15              0.55        1.0         5.0  0.624706  \n",
      "16              0.50        0.9         5.0  0.623378  \n",
      "17              0.50        1.0         1.5  0.630788  \n",
      "18              0.50        1.0         5.0  0.632831  \n",
      "19              0.50        1.0         5.0  0.631785  \n",
      "20              0.50        1.0         5.0  0.634158  \n",
      "21              0.50        1.0         5.0  0.633689  \n",
      "22              0.50        1.0         2.0  0.631440  \n",
      "Theoretical Pattern Move:  [13.    49.     0.095  1.01   0.45   1.1    8.9  ]\n",
      "Closest point in the Space {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1]\n",
      "7\n",
      "Theoretical Pattern Move:  [13.    49.     0.095  1.01   0.45   1.1    8.9  ]\n",
      "Closest point in the Space {'max_depth': 12, 'n_estimators': 51, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 5}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1, 1]\n",
      "7\n",
      "Best Parameters Found:\n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1.0, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11.0, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53.0, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5.0, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1.0}\n",
      " \n",
      "{'max_depth': 11.0, 'n_estimators': 53.0, 'learning_rate': 0.097, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.5, 'subsample': 1.0, 'reg_lambda': 5.0}\n",
      " \n",
      "max_depth            11.000000\n",
      "n_estimators         53.000000\n",
      "learning_rate         0.097000\n",
      "colsample_bylevel     1.000000\n",
      "colsample_bytree      0.500000\n",
      "subsample             1.000000\n",
      "reg_lambda            5.000000\n",
      "score                 0.634206\n",
      "Name: 12, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.634206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.634158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.633689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.633401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.633075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.632831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.632173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.631785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.631440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.630788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.629106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.624706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.623808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.623378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.621570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.621080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.616429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.614618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.614355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.614221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.613663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.605803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.500737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
       "12       11.0          53.0          0.097               1.00   \n",
       "20       11.0          54.0          0.097               1.00   \n",
       "21       11.0          52.0          0.097               1.00   \n",
       "11       11.0          51.0          0.097               1.00   \n",
       "13       11.0          53.0          0.098               1.00   \n",
       "18       12.0          53.0          0.097               1.00   \n",
       "10       12.0          51.0          0.097               1.00   \n",
       "19       10.0          53.0          0.097               1.00   \n",
       "22       11.0          53.0          0.097               1.00   \n",
       "17       11.0          53.0          0.097               1.00   \n",
       "9        12.0          51.0          0.097               1.00   \n",
       "15       11.0          53.0          0.097               1.00   \n",
       "8        12.0          51.0          0.097               1.00   \n",
       "16       11.0          53.0          0.097               1.00   \n",
       "6        12.0          51.0          0.097               1.00   \n",
       "14       11.0          53.0          0.097               0.99   \n",
       "5        12.0          51.0          0.097               0.99   \n",
       "3        12.0          51.0          0.099               0.99   \n",
       "1        12.0          53.0          0.099               0.99   \n",
       "2        12.0          58.0          0.099               0.99   \n",
       "4        12.0          51.0          0.101               0.99   \n",
       "0        11.0          53.0          0.099               0.99   \n",
       "7        12.0          51.0          0.097               1.00   \n",
       "\n",
       "    colsample_bytree  subsample  reg_lambda     score  \n",
       "12              0.50        1.0         5.0  0.634206  \n",
       "20              0.50        1.0         5.0  0.634158  \n",
       "21              0.50        1.0         5.0  0.633689  \n",
       "11              0.50        1.0         5.0  0.633401  \n",
       "13              0.50        1.0         5.0  0.633075  \n",
       "18              0.50        1.0         5.0  0.632831  \n",
       "10              0.50        1.0         5.0  0.632173  \n",
       "19              0.50        1.0         5.0  0.631785  \n",
       "22              0.50        1.0         2.0  0.631440  \n",
       "17              0.50        1.0         1.5  0.630788  \n",
       "9               0.50        1.0         1.1  0.629106  \n",
       "15              0.55        1.0         5.0  0.624706  \n",
       "8               0.50        0.9         1.1  0.623808  \n",
       "16              0.50        0.9         5.0  0.623378  \n",
       "6               0.55        0.9         1.1  0.621570  \n",
       "14              0.50        1.0         5.0  0.621080  \n",
       "5               0.55        0.9         1.1  0.616429  \n",
       "3               0.55        0.9         1.1  0.614618  \n",
       "1               0.55        0.9         1.1  0.614355  \n",
       "2               0.55        0.9         1.1  0.614221  \n",
       "4               0.55        0.9         1.1  0.613663  \n",
       "0               0.55        0.9         1.1  0.605803  \n",
       "7               1.00        0.9         1.1  0.500737  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 0, \n",
    "#'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3.0, 'min_child_weight': 1, 'missing': None, \n",
    "#'n_estimators': 100, 'n_jobs': 1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0,\n",
    "#'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
    "\n",
    "#23        6,           54,           0.101,              0.5,   0.5  -1014\n",
    "#{'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0.1}\n",
    "#28        10,          53,           0.098,              1,     0.5,  0 score:0.634091\n",
    "#0.634091: {'max_depth': 10.0, 'n_estimators': 53.0, 'learning_rate': 0.098, 'colsample_bylevel': 1.0, 'colsample_bytree': 0.5, 'subsample': 1.0}\n",
    "\n",
    "param_dist={'max_depth':[10,11,12],\n",
    "            'n_estimators': [51,52,53,54,57,58],\n",
    "            'learning_rate': [0.097,0.098, 0.099,0.1,0.101],\n",
    "            'colsample_bylevel': [0.99,1],\n",
    "            'colsample_bytree': [0.5,0.55,1],\n",
    "            'subsample': [0.8,0.9,1],\n",
    "            'reg_lambda':[0.1,0.5,1,1.1,1.5,2,5]\n",
    "           } \n",
    "\n",
    "Mdl=xgb.XGBRegressor(random_state=0, n_jobs=-1)\n",
    "PatternSearch(param_dist, 100, Mdl=Mdl, alfa=1.40, metric=2, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Lambda (regularization parameter) Study: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T16:06:35.478108Z",
     "start_time": "2019-06-22T16:03:59.655884Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      "lambda:  4.8\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  7.890299558639526\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.582577\n",
      "EV: 0.624769\n",
      "EV for each Fold: [0.68141772 0.55395105 0.67743406 0.65793015 0.55310988]\n",
      "MAE: 1167.111285\n",
      "MAE for each Fold: [1051.65082922 1310.90438875 1150.13958852 1065.57996828 1257.28164919]\n",
      "MSE: 3127344.130964\n",
      "MSE for each Fold: [2308277.87014384 3985201.97191318 2311925.61641604 2468419.03535473\n",
      " 4562896.16099134]\n",
      "RMSE: 1748.662916\n",
      "RMSE for each Fold: [1519.3017706  1996.29706505 1520.50176469 1571.12031218 2136.09366859]\n",
      "XV R2 Actuals: [0.62398823 0.5533089  0.66799699 0.65741683 0.54125001]\n",
      "Cross Validation R2: 0.608792\n",
      "XVR_fit_time Actuals:  [1.15943694 1.34260249 2.16500711 2.68162704 3.50385952]\n",
      "XVR_fit_time: 10.852533\n",
      "score_time Actuals:  [0.14039993 0.14040041 0.16760874 0.14040017 0.14040017]\n",
      "score_time: 0.729209\n",
      " \n",
      " \n",
      "Prediction Time:  0.04200243949890137\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.321188\n",
      "MSE: 1955006.759357\n",
      "RMSE: 1398.215563\n",
      "Validation Set R2: 0.673753\n",
      "Total Mean EV:  [0.68141772 0.55395105 0.67743406 0.65793015 0.55310988 0.6848397 ] 0.6347804272090081\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848397001753459 0.6247685726157405 0.6347804272090081\n",
      "(0.6848397001753459, 0.6247685726157405, 0.6347804272090081, 0.673752595269545, 0.6087921912196735, -1167.1112847937889, -1003.321188493001)\n",
      "lambda:  4.9\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.392427206039429\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 4.9, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.221203\n",
      "EV: 0.622139\n",
      "EV for each Fold: [0.68010153 0.55770333 0.67176197 0.65341196 0.54771775]\n",
      "MAE: 1168.712983\n",
      "MAE for each Fold: [1061.14032208 1300.82578032 1157.70553624 1072.53654803 1251.35673027]\n",
      "MSE: 3145623.488546\n",
      "MSE for each Fold: [2342073.89025701 3950645.06662442 2345584.02853571 2499491.96660801\n",
      " 4590322.49070556]\n",
      "RMSE: 1754.603693\n",
      "RMSE for each Fold: [1530.38357619 1987.62296893 1531.52996332 1580.97816766 2142.50379013]\n",
      "XV R2 Actuals: [0.61848296 0.5571823  0.66316348 0.65310432 0.53849259]\n",
      "Cross Validation R2: 0.606085\n",
      "XVR_fit_time Actuals:  [0.76220751 1.45180249 1.9564569  2.8094275  3.47105575]\n",
      "XVR_fit_time: 10.450950\n",
      "score_time Actuals:  [0.12580013 0.14040017 0.15200424 0.14040041 0.14040041]\n",
      "score_time: 0.699005\n",
      " \n",
      " \n",
      "Prediction Time:  0.04200243949890137\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1004.684238\n",
      "MSE: 1955922.505321\n",
      "RMSE: 1398.542994\n",
      "Validation Set R2: 0.673600\n",
      "Total Mean EV:  [0.68010153 0.55770333 0.67176197 0.65341196 0.54771775 0.68299512] 0.6322819446911226\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6829951160024678 0.6221393104288536 0.6322819446911226\n",
      "(0.6829951160024678, 0.6221393104288536, 0.6322819446911226, 0.6735997775145357, 0.6060851305317454, -1168.712983386174, -1004.6842384873871)\n",
      "lambda:  4.95\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.509481430053711\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 4.95, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.340627\n",
      "EV: 0.622850\n",
      "EV for each Fold: [0.68159375 0.55522023 0.67548183 0.65203853 0.54991711]\n",
      "MAE: 1169.221048\n",
      "MAE for each Fold: [1055.86423955 1301.30688686 1151.8178888  1077.91542983 1259.2007974 ]\n",
      "MSE: 3145454.322271\n",
      "MSE for each Fold: [2321458.23485661 3971430.68924816 2319152.10954487 2510295.41458452\n",
      " 4604935.16312167]\n",
      "RMSE: 1753.931364\n",
      "RMSE for each Fold: [1523.63323502 1992.84487335 1522.87626206 1584.39118105 2145.91126637]\n",
      "XV R2 Actuals: [0.62184119 0.55485249 0.66695923 0.65160495 0.53702344]\n",
      "Cross Validation R2: 0.606456\n",
      "XVR_fit_time Actuals:  [0.82101655 1.37443185 2.15962315 2.77180886 3.50185919]\n",
      "XVR_fit_time: 10.628740\n",
      "score_time Actuals:  [0.13000774 0.12480044 0.13800788 0.14040017 0.14880157]\n",
      "score_time: 0.682018\n",
      " \n",
      " \n",
      "Prediction Time:  0.04300236701965332\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.142113\n",
      "MSE: 1951119.677318\n",
      "RMSE: 1396.824856\n",
      "Validation Set R2: 0.674401\n",
      "Total Mean EV:  [0.68159375 0.55522023 0.67548183 0.65203853 0.54991711 0.6832415 ] 0.6329154911152366\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6832415049461509 0.6228502883490538 0.6329154911152366\n",
      "(0.6832415049461509, 0.6228502883490538, 0.6329154911152366, 0.6744012633220939, 0.6064562600668235, -1169.2210484905015, -1003.1421128944012)\n",
      "lambda:  5\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.647459030151367\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.903595\n",
      "EV: 0.623928\n",
      "EV for each Fold: [0.68190691 0.56076369 0.67496651 0.65102564 0.55097968]\n",
      "MAE: 1170.456117\n",
      "MAE for each Fold: [1058.7341904  1298.82717459 1154.30082154 1081.80776398 1258.61063623]\n",
      "MSE: 3138426.957409\n",
      "MSE for each Fold: [2331326.57835202 3923153.33696417 2329060.63802474 2517511.5149839\n",
      " 4591082.71871968]\n",
      "RMSE: 1752.607481\n",
      "RMSE for each Fold: [1526.8682256  1980.69516508 1526.12602298 1586.66679394 2142.68119857]\n",
      "XV R2 Actuals: [0.62023367 0.56026378 0.66553632 0.65060345 0.53841615]\n",
      "Cross Validation R2: 0.607011\n",
      "XVR_fit_time Actuals:  [0.86841178 1.39685535 2.24120522 2.932827   3.74005842]\n",
      "XVR_fit_time: 11.179358\n",
      "score_time Actuals:  [0.14040017 0.12480044 0.14300823 0.14240026 0.13260245]\n",
      "score_time: 0.683212\n",
      " \n",
      " \n",
      "Prediction Time:  0.04679989814758301\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 1000.375395\n",
      "MSE: 1928647.830256\n",
      "RMSE: 1388.757657\n",
      "Validation Set R2: 0.678151\n",
      "Total Mean EV:  [0.68190691 0.56076369 0.67496651 0.65102564 0.55097968 0.68559582] 0.6342063760826037\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6855958186900012 0.6239284875611242 0.6342063760826037\n",
      "(0.6855958186900012, 0.6239284875611242, 0.6342063760826037, 0.6781513177647969, 0.6070106735659544, -1170.4561173464306, -1000.3753948798661)\n",
      "lambda:  5.05\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.6175007820129395\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5.05, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.743078\n",
      "EV: 0.623211\n",
      "EV for each Fold: [0.68058435 0.56028828 0.67280508 0.65401448 0.54836285]\n",
      "MAE: 1165.690000\n",
      "MAE for each Fold: [1060.63258729 1302.03573506 1155.90623577 1071.39002429 1238.48541597]\n",
      "MSE: 3136176.972381\n",
      "MSE for each Fold: [2340111.11729684 3927981.5672502  2338358.59517269 2499871.89231367\n",
      " 4574561.6898692 ]\n",
      "RMSE: 1752.149171\n",
      "RMSE for each Fold: [1529.74217347 1981.91361246 1529.16925001 1581.09831836 2138.82250079]\n",
      "XV R2 Actuals: [0.61880269 0.55972259 0.66420109 0.6530516  0.54007716]\n",
      "Cross Validation R2: 0.607171\n",
      "XVR_fit_time Actuals:  [0.7862041  1.60265636 2.1684041  3.54989672 3.76551247]\n",
      "XVR_fit_time: 11.872674\n",
      "score_time Actuals:  [0.12480021 0.14039993 0.14040017 0.14039993 0.14040041]\n",
      "score_time: 0.686401\n",
      " \n",
      " \n",
      "Prediction Time:  0.04680013656616211\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.016015\n",
      "MSE: 1938840.744226\n",
      "RMSE: 1392.422617\n",
      "Validation Set R2: 0.676450\n",
      "Total Mean EV:  [0.68058435 0.56028828 0.67280508 0.65401448 0.54836285 0.68449009] 0.6334241885161026\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6844900869808423 0.6232110088231547 0.6334241885161026\n",
      "(0.6844900869808423, 0.6232110088231547, 0.6334241885161026, 0.6764503457792699, 0.607171026273924, -1165.6899996764412, -1003.0160149557893)\n",
      "lambda:  5.1\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.624453783035278\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5.1, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.235903\n",
      "EV: 0.622099\n",
      "EV for each Fold: [0.67827806 0.55544513 0.67283773 0.65722789 0.54670477]\n",
      "MAE: 1165.044392\n",
      "MAE for each Fold: [1063.32695927 1306.86342335 1150.15800138 1065.58451482 1239.28906126]\n",
      "MSE: 3143069.890304\n",
      "MSE for each Fold: [2346860.39933606 3971022.66403558 2322673.1624753  2476156.95773029\n",
      " 4598636.26794164]\n",
      "RMSE: 1753.349006\n",
      "RMSE for each Fold: [1531.9466046  1992.74249818 1524.03187712 1573.5809346  2144.44311371]\n",
      "XV R2 Actuals: [0.61770325 0.55489823 0.66645359 0.65634291 0.53765672]\n",
      "Cross Validation R2: 0.606611\n",
      "XVR_fit_time Actuals:  [0.76900506 1.52880263 2.18600392 2.99549007 3.84910774]\n",
      "XVR_fit_time: 11.328409\n",
      "score_time Actuals:  [0.14040017 0.14040041 0.14040041 0.15200877 0.14500809]\n",
      "score_time: 0.718218\n",
      " \n",
      " \n",
      "Prediction Time:  0.04600262641906738\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1004.808354\n",
      "MSE: 1954644.374406\n",
      "RMSE: 1398.085968\n",
      "Validation Set R2: 0.673813\n",
      "Total Mean EV:  [0.67827806 0.55544513 0.67283773 0.65722789 0.54670477 0.68207322] 0.6320944666080304\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6820732201846502 0.6220987158927064 0.6320944666080304\n",
      "(0.6820732201846502, 0.6220987158927064, 0.6320944666080304, 0.673813069305989, 0.6066109397584356, -1165.0443920150708, -1004.8083542090133)\n",
      "lambda:  5.15\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.653287172317505\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5.15, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.096188\n",
      "EV: 0.622166\n",
      "EV for each Fold: [0.67763533 0.55619099 0.66959813 0.65840781 0.54899592]\n",
      "MAE: 1166.609409\n",
      "MAE for each Fold: [1064.50161984 1307.01213419 1155.75658187 1060.54318031 1245.23352725]\n",
      "MSE: 3144378.467793\n",
      "MSE for each Fold: [2350709.28840824 3965088.74957525 2355705.04588228 2468399.14412257\n",
      " 4581990.11097564]\n",
      "RMSE: 1754.191666\n",
      "RMSE for each Fold: [1533.20229859 1991.25306015 1534.83062449 1571.1139819  2140.5583643 ]\n",
      "XV R2 Actuals: [0.61707628 0.55556334 0.66171006 0.65741959 0.53933032]\n",
      "Cross Validation R2: 0.606220\n",
      "XVR_fit_time Actuals:  [0.77760458 1.47223926 2.28324676 2.9002564  3.73840737]\n",
      "XVR_fit_time: 11.171754\n",
      "score_time Actuals:  [0.14880586 0.14040017 0.14040041 0.13360214 0.14040041]\n",
      "score_time: 0.703609\n",
      " \n",
      " \n",
      "Prediction Time:  0.04679989814758301\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 997.757510\n",
      "MSE: 1925721.491241\n",
      "RMSE: 1387.703676\n",
      "Validation Set R2: 0.678640\n",
      "Total Mean EV:  [0.67763533 0.55619099 0.66959813 0.65840781 0.54899592 0.68809244] 0.6331534386887346\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6880924420830434 0.6221656380098729 0.6331534386887346\n",
      "(0.6880924420830434, 0.6221656380098729, 0.6331534386887346, 0.6786396590476891, 0.606219917188118, -1166.6094086917828, -997.7575101007626)\n",
      "lambda:  5.2\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.712062120437622\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 5.2, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.295324\n",
      "EV: 0.622808\n",
      "EV for each Fold: [0.67916648 0.55607973 0.67197147 0.65739798 0.54942235]\n",
      "MAE: 1166.319004\n",
      "MAE for each Fold: [1060.50975195 1300.86510606 1155.76602309 1063.52971626 1250.92442432]\n",
      "MSE: 3140315.661680\n",
      "MSE for each Fold: [2336146.77174138 3963536.20167018 2346920.90554484 2475985.58547656\n",
      " 4578988.84396851]\n",
      "RMSE: 1752.931817\n",
      "RMSE for each Fold: [1528.44586811 1990.86318005 1531.96635262 1573.5264807  2139.85720177]\n",
      "XV R2 Actuals: [0.61944847 0.55573736 0.6629715  0.65636669 0.53963206]\n",
      "Cross Validation R2: 0.606831\n",
      "XVR_fit_time Actuals:  [0.92500544 1.43520236 2.18600392 2.91720533 3.92025828]\n",
      "XVR_fit_time: 11.383675\n",
      "score_time Actuals:  [0.14040017 0.14040041 0.15600038 0.17160034 0.14840508]\n",
      "score_time: 0.756806\n",
      " \n",
      " \n",
      "Prediction Time:  0.04400229454040527\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 994.255674\n",
      "MSE: 1902153.210325\n",
      "RMSE: 1379.185706\n",
      "Validation Set R2: 0.682573\n",
      "Total Mean EV:  [0.67916648 0.55607973 0.67197147 0.65739798 0.54942235 0.69095857] 0.634166096229603\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6909585650138084 0.6228076024727619 0.634166096229603\n",
      "(0.6909585650138084, 0.6228076024727619, 0.634166096229603, 0.6825726840596338, 0.6068312181561926, -1166.3190043356826, -994.2556736105098)\n"
     ]
    }
   ],
   "source": [
    "Lamb = [4.8,4.9,4.95,5,5.05,5.1,5.15,5.2] \n",
    "\n",
    "MdlParams= {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 4.8}\n",
    "Mdl=xgb.XGBRegressor(random_state=0, n_jobs=-1)\n",
    "Mdl.set_params(**MdlParams)\n",
    "MdlParams={}\n",
    "print(Mdl.get_params(deep=True))\n",
    "for i in Lamb:\n",
    "    MdlParams['reg_lambda']=i\n",
    "    print(\"lambda: \",i)\n",
    "    Comment=\"Regularization Study, reg_lambda = \"+str(i)\n",
    "    print(CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "           SplitPercent=100, Mdl=Mdl, MdlParams=MdlParams,verbose=True,AlgName=\"XGBRegressor\",\n",
    "           Comment=Comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T16:35:09.666375Z",
     "start_time": "2019-06-22T16:30:00.526786Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      "lambda:  0\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  6.129440546035767\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.075587\n",
      "EV: 0.624769\n",
      "EV for each Fold: [0.68141772 0.55395105 0.67743406 0.65793015 0.55310988]\n",
      "MAE: 1167.111285\n",
      "MAE for each Fold: [1051.65082922 1310.90438875 1150.13958852 1065.57996828 1257.28164919]\n",
      "MSE: 3127344.130964\n",
      "MSE for each Fold: [2308277.87014384 3985201.97191318 2311925.61641604 2468419.03535473\n",
      " 4562896.16099134]\n",
      "RMSE: 1748.662916\n",
      "RMSE for each Fold: [1519.3017706  1996.29706505 1520.50176469 1571.12031218 2136.09366859]\n",
      "XV R2 Actuals: [0.62398823 0.5533089  0.66799699 0.65741683 0.54125001]\n",
      "Cross Validation R2: 0.608792\n",
      "XVR_fit_time Actuals:  [0.77303123 1.29161239 2.20060372 2.63365006 3.47503018]\n",
      "XVR_fit_time: 10.373928\n",
      "score_time Actuals:  [0.13100743 0.14040041 0.14040041 0.14040041 0.14040041]\n",
      "score_time: 0.692609\n",
      " \n",
      " \n",
      "Prediction Time:  0.04679989814758301\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.321188\n",
      "MSE: 1955006.759357\n",
      "RMSE: 1398.215563\n",
      "Validation Set R2: 0.673753\n",
      "Total Mean EV:  [0.68141772 0.55395105 0.67743406 0.65793015 0.55310988 0.6848397 ] 0.6347804272090081\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848397001753459 0.6247685726157405 0.6347804272090081\n",
      "(0.6848397001753459, 0.6247685726157405, 0.6347804272090081, 0.673752595269545, 0.6087921912196735, -1167.1112847937889, -1003.321188493001)\n",
      "lambda:  0.1\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.379698276519775\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0.1, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.360085\n",
      "EV: 0.623745\n",
      "EV for each Fold: [0.68115817 0.55395104 0.67743408 0.65306955 0.55310992]\n",
      "MAE: 1169.013427\n",
      "MAE for each Fold: [1052.87267784 1310.90424963 1150.13953869 1073.86911792 1257.28154985]\n",
      "MSE: 3135180.573056\n",
      "MSE for each Fold: [2313539.58489545 3985201.8986114  2311925.22434519 2502341.11987279\n",
      " 4562895.03755502]\n",
      "RMSE: 1751.160696\n",
      "RMSE for each Fold: [1521.03240758 1996.29704669 1520.50163576 1581.87898395 2136.09340563]\n",
      "XV R2 Actuals: [0.62313112 0.55330891 0.66799704 0.6527089  0.54125012]\n",
      "Cross Validation R2: 0.607679\n",
      "XVR_fit_time Actuals:  [0.77020764 1.4820025  1.99680328 2.86565924 3.49640632]\n",
      "XVR_fit_time: 10.611079\n",
      "score_time Actuals:  [0.14040041 0.12480021 0.14040041 0.14040017 0.14040017]\n",
      "score_time: 0.686401\n",
      " \n",
      " \n",
      "Prediction Time:  0.05720162391662598\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.321300\n",
      "MSE: 1955006.955565\n",
      "RMSE: 1398.215633\n",
      "Validation Set R2: 0.673753\n",
      "Total Mean EV:  [0.68115817 0.55395104 0.67743408 0.65306955 0.55310992 0.68483966] 0.6339270682105967\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848396553620539 0.6237445507803053 0.6339270682105967\n",
      "(0.6848396553620539, 0.6237445507803053, 0.6339270682105967, 0.67375256252693, 0.6076792178023291, -1169.0134267849148, -1003.3212999915849)\n",
      "lambda:  0.5\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.434830665588379\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 0.5, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.432590\n",
      "EV: 0.624067\n",
      "EV for each Fold: [0.68115752 0.55395098 0.67738023 0.65306994 0.55477496]\n",
      "MAE: 1167.325667\n",
      "MAE for each Fold: [1052.87411944 1310.90388005 1148.77869697 1073.8684371  1250.20320046]\n",
      "MSE: 3129746.676202\n",
      "MSE for each Fold: [2313545.75137614 3985202.03157395 2309752.41399287 2502338.76965574\n",
      " 4537894.41440904]\n",
      "RMSE: 1749.846027\n",
      "RMSE for each Fold: [1521.03443465 1996.29707999 1519.78696336 1581.8782411  2130.23341782]\n",
      "XV R2 Actuals: [0.62313011 0.5533089  0.66830907 0.65270923 0.54376366]\n",
      "Cross Validation R2: 0.608244\n",
      "XVR_fit_time Actuals:  [0.80640793 1.38602734 2.17040825 2.72485828 3.63625956]\n",
      "XVR_fit_time: 10.723961\n",
      "score_time Actuals:  [0.12480021 0.13500762 0.14040041 0.14039993 0.15600014]\n",
      "score_time: 0.696608\n",
      " \n",
      " \n",
      "Prediction Time:  0.031199932098388672\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.233885\n",
      "MSE: 1954954.734528\n",
      "RMSE: 1398.196958\n",
      "Validation Set R2: 0.673761\n",
      "Total Mean EV:  [0.68115752 0.55395098 0.67738023 0.65306994 0.55477496 0.68486294] 0.634199429097573\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848629412194711 0.6240667266731934 0.634199429097573\n",
      "(0.6848629412194711, 0.6240667266731934, 0.634199429097573, 0.6737612770633745, 0.6082441928428538, -1167.3256668044241, -1003.2338852105889)\n",
      "lambda:  1\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.505261421203613\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 1, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.911957\n",
      "EV: 0.624984\n",
      "EV for each Fold: [0.68115974 0.55431225 0.67734179 0.65598669 0.55611872]\n",
      "MAE: 1165.556846\n",
      "MAE for each Fold: [1052.86958816 1311.44827437 1148.61288493 1065.76747982 1249.08600243]\n",
      "MSE: 3122662.208021\n",
      "MSE for each Fold: [2313537.03054292 3982318.49638298 2310031.91501115 2482678.46168519\n",
      " 4524745.13648396]\n",
      "RMSE: 1747.856360\n",
      "RMSE for each Fold: [1521.0315679  1995.57472834 1519.87891459 1575.65175775 2127.14483204]\n",
      "XV R2 Actuals: [0.62313153 0.5536321  0.66826893 0.65543781 0.54508568]\n",
      "Cross Validation R2: 0.609111\n",
      "XVR_fit_time Actuals:  [0.78460503 1.5766027  2.13920403 2.93584847 3.69103384]\n",
      "XVR_fit_time: 11.127294\n",
      "score_time Actuals:  [0.1414001  0.12480021 0.14039993 0.14040041 0.14400816]\n",
      "score_time: 0.691009\n",
      " \n",
      " \n",
      "Prediction Time:  0.04100227355957031\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.919223\n",
      "MSE: 1954891.536926\n",
      "RMSE: 1398.174359\n",
      "Validation Set R2: 0.673772\n",
      "Total Mean EV:  [0.68115974 0.55431225 0.67734179 0.65598669 0.55611872 0.68477182] 0.6349484984802847\n",
      "Validation Set EV, XVal EV, Mean EV:  0.684771816108193 0.624983834954703 0.6349484984802847\n",
      "(0.684771816108193, 0.624983834954703, 0.6349484984802847, 0.6737718233458776, 0.6091112116328816, -1165.5568459404974, -1003.9192229442782)\n",
      "lambda:  4.8\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.582468271255493\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 4.8, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.602407\n",
      "EV: 0.623548\n",
      "EV for each Fold: [0.68055193 0.55340305 0.67772345 0.6511228  0.55493712]\n",
      "MAE: 1167.656644\n",
      "MAE for each Fold: [1056.61739841 1312.64071947 1148.87308396 1073.83087985 1246.32114002]\n",
      "MSE: 3133246.494398\n",
      "MSE for each Fold: [2318326.97726656 3990583.0209523  2309910.37823052 2516387.16349614\n",
      " 4531024.93204326]\n",
      "RMSE: 1751.004299\n",
      "RMSE for each Fold: [1522.60532551 1997.64436799 1519.83893167 1586.31244195 2128.6204293 ]\n",
      "XV R2 Actuals: [0.62235126 0.55270576 0.66828638 0.6507595  0.54445432]\n",
      "Cross Validation R2: 0.607711\n",
      "XVR_fit_time Actuals:  [0.75500441 1.48200226 2.2474041  2.94381428 4.30108595]\n",
      "XVR_fit_time: 11.729311\n",
      "score_time Actuals:  [0.12480021 0.14040041 0.14040017 0.14300823 0.1440084 ]\n",
      "score_time: 0.692617\n",
      " \n",
      " \n",
      "Prediction Time:  0.04500269889831543\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.225569\n",
      "MSE: 1954828.248899\n",
      "RMSE: 1398.151726\n",
      "Validation Set R2: 0.673782\n",
      "Total Mean EV:  [0.68055193 0.55340305 0.67772345 0.6511228  0.55493712 0.68488091] 0.6337698775087758\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848809112862666 0.6235476707532775 0.6337698775087758\n",
      "(0.6848809112862666, 0.6235476707532775, 0.6337698775087758, 0.6737823847182864, 0.6077114431975646, -1167.6566443414156, -1003.2255685308143)\n",
      "lambda:  4.9\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.635479688644409\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 4.9, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 13.923898\n",
      "EV: 0.622514\n",
      "EV for each Fold: [0.68055366 0.55340287 0.67772347 0.6511228  0.5497696 ]\n",
      "MAE: 1169.065077\n",
      "MAE for each Fold: [1056.60882392 1312.63744838 1148.87306608 1073.83089164 1253.3751535 ]\n",
      "MSE: 3143520.441930\n",
      "MSE for each Fold: [2318297.48729156 3990584.06867944 2309910.04666583 2516387.11040286\n",
      " 4582423.49660999]\n",
      "RMSE: 1753.410223\n",
      "RMSE for each Fold: [1522.59564143 1997.64463023 1519.83882259 1586.31242522 2140.65959382]\n",
      "XV R2 Actuals: [0.62235607 0.55270564 0.66828643 0.65075951 0.53928674]\n",
      "Cross Validation R2: 0.606679\n",
      "XVR_fit_time Actuals:  [0.77060437 1.45444512 2.32525802 2.91705871 3.58203626]\n",
      "XVR_fit_time: 11.049402\n",
      "score_time Actuals:  [0.14040041 0.14000797 0.14039993 0.1972096  0.14500833]\n",
      "score_time: 0.763026\n",
      " \n",
      " \n",
      "Prediction Time:  0.04679989814758301\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.269681\n",
      "MSE: 1954816.460703\n",
      "RMSE: 1398.147510\n",
      "Validation Set R2: 0.673784\n",
      "Total Mean EV:  [0.68055366 0.55340287 0.67772347 0.6511228  0.5497696  0.68487689] 0.6329082143541994\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848768893163636 0.6225144793617666 0.6329082143541994\n",
      "(0.6848768893163636, 0.6225144793617666, 0.6329082143541994, 0.6737843519076516, 0.6066788772173324, -1169.0650767014922, -1003.2696811635802)\n",
      "lambda:  4.95\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.689466238021851\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 4.95, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 28.473720\n",
      "EV: 0.623393\n",
      "EV for each Fold: [0.68087481 0.55340037 0.67772348 0.65519911 0.5497696 ]\n",
      "MAE: 1167.940556\n",
      "MAE for each Fold: [1055.97782319 1312.74840981 1148.87301612 1068.72843153 1253.37509787]\n",
      "MSE: 3137182.573538\n",
      "MSE for each Fold: [2314838.28640631 3990613.4479999  2309909.8710647  2488127.94803833\n",
      " 4582423.31417829]\n",
      "RMSE: 1751.397930\n",
      "RMSE for each Fold: [1521.45926216 1997.6519837  1519.83876482 1577.38008991 2140.65955121]\n",
      "XV R2 Actuals: [0.62291956 0.55270235 0.66828646 0.6546815  0.53928676]\n",
      "Cross Validation R2: 0.607575\n",
      "XVR_fit_time Actuals:  [1.11880517 1.49960256 7.27232432 7.32266331 6.61399007]\n",
      "XVR_fit_time: 23.827385\n",
      "score_time Actuals:  [0.14040017 0.20280051 0.21840072 0.3138113  0.22301245]\n",
      "score_time: 1.098425\n",
      " \n",
      " \n",
      "Prediction Time:  0.06600356101989746\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.269776\n",
      "MSE: 1954816.714095\n",
      "RMSE: 1398.147601\n",
      "Validation Set R2: 0.673784\n",
      "Total Mean EV:  [0.68087481 0.55340037 0.67772348 0.65519911 0.5497696  0.68487685] 0.6336407029245379\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848768477643494 0.6233934739565756 0.6336407029245379\n",
      "(0.6848768477643494, 0.6233934739565756, 0.6336407029245379, 0.6737843096221776, 0.6075753239746698, -1167.9405557042487, -1003.2697758418531)\n",
      "lambda:  5\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.79206657409668\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 5, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.382631\n",
      "EV: 0.622578\n",
      "EV for each Fold: [0.68087479 0.55340036 0.67772353 0.65112279 0.54976984]\n",
      "MAE: 1168.961308\n",
      "MAE for each Fold: [1055.97995752 1312.74864337 1148.8729008  1073.83102844 1253.3740085 ]\n",
      "MSE: 3142834.293966\n",
      "MSE for each Fold: [2314839.03726358 3990613.51986601 2309909.38281225 2516387.19756807\n",
      " 4582422.33232081]\n",
      "RMSE: 1753.184378\n",
      "RMSE for each Fold: [1521.45950891 1997.65200169 1519.8386042  1586.31245269 2140.65932187]\n",
      "XV R2 Actuals: [0.62291944 0.55270234 0.66828653 0.65075949 0.53928686]\n",
      "Cross Validation R2: 0.606791\n",
      "XVR_fit_time Actuals:  [0.91081905 1.4576261  2.2436409  2.9972055  3.89486003]\n",
      "XVR_fit_time: 11.504152\n",
      "score_time Actuals:  [0.13700795 0.14040041 0.14500833 0.15600038 0.14040041]\n",
      "score_time: 0.718817\n",
      " \n",
      " \n",
      "Prediction Time:  0.044002532958984375\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.272753\n",
      "MSE: 1954816.525953\n",
      "RMSE: 1398.147534\n",
      "Validation Set R2: 0.673784\n",
      "Total Mean EV:  [0.68087479 0.55340036 0.67772353 0.65112279 0.54976984 0.68487643] 0.6329612912186228\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848764331085584 0.6225782628406357 0.6329612912186228\n",
      "(0.6848764331085584, 0.6225782628406357, 0.6329612912186228, 0.6737843410188114, 0.6067909313486162, -1168.9613077244303, -1003.2727525256219)\n",
      "lambda:  5.05\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.798666000366211\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 5.05, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 15.011188\n",
      "EV: 0.622575\n",
      "EV for each Fold: [0.68087469 0.55340035 0.67772354 0.65112278 0.54975225]\n",
      "MAE: 1169.006190\n",
      "MAE for each Fold: [1055.98017482 1312.74863554 1148.87286982 1073.8310607  1253.59820865]\n",
      "MSE: 3142910.859376\n",
      "MSE for each Fold: [2314839.70524286 3990613.57717075 2309909.187216   2516387.27834292\n",
      " 4582804.54890849]\n",
      "RMSE: 1753.202272\n",
      "RMSE for each Fold: [1521.45972843 1997.65201604 1519.83853985 1586.31247815 2140.74859545]\n",
      "XV R2 Actuals: [0.62291933 0.55270233 0.66828655 0.65075948 0.53924843]\n",
      "Cross Validation R2: 0.606783\n",
      "XVR_fit_time Actuals:  [0.8018043  1.60444927 2.66860461 3.07563233 3.88744569]\n",
      "XVR_fit_time: 12.037936\n",
      "score_time Actuals:  [0.14040041 0.15600038 0.16120315 0.14040017 0.20280027]\n",
      "score_time: 0.800804\n",
      " \n",
      " \n",
      "Prediction Time:  0.045601844787597656\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.273172\n",
      "MSE: 1954816.875623\n",
      "RMSE: 1398.147659\n",
      "Validation Set R2: 0.673784\n",
      "Total Mean EV:  [0.68087469 0.55340035 0.67772354 0.65112278 0.54975225 0.68487632] 0.6329583233297225\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848763222234637 0.6225747235509742 0.6329583233297225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6848763222234637, 0.6225747235509742, 0.6329583233297225, 0.6737842826666404, 0.6067832261261662, -1169.0061899064467, -1003.273172080917)\n",
      "lambda:  5.1\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.771871328353882\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 5.1, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.507246\n",
      "EV: 0.622574\n",
      "EV for each Fold: [0.68087462 0.55340035 0.67772386 0.65112128 0.54975226]\n",
      "MAE: 1169.007422\n",
      "MAE for each Fold: [1055.98037524 1312.74858104 1148.8721396  1073.83790927 1253.59810293]\n",
      "MSE: 3142912.699159\n",
      "MSE for each Fold: [2314840.27836583 3990613.52919755 2309907.49656631 2516397.84603746\n",
      " 4582804.3456254 ]\n",
      "RMSE: 1753.202852\n",
      "RMSE for each Fold: [1521.45991678 1997.65200403 1519.83798366 1586.31580905 2140.74854797]\n",
      "XV R2 Actuals: [0.62291924 0.55270234 0.6682868  0.65075802 0.53924845]\n",
      "Cross Validation R2: 0.606783\n",
      "XVR_fit_time Actuals:  [0.80822706 1.64441705 2.21520376 3.17425847 3.77988815]\n",
      "XVR_fit_time: 11.621994\n",
      "score_time Actuals:  [0.13800788 0.14039993 0.15600038 0.15600038 0.15600038]\n",
      "score_time: 0.746409\n",
      " \n",
      " \n",
      "Prediction Time:  0.044002532958984375\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 1003.298054\n",
      "MSE: 1955645.783707\n",
      "RMSE: 1398.444058\n",
      "Validation Set R2: 0.673646\n",
      "Total Mean EV:  [0.68087462 0.55340035 0.67772386 0.65112128 0.54975226 0.6851378 ] 0.6330016961709564\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6851377982047073 0.6225744757642062 0.6330016961709564\n",
      "(0.6851377982047073, 0.6225744757642062, 0.6330016961709564, 0.6736459562338772, 0.6067829678435575, -1169.0074216158264, -1003.2980535801413)\n",
      "lambda:  5.15\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  5.6859540939331055\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 5.15, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.618192\n",
      "EV: 0.622578\n",
      "EV for each Fold: [0.68087451 0.55340036 0.67772387 0.65112127 0.54976946]\n",
      "MAE: 1168.964516\n",
      "MAE for each Fold: [1055.98061204 1312.74855415 1148.87212268 1073.83792828 1253.38336384]\n",
      "MSE: 3142836.935254\n",
      "MSE for each Fold: [2314841.05048022 3990613.4490061  2309907.37832202 2516397.88917062\n",
      " 4582424.90929058]\n",
      "RMSE: 1753.185169\n",
      "RMSE for each Fold: [1521.46017052 1997.65198396 1519.83794476 1586.31582264 2140.65992378]\n",
      "XV R2 Actuals: [0.62291911 0.55270234 0.66828681 0.65075801 0.5392866 ]\n",
      "Cross Validation R2: 0.606791\n",
      "XVR_fit_time Actuals:  [0.83100414 1.6390028  2.21620393 3.08506417 3.96005368]\n",
      "XVR_fit_time: 11.731329\n",
      "score_time Actuals:  [0.14040041 0.14040041 0.14040017 0.14900851 0.1559999 ]\n",
      "score_time: 0.726209\n",
      " \n",
      " \n",
      "Prediction Time:  0.05680060386657715\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.878484\n",
      "MSE: 1955126.047370\n",
      "RMSE: 1398.258219\n",
      "Validation Set R2: 0.673733\n",
      "Total Mean EV:  [0.68087451 0.55340036 0.67772387 0.65112127 0.54976946 0.68476851] 0.6329429974482281\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6847685092381545 0.6225778950902429 0.6329429974482281\n",
      "(0.6847685092381545, 0.6225778950902429, 0.6329429974482281, 0.6737326887376216, 0.6067905763312008, -1168.964516197745, -1003.8784844232815)\n",
      "lambda:  5.2\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.779233694076538\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 5.2, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.580332\n",
      "EV: 0.622574\n",
      "EV for each Fold: [0.68087443 0.55340038 0.67772386 0.65112128 0.54975228]\n",
      "MAE: 1169.007502\n",
      "MAE for each Fold: [1055.98079874 1312.74851618 1148.87213687 1073.83783427 1253.59822364]\n",
      "MSE: 3142913.119520\n",
      "MSE for each Fold: [2314841.59455582 3990613.30395569 2309907.35800385 2516397.87446647\n",
      " 4582805.46662051]\n",
      "RMSE: 1753.202973\n",
      "RMSE for each Fold: [1521.46034932 1997.65194765 1519.83793807 1586.31581801 2140.74880979]\n",
      "XV R2 Actuals: [0.62291902 0.55270236 0.66828682 0.65075801 0.53924834]\n",
      "Cross Validation R2: 0.606783\n",
      "XVR_fit_time Actuals:  [0.78820467 1.62240291 2.27760386 3.14504576 3.87485909]\n",
      "XVR_fit_time: 11.708116\n",
      "score_time Actuals:  [0.14040017 0.14140034 0.15600014 0.15600038 0.14040017]\n",
      "score_time: 0.734201\n",
      " \n",
      " \n",
      "Prediction Time:  0.04780006408691406\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 1003.298988\n",
      "MSE: 1955646.752517\n",
      "RMSE: 1398.444405\n",
      "Validation Set R2: 0.673646\n",
      "Total Mean EV:  [0.68087443 0.55340038 0.67772386 0.65112128 0.54975228 0.68513745] 0.6330016124728514\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6851374539338169 0.6225744441806582 0.6330016124728514\n",
      "(0.6851374539338169, 0.6225744441806582, 0.6330016124728514, 0.6736457945609369, 0.6067829106619422, -1169.0075019422716, -1003.2989877271261)\n",
      "lambda:  7.5\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.8178651332855225\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 7.5, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.590789\n",
      "EV: 0.625320\n",
      "EV for each Fold: [0.67978677 0.55283042 0.67670559 0.65491709 0.5623606 ]\n",
      "MAE: 1170.009288\n",
      "MAE for each Fold: [1058.39771335 1313.45879537 1150.07042525 1069.16141648 1258.95809198]\n",
      "MSE: 3121769.831233\n",
      "MSE for each Fold: [2323566.75849603 3996089.52610125 2317174.15106754 2490032.81962163\n",
      " 4481985.900879  ]\n",
      "RMSE: 1748.125555\n",
      "RMSE for each Fold: [1524.32501734 1999.02214247 1522.22670817 1577.98378307 2117.07012186]\n",
      "XV R2 Actuals: [0.62149772 0.55208855 0.66724327 0.65441713 0.54938466]\n",
      "Cross Validation R2: 0.608926\n",
      "XVR_fit_time Actuals:  [0.79920506 1.57560277 2.21005702 3.07584786 3.88725638]\n",
      "XVR_fit_time: 11.547969\n",
      "score_time Actuals:  [0.14040017 0.14040017 0.14040017 0.15600038 0.14040446]\n",
      "score_time: 0.717605\n",
      " \n",
      " \n",
      "Prediction Time:  0.04680013656616211\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1004.439641\n",
      "MSE: 1956523.784300\n",
      "RMSE: 1398.757943\n",
      "Validation Set R2: 0.673499\n",
      "Total Mean EV:  [0.67978677 0.55283042 0.67670559 0.65491709 0.5623606  0.68440749] 0.6351679947983984\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6844074889879175 0.6253200959604945 0.6351679947983984\n",
      "(0.6844074889879175, 0.6253200959604945, 0.6351679947983984, 0.6734994373466769, 0.6089262644192486, -1170.0092884878536, -1004.4396408058008)\n",
      "lambda:  10\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  5.011096239089966\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 10, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.609097\n",
      "EV: 0.624376\n",
      "EV for each Fold: [0.68064266 0.5527759  0.67656795 0.66074904 0.55114375]\n",
      "MAE: 1166.563245\n",
      "MAE for each Fold: [1056.93124953 1313.45735488 1150.23604413 1060.82572576 1251.36584885]\n",
      "MSE: 3131298.661359\n",
      "MSE for each Fold: [2318875.65089331 3996531.65899872 2317316.43025003 2446587.99739712\n",
      " 4577181.56925434]\n",
      "RMSE: 1749.556762\n",
      "RMSE for each Fold: [1522.78549077 1999.13272671 1522.27344135 1564.15728026 2139.43487147]\n",
      "XV R2 Actuals: [0.62226189 0.55203899 0.66722284 0.66044668 0.53981376]\n",
      "Cross Validation R2: 0.608357\n",
      "XVR_fit_time Actuals:  [0.85961819 1.49322748 2.33845663 3.1792593  3.78190136]\n",
      "XVR_fit_time: 11.652463\n",
      "score_time Actuals:  [0.14200807 0.14040041 0.14040041 0.14940047 0.18720031]\n",
      "score_time: 0.759410\n",
      " \n",
      " \n",
      "Prediction Time:  0.04680013656616211\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1003.761581\n",
      "MSE: 1953773.157528\n",
      "RMSE: 1397.774359\n",
      "Validation Set R2: 0.673958\n",
      "Total Mean EV:  [0.68064266 0.5527759  0.67656795 0.66074904 0.55114375 0.68489252] 0.6344619716140109\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6848925202652224 0.6243758618837685 0.6344619716140109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6848925202652224, 0.6243758618837685, 0.6344619716140109, 0.6739584561410333, 0.6083568312880687, -1166.5632446305362, -1003.7615807305624)\n",
      "lambda:  15\n",
      "Current Model:  XGBRegressor\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  4.831918239593506\n",
      " \n",
      "Model Parameters: \n",
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.097, 'max_delta_step': 0, 'max_depth': 11, 'min_child_weight': 1, 'missing': None, 'n_estimators': 53, 'n_jobs': -1, 'nthread': None, 'objective': 'reg:linear', 'random_state': 0, 'reg_alpha': 15, 'reg_lambda': 4.8, 'scale_pos_weight': 1, 'seed': None, 'silent': True, 'subsample': 1}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 14.760041\n",
      "EV: 0.623317\n",
      "EV for each Fold: [0.67973348 0.5535391  0.67691145 0.65607121 0.55032921]\n",
      "MAE: 1167.897991\n",
      "MAE for each Fold: [1059.52063335 1312.1175169  1150.77631521 1068.20471868 1248.87076975]\n",
      "MSE: 3139799.442659\n",
      "MSE for each Fold: [2332664.98552006 3989431.93023426 2315459.9059661  2481392.79997236\n",
      " 4580047.59160093]\n",
      "RMSE: 1752.334904\n",
      "RMSE for each Fold: [1527.30644781 1997.35623519 1521.66353244 1575.24372716 2140.10457492]\n",
      "XV R2 Actuals: [0.62001564 0.55283478 0.66748945 0.65561624 0.53952562]\n",
      "Cross Validation R2: 0.607096\n",
      "XVR_fit_time Actuals:  [0.78420401 1.48200274 2.34443998 2.99070072 4.18485928]\n",
      "XVR_fit_time: 11.786207\n",
      "score_time Actuals:  [0.14040041 0.14760303 0.15600014 0.15600061 0.14040017]\n",
      "score_time: 0.740404\n",
      " \n",
      " \n",
      "Prediction Time:  0.11960601806640625\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.69\n",
      "MAE: 1005.195358\n",
      "MSE: 1953290.503375\n",
      "RMSE: 1397.601697\n",
      "Validation Set R2: 0.674039\n",
      "Total Mean EV:  [0.67973348 0.5535391  0.67691145 0.65607121 0.55032921 0.6853136 ] 0.6336496750348916\n",
      "Validation Set EV, XVal EV, Mean EV:  0.6853136045561039 0.623316889130649 0.6336496750348916\n",
      "(0.6853136045561039, 0.623316889130649, 0.6336496750348916, 0.6740390004481371, 0.6070963452298428, -1167.897990780709, -1005.1953575175144)\n"
     ]
    }
   ],
   "source": [
    "alpha = [0,0.1,0.5,1,4.8,4.9,4.95,5,5.05,5.1,5.15,5.2,7.5,10,15] \n",
    "\n",
    "MdlParams= {'max_depth': 11, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1,\n",
    "            'colsample_bytree': 0.5, 'subsample': 1, 'reg_lambda': 4.8}\n",
    "Mdl=xgb.XGBRegressor(random_state=0, n_jobs=-1)\n",
    "Mdl.set_params(**MdlParams)\n",
    "MdlParams={}\n",
    "print(Mdl.get_params(deep=True))\n",
    "for i in alpha:\n",
    "    MdlParams['reg_alpha']=i\n",
    "    print(\"lambda: \",i)\n",
    "    Comment=\"Regularization Study, reg_alpha = \"+str(i)\n",
    "    print(CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "           SplitPercent=100, Mdl=Mdl, MdlParams=MdlParams,verbose=True,AlgName=\"XGBRegressor\",\n",
    "           Comment=Comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start H2O:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T13:51:28.242647Z",
     "start_time": "2019-06-18T13:51:17.402027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.2+9-LTS, mixed mode)\n",
      "  Starting server from c:\\program files\\python36\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Rodrigo\\AppData\\Local\\Temp\\tmp670sjc7q\n",
      "  JVM stdout: C:\\Users\\Rodrigo\\AppData\\Local\\Temp\\tmp670sjc7q\\h2o_Rodrigo_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Rodrigo\\AppData\\Local\\Temp\\tmp670sjc7q\\h2o_Rodrigo_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n",
      "Warning: Your H2O cluster version is too old (4 months and 2 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/Sao_Paulo</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>4 months and 2 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Rodrigo_ygy2bu</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>8 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       America/Sao_Paulo\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.4\n",
       "H2O cluster version age:    4 months and 2 days !!!\n",
       "H2O cluster name:           H2O_from_python_Rodrigo_ygy2bu\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    8 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init(min_mem_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data Sets to H20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:18:02.960103Z",
     "start_time": "2019-06-15T22:17:59.119884Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "#Convert Data Sets to H20:\n",
    "trainH2O_X=h2o.H2OFrame(trainDataset_X)\n",
    "trainH2O_y=h2o.H2OFrame(trainDataset_y)\n",
    "validH2O_X=h2o.H2OFrame(validBench_X)\n",
    "validH2O_y=h2o.H2OFrame(validBench_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data Sets to Panda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:14:01.144872Z",
     "start_time": "2019-06-15T22:14:01.124871Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'as_data_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-6803de4d6f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Convert Data Sets to Panda:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrainDataset_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainDataset_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrainDataset_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainDataset_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvalidBench_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidBench_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalidBench_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidBench_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'as_data_frame'"
     ]
    }
   ],
   "source": [
    "#Convert Data Sets to Panda:\n",
    "trainDataset_X=trainDataset_X.as_data_frame()\n",
    "trainDataset_y=trainDataset_y.as_data_frame()\n",
    "validBench_X=validBench_X.as_data_frame()\n",
    "validBench_y=validBench_y.as_data_frame()\n",
    "trainDataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#Code Samples:\n",
    "col_types = [\"numeric\", \"numeric\", \"numeric\", \"enum\", \"enum\", \"numeric\", \"numeric\", \"numeric\", \"numeric\"]\n",
    "data = h2o.import_file(\"http://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv\", col_types=col_types)\n",
    "train, test = data.split_frame(ratios=[.8], seed=1)\n",
    "x = [\"CAPSULE\",\"GLEASON\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\"]\n",
    "y = \"AGE\"\n",
    "nfolds = 5\n",
    "my_gbm = H2OGradientBoostingEstimator(nfolds=nfolds, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "my_gbm.train(x=x, y=y, training_frame=train)\n",
    "my_rf = H2ORandomForestEstimator(nfolds=nfolds, fold_assignment=\"Modulo\", keep_cross_validation_predictions=True)\n",
    "my_rf.train(x=x, y=y, training_frame=train)\n",
    "stack = H2OStackedEnsembleEstimator(model_id=\"my_ensemble\", training_frame=train, validation_frame=test, base_models=[my_gbm.model_id, my_rf.model_id])\n",
    "stack.train(x=x, y=y, training_frame=train, validation_frame=test)\n",
    "stack.model_performance()\n",
    "\n",
    "import h2o\n",
    "h2o.init(strict_version_check= False , port = 54345)\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "model = H2ODeepLearningEstimator()\n",
    "rows = [[1,2,3,4,0], [2,1,2,4,1], [2,1,4,2,1], [0,1,2,34,1], [2,3,4,1,0]] * 50\n",
    "fr = h2o.H2OFrame(rows)\n",
    "X = fr.col_names[0:4]\n",
    "\n",
    "## Classification Model\n",
    "fr[4] = fr[4].asfactor()\n",
    "model.train(x=X, y=\"C5\", training_frame=fr)\n",
    "print('Model Type:', model.type)\n",
    "print('logloss', model.logloss(valid = False))\n",
    "print('Accuracy', model.accuracy(valid = False))\n",
    "print('AUC', model.auc(valid = False))\n",
    "print('R2', model.r2(valid = False))\n",
    "print('RMSE', model.rmse(valid = False))\n",
    "print('Error', model.error(valid = False))\n",
    "print('MCC', model.mcc(valid = False))\n",
    "\n",
    "## Regression Model\n",
    "fr = h2o.H2OFrame(rows)\n",
    "model.train(x=X, y=\"C5\", training_frame=fr)\n",
    "print('Model Type:', model.type)\n",
    "print('R2', model.r2(valid = False))\n",
    "print('RMSE', model.rmse(valid = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:23:47.209675Z",
     "start_time": "2019-06-15T22:23:13.976775Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Using Time Series Cross Validation\n",
      "Measured Fit Time:  3.096177101135254\n",
      " \n",
      "Model Parameters: \n",
      "{'bootstrap': False, 'criterion': 'mse', 'max_depth': 16, 'max_features': 2, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 245, 'n_jobs': -1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 29.424683\n",
      "EV: 0.591674\n",
      "EV for each Fold: [0.69006664 0.55417529 0.64862532 0.67283649 0.39266483]\n",
      "MAE: 1133.420813\n",
      "MAE for each Fold: [ 964.37934631 1290.03751848 1130.30893981 1031.29561928 1251.08264017]\n",
      "MSE: 3371595.757209\n",
      "MSE for each Fold: [2026152.68408574 3980885.4456046  2449852.3584889  2357499.68848517\n",
      " 6043588.60937812]\n",
      "RMSE: 1795.526449\n",
      "RMSE for each Fold: [1423.42990136 1995.21563887 1565.20042119 1535.41515184 2458.3711293 ]\n",
      "XV R2 Actuals: [0.66994561 0.55379273 0.64819008 0.67281093 0.39238235]\n",
      "Cross Validation R2: 0.587424\n",
      "XVR_fit_time Actuals:  [3.82821918 2.00511456 2.61214924 3.28418779 3.96922708]\n",
      "XVR_fit_time: 3.139780\n",
      "score_time Actuals:  [0.83604789 0.83004761 0.8320477  0.82704735 0.82704711]\n",
      "score_time: 0.830448\n",
      " \n",
      " \n",
      "Prediction Time:  0.31001782417297363\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1058.000288\n",
      "MSE: 1993391.645733\n",
      "RMSE: 1411.875223\n",
      "Validation Set R2: 0.667347\n",
      "Total Mean EV:  [0.69006664 0.55417529 0.64862532 0.67283649 0.39266483 0.68079826] 0.6065278035680955\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.365084)\n",
      "2. feature 4 HasPromotions (0.150483)\n",
      "3. feature 2 Day of week (number) (0.103743)\n",
      "4. feature 13 StoreID (0.049341)\n",
      "5. feature 11 Region_GDP (0.043071)\n",
      "6. feature 8 NearestCompetitor (0.042878)\n",
      "7. feature 14 StoreType (0.041788)\n",
      "8. feature 5 IsHoliday (0.040219)\n",
      "9. feature 0 AssortmentType (0.036623)\n",
      "10. feature 10 Region_AreaKM2 (0.027018)\n",
      "11. feature 12 Region_PopulationK (0.023820)\n",
      "12. feature 9 Region (0.020059)\n",
      "13. feature 3 Day of year (0.015106)\n",
      "14. feature 1 Day of month (0.013976)\n",
      "15. feature 15 Week (0.012785)\n",
      "16. feature 7 Month (number) (0.010505)\n",
      "17. feature 16 Year (0.003501)\n",
      " \n",
      "Validation Set EV, XVal EV, Mean EV:  0.6807982564988801 0.5916737129819387 0.6065278035680955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6807982564988801,\n",
       " 0.5916737129819387,\n",
       " 0.6065278035680955,\n",
       " 0.667347006388035,\n",
       " 0.5874243404840248)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross Eval Test with ExtraTreeRegressor:\n",
    "Mdl=ExtraTreesRegressor; MdlParams={}\n",
    "CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "           SplitPercent=100,Mdl=Mdl,MdlParams=MdlParams, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T23:38:57.216423Z",
     "start_time": "2019-06-15T23:38:31.915484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model:  H2ORandomForestEstimator\n",
      "100553\n",
      "Pandas Value: False\n",
      "Y column:  ['NumberOfSales']\n",
      "X columns:  ['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year', 'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)', 'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP', 'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year', 'NumberOfSales']\n",
      "Training data set shape: (100553, 18)\n",
      "{}\n",
      "(100553, 18) (100553, 1)\n",
      "16762 16757\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      "33520 16757\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      "50278 16757\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      "67036 16757\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      "83794 16757\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      "                                                                     model_id  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                {'__meta': {'schema_version': 3, 'schema_name'...   \n",
      "default_value                                                            None   \n",
      "gridable                                                                False   \n",
      "help                        Destination id for this model; auto-generated ...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                                model_id   \n",
      "level                                                                critical   \n",
      "name                                                                 model_id   \n",
      "required                                                                False   \n",
      "type                                                               Key<Model>   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                               training_frame  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                {'__meta': {'schema_version': 3, 'schema_name'...   \n",
      "default_value                                                            None   \n",
      "gridable                                                                False   \n",
      "help                                           Id of the training data frame.   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                          training_frame   \n",
      "level                                                                critical   \n",
      "name                                                           training_frame   \n",
      "required                                                                False   \n",
      "type                                                               Key<Frame>   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                             validation_frame  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                {'__meta': {'schema_version': 3, 'schema_name'...   \n",
      "default_value                                                            None   \n",
      "gridable                                                                 True   \n",
      "help                                         Id of the validation data frame.   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                        validation_frame   \n",
      "level                                                                critical   \n",
      "name                                                         validation_frame   \n",
      "required                                                                False   \n",
      "type                                                               Key<Frame>   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                                       nfolds  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                                0   \n",
      "default_value                                                               0   \n",
      "gridable                                                                False   \n",
      "help                        Number of folds for K-fold cross-validation (0...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                                  nfolds   \n",
      "level                                                                critical   \n",
      "name                                                                   nfolds   \n",
      "required                                                                False   \n",
      "type                                                                      int   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                 keep_cross_validation_models  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             True   \n",
      "default_value                                                            True   \n",
      "gridable                                                                False   \n",
      "help                             Whether to keep the cross-validation models.   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                            keep_cross_validation_models   \n",
      "level                                                                  expert   \n",
      "name                                             keep_cross_validation_models   \n",
      "required                                                                False   \n",
      "type                                                                  boolean   \n",
      "values                                                                     []   \n",
      "\n",
      "                                            keep_cross_validation_predictions  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                            False   \n",
      "default_value                                                           False   \n",
      "gridable                                                                False   \n",
      "help                        Whether to keep the predictions of the cross-v...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                       keep_cross_validation_predictions   \n",
      "level                                                                  expert   \n",
      "name                                        keep_cross_validation_predictions   \n",
      "required                                                                False   \n",
      "type                                                                  boolean   \n",
      "values                                                                     []   \n",
      "\n",
      "                                        keep_cross_validation_fold_assignment  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                            False   \n",
      "default_value                                                           False   \n",
      "gridable                                                                False   \n",
      "help                        Whether to keep the cross-validation fold assi...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                   keep_cross_validation_fold_assignment   \n",
      "level                                                                  expert   \n",
      "name                                    keep_cross_validation_fold_assignment   \n",
      "required                                                                False   \n",
      "type                                                                  boolean   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                         score_each_iteration  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                            False   \n",
      "default_value                                                           False   \n",
      "gridable                                                                False   \n",
      "help                        Whether to score during each iteration of mode...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                    score_each_iteration   \n",
      "level                                                               secondary   \n",
      "name                                                     score_each_iteration   \n",
      "required                                                                False   \n",
      "type                                                                  boolean   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                          score_tree_interval  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                                0   \n",
      "default_value                                                               0   \n",
      "gridable                                                                False   \n",
      "help                        Score the model after every so many trees. Dis...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                     score_tree_interval   \n",
      "level                                                               secondary   \n",
      "name                                                      score_tree_interval   \n",
      "required                                                                False   \n",
      "type                                                                      int   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                              fold_assignment  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             AUTO   \n",
      "default_value                                                            AUTO   \n",
      "gridable                                                                 True   \n",
      "help                        Cross-validation fold assignment scheme, if fo...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                         fold_assignment   \n",
      "level                                                               secondary   \n",
      "name                                                          fold_assignment   \n",
      "required                                                                False   \n",
      "type                                                                     enum   \n",
      "values                                     [AUTO, Random, Modulo, Stratified]   \n",
      "\n",
      "                                                  ...                          \\\n",
      "__meta                                            ...                           \n",
      "actual_value                                      ...                           \n",
      "default_value                                     ...                           \n",
      "gridable                                          ...                           \n",
      "help                                              ...                           \n",
      "is_member_of_frames                               ...                           \n",
      "is_mutually_exclusive_with                        ...                           \n",
      "label                                             ...                           \n",
      "level                                             ...                           \n",
      "name                                              ...                           \n",
      "required                                          ...                           \n",
      "type                                              ...                           \n",
      "values                                            ...                           \n",
      "\n",
      "                                             col_sample_rate_change_per_level  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                                1   \n",
      "default_value                                                               1   \n",
      "gridable                                                                 True   \n",
      "help                        Relative change of the column sampling rate fo...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                        col_sample_rate_change_per_level   \n",
      "level                                                                  expert   \n",
      "name                                         col_sample_rate_change_per_level   \n",
      "required                                                                False   \n",
      "type                                                                   double   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                     col_sample_rate_per_tree  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                                1   \n",
      "default_value                                                               1   \n",
      "gridable                                                                 True   \n",
      "help                            Column sample rate per tree (from 0.0 to 1.0)   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                col_sample_rate_per_tree   \n",
      "level                                                               secondary   \n",
      "name                                                 col_sample_rate_per_tree   \n",
      "required                                                                False   \n",
      "type                                                                   double   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                        min_split_improvement  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                            1e-05   \n",
      "default_value                                                           1e-05   \n",
      "gridable                                                                 True   \n",
      "help                        Minimum relative improvement in squared error ...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                   min_split_improvement   \n",
      "level                                                               secondary   \n",
      "name                                                    min_split_improvement   \n",
      "required                                                                False   \n",
      "type                                                                   double   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                               histogram_type  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             AUTO   \n",
      "default_value                                                            AUTO   \n",
      "gridable                                                                 True   \n",
      "help                        What type of histogram to use for finding opti...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                          histogram_type   \n",
      "level                                                               secondary   \n",
      "name                                                           histogram_type   \n",
      "required                                                                False   \n",
      "type                                                                     enum   \n",
      "values                      [AUTO, UniformAdaptive, Random, QuantilesGloba...   \n",
      "\n",
      "                                                         categorical_encoding  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             AUTO   \n",
      "default_value                                                            AUTO   \n",
      "gridable                                                                 True   \n",
      "help                                 Encoding scheme for categorical features   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                    categorical_encoding   \n",
      "level                                                               secondary   \n",
      "name                                                     categorical_encoding   \n",
      "required                                                                False   \n",
      "type                                                                     enum   \n",
      "values                      [AUTO, Enum, OneHotInternal, OneHotExplicit, B...   \n",
      "\n",
      "                                                              calibrate_model  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                            False   \n",
      "default_value                                                           False   \n",
      "gridable                                                                False   \n",
      "help                        Use Platt Scaling to calculate calibrated clas...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                         calibrate_model   \n",
      "level                                                                  expert   \n",
      "name                                                          calibrate_model   \n",
      "required                                                                False   \n",
      "type                                                                  boolean   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                            calibration_frame  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             None   \n",
      "default_value                                                            None   \n",
      "gridable                                                                False   \n",
      "help                                      Calibration frame for Platt Scaling   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                       calibration_frame   \n",
      "level                                                                  expert   \n",
      "name                                                        calibration_frame   \n",
      "required                                                                False   \n",
      "type                                                               Key<Frame>   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                                 distribution  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                         gaussian   \n",
      "default_value                                                            AUTO   \n",
      "gridable                                                                 True   \n",
      "help                                                    Distribution function   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                            distribution   \n",
      "level                                                               secondary   \n",
      "name                                                             distribution   \n",
      "required                                                                False   \n",
      "type                                                                     enum   \n",
      "values                      [AUTO, bernoulli, quasibinomial, ordinal, mult...   \n",
      "\n",
      "                                                           custom_metric_func  \\\n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
      "actual_value                                                             None   \n",
      "default_value                                                            None   \n",
      "gridable                                                                False   \n",
      "help                        Reference to custom evaluation function, forma...   \n",
      "is_member_of_frames                                                        []   \n",
      "is_mutually_exclusive_with                                                 []   \n",
      "label                                                      custom_metric_func   \n",
      "level                                                               secondary   \n",
      "name                                                       custom_metric_func   \n",
      "required                                                                False   \n",
      "type                                                                   string   \n",
      "values                                                                     []   \n",
      "\n",
      "                                                       export_checkpoints_dir  \n",
      "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...  \n",
      "actual_value                                                             None  \n",
      "default_value                                                            None  \n",
      "gridable                                                                False  \n",
      "help                        Automatically export generated models to this ...  \n",
      "is_member_of_frames                                                        []  \n",
      "is_mutually_exclusive_with                                                 []  \n",
      "label                                                  export_checkpoints_dir  \n",
      "level                                                               secondary  \n",
      "name                                                   export_checkpoints_dir  \n",
      "required                                                                False  \n",
      "type                                                                   string  \n",
      "values                                                                     []  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 rows x 49 columns]\n",
      "Measured Fit Time:  0\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 24.846913\n",
      "EV: 0.544297\n",
      "EV for each Fold: [0.6649441295122347, 0.5245604140889906, 0.5801434727632185, 0.571914921400587, 0.3799232620088753]\n",
      "MAE: 1197.407314\n",
      "MAE for each Fold: [1047.7899668650205, 1333.4852115061756, 1271.5801615689957, 1080.536627004738, 1253.644603397413]\n",
      "MSE: 3818722.776084\n",
      "MSE for each Fold: [2401001.882118972, 4245930.861271092, 3182573.317761772, 3090903.848854855, 6173203.970413877]\n",
      "RMSE: 1927.349826\n",
      "RMSE for each Fold: [1549.51666081 2060.56566536 1783.97682658 1758.09665515 2484.59332093]\n",
      "XV R2 Actuals: [0.6089050853177054, 0.5240539712575765, 0.5429922499474955, 0.5710489909344235, 0.3793851555404443]\n",
      "Cross Validation R2: 0.525277\n",
      "XVR_fit_time Actuals:  [0, 0, 0, 0, 0]\n",
      "XVR_fit_time: 0.000000\n",
      "score_time Actuals:  [0, 0, 0, 0, 0]\n",
      "score_time: 0.000000\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n",
      " \n",
      " \n",
      "Prediction Time:  0.31201767921447754\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.57\n",
      "MAE: 1195.266310\n",
      "MSE: 2832543.383876\n",
      "RMSE: 1683.016157\n",
      "Validation Set R2: 0.527311\n",
      "Total Mean EV:  [0.66494413 0.52456041 0.58014347 0.57191492 0.37992326 0.56616449] 0.5479417823357213\n",
      "Validation Set EV, XVal EV, Mean EV:  0.5661644942404221 0.5442972399547812 0.5479417823357213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5661644942404221,\n",
       " 0.5442972399547812,\n",
       " 0.5479417823357213,\n",
       " 0.5273111341672223,\n",
       " 0.525277090599529)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross Eval Test with H2O:\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "MdlParams={'max_depth':16,'ntrees':245, 'stopping_rounds':3}\n",
    "Mdl=h2o.estimators.H2ORandomForestEstimator\n",
    "#Mdl=h2o.estimators.H2OGradientBoostingEstimator\n",
    "#Mdl=h2o.estimators.deeplearning.H2ODeepLearningEstimator\n",
    "CrossEval3(trainH2O_X,trainH2O_y,validH2O_X,validH2O_y,\n",
    "           SplitPercent=100,Mdl=Mdl,MdlParams=MdlParams, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O AutoML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:53:26.497952Z",
     "start_time": "2019-06-15T21:24:52.380694Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "#Run AutoML\n",
    "#X=trainDataset_X.concat(trainDataset_y, axis=1)\n",
    "Xcols=list(set(X.names)-set('NumberOfSales'))\n",
    "Ycol='NumberOfSales'\n",
    "aml = H2OAutoML(max_runtime_secs = 18000, max_models=20,seed = 0, project_name = \"Forecasting\",      \n",
    "                nfolds=5, keep_cross_validation_predictions=False, keep_cross_validation_models=False,\n",
    "                exclude_algos = [\"GLM\",\"DeepLearning\"],export_checkpoints_dir=\"C:/Benchmarking/Checkpoints\" \n",
    "                ,sort_metric='mae', stopping_metric='MAE')\n",
    "aml.train(x = Xcols, y = Ycol, training_frame = X, validation_frame=ValidationSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:02:21.375205Z",
     "start_time": "2019-06-15T22:02:21.354204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                 </th><th style=\"text-align: right;\">  mean_residual_deviance</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">   mse</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">  rmsle</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_172108_model_1</td><td style=\"text-align: right;\">                  231367</td><td style=\"text-align: right;\">481.007</td><td style=\"text-align: right;\">231367</td><td style=\"text-align: right;\">284.153</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_182452_model_1</td><td style=\"text-align: right;\">                  231367</td><td style=\"text-align: right;\">481.007</td><td style=\"text-align: right;\">231367</td><td style=\"text-align: right;\">284.153</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_182452_model_7</td><td style=\"text-align: right;\">                  211987</td><td style=\"text-align: right;\">460.421</td><td style=\"text-align: right;\">211987</td><td style=\"text-align: right;\">290.953</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_172108_model_7</td><td style=\"text-align: right;\">                  211987</td><td style=\"text-align: right;\">460.421</td><td style=\"text-align: right;\">211987</td><td style=\"text-align: right;\">290.953</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20190615_172108             </td><td style=\"text-align: right;\">                  223160</td><td style=\"text-align: right;\">472.398</td><td style=\"text-align: right;\">223160</td><td style=\"text-align: right;\">294.461</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20190615_182452             </td><td style=\"text-align: right;\">                  223160</td><td style=\"text-align: right;\">472.398</td><td style=\"text-align: right;\">223160</td><td style=\"text-align: right;\">294.461</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_182452_model_4</td><td style=\"text-align: right;\">                  241113</td><td style=\"text-align: right;\">491.032</td><td style=\"text-align: right;\">241113</td><td style=\"text-align: right;\">305.162</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190615_172108_model_4</td><td style=\"text-align: right;\">                  241113</td><td style=\"text-align: right;\">491.032</td><td style=\"text-align: right;\">241113</td><td style=\"text-align: right;\">305.162</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_5_AutoML_20190615_172108             </td><td style=\"text-align: right;\">                  270686</td><td style=\"text-align: right;\">520.275</td><td style=\"text-align: right;\">270686</td><td style=\"text-align: right;\">315.642</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "<tr><td>GBM_5_AutoML_20190615_182452             </td><td style=\"text-align: right;\">                  270686</td><td style=\"text-align: right;\">520.275</td><td style=\"text-align: right;\">270686</td><td style=\"text-align: right;\">315.642</td><td style=\"text-align: right;\">    nan</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:06:26.531581Z",
     "start_time": "2019-06-15T22:06:26.519580Z"
    }
   },
   "outputs": [],
   "source": [
    "h2o.remove(\"GBM_5_AutoML_20190615_182452\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T22:25:24.486197Z",
     "start_time": "2019-06-15T22:25:22.752098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 2519250.2880115416\n",
      "RMSE: 1587.214631992643\n",
      "MAE: 1090.9337094666366\n",
      "RMSLE: NaN\n",
      "Mean Residual Deviance: 2519250.2880115416\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidationSet= validH2O_X.concat(validH2O_y, axis=1)\n",
    "perf = aml.leader.model_performance(ValidationSet)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T14:54:42.795251Z",
     "start_time": "2019-06-20T14:54:42.779250Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-72a9aff3e70b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'aml' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(aml.leader.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T21:17:09.511932Z",
     "start_time": "2019-06-15T21:17:09.431928Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  GBM_grid_1_AutoML_20190615_172108_model_1\n",
      "\n",
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 57072.048579952774\n",
      "RMSE: 238.89756922152384\n",
      "MAE: 161.85507226509128\n",
      "RMSLE: NaN\n",
      "Mean Residual Deviance: 57072.048579952774\n",
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 231367.32748546518\n",
      "RMSE: 481.00657738274765\n",
      "MAE: 284.15286929217507\n",
      "RMSLE: NaN\n",
      "Mean Residual Deviance: 231367.32748546518\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>mae</td>\n",
       "<td>284.15286</td>\n",
       "<td>0.8930332</td>\n",
       "<td>285.70584</td>\n",
       "<td>282.97086</td>\n",
       "<td>283.58084</td>\n",
       "<td>282.87158</td>\n",
       "<td>285.6352</td></tr>\n",
       "<tr><td>mean_residual_deviance</td>\n",
       "<td>231367.33</td>\n",
       "<td>7664.469</td>\n",
       "<td>248317.98</td>\n",
       "<td>224275.08</td>\n",
       "<td>220786.83</td>\n",
       "<td>223433.56</td>\n",
       "<td>240023.22</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>231367.33</td>\n",
       "<td>7664.469</td>\n",
       "<td>248317.98</td>\n",
       "<td>224275.08</td>\n",
       "<td>220786.83</td>\n",
       "<td>223433.56</td>\n",
       "<td>240023.22</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.9697207</td>\n",
       "<td>0.0010400</td>\n",
       "<td>0.967442</td>\n",
       "<td>0.9705879</td>\n",
       "<td>0.9712215</td>\n",
       "<td>0.9708197</td>\n",
       "<td>0.9685326</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>231367.33</td>\n",
       "<td>7664.469</td>\n",
       "<td>248317.98</td>\n",
       "<td>224275.08</td>\n",
       "<td>220786.83</td>\n",
       "<td>223433.56</td>\n",
       "<td>240023.22</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>480.87616</td>\n",
       "<td>7.919567</td>\n",
       "<td>498.31516</td>\n",
       "<td>473.5769</td>\n",
       "<td>469.87958</td>\n",
       "<td>472.6876</td>\n",
       "<td>489.92163</td></tr>\n",
       "<tr><td>rmsle</td>\n",
       "<td>0.0</td>\n",
       "<td>NaN</td>\n",
       "<td>NaN</td>\n",
       "<td>NaN</td>\n",
       "<td>NaN</td>\n",
       "<td>NaN</td>\n",
       "<td>NaN</td></tr></table></div>"
      ],
      "text/plain": [
       "                        mean      sd          cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "----------------------  --------  ----------  ------------  ------------  ------------  ------------  ------------\n",
       "mae                     284.153   0.893033    285.706       282.971       283.581       282.872       285.635\n",
       "mean_residual_deviance  231367    7664.47     248318        224275        220787        223434        240023\n",
       "mse                     231367    7664.47     248318        224275        220787        223434        240023\n",
       "r2                      0.969721  0.00104002  0.967442      0.970588      0.971221      0.97082       0.968533\n",
       "residual_deviance       231367    7664.47     248318        224275        220787        223434        240023\n",
       "rmse                    480.876   7.91957     498.315       473.577       469.88        472.688       489.922\n",
       "rmsle                   0         nan         nan           nan           nan           nan           nan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_mae</b></td>\n",
       "<td><b>training_deviance</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:12</td>\n",
       "<td> 1 min 54.089 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>2764.3735851</td>\n",
       "<td>2043.0903715</td>\n",
       "<td>7641761.3181354</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:12</td>\n",
       "<td> 1 min 54.589 sec</td>\n",
       "<td>5.0</td>\n",
       "<td>2192.5248386</td>\n",
       "<td>1630.4838124</td>\n",
       "<td>4807165.1680170</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:13</td>\n",
       "<td> 1 min 55.084 sec</td>\n",
       "<td>10.0</td>\n",
       "<td>1753.1547323</td>\n",
       "<td>1309.1702723</td>\n",
       "<td>3073551.5154960</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:13</td>\n",
       "<td> 1 min 55.685 sec</td>\n",
       "<td>15.0</td>\n",
       "<td>1419.2924544</td>\n",
       "<td>1066.3941791</td>\n",
       "<td>2014391.0711882</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:14</td>\n",
       "<td> 1 min 56.205 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>1164.6183108</td>\n",
       "<td>878.2490784</td>\n",
       "<td>1356335.8099166</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:29</td>\n",
       "<td> 2 min 11.230 sec</td>\n",
       "<td>135.0</td>\n",
       "<td>238.8975692</td>\n",
       "<td>161.8550723</td>\n",
       "<td>57072.0485800</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:29</td>\n",
       "<td> 2 min 11.283 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>238.8975692</td>\n",
       "<td>161.8550723</td>\n",
       "<td>57072.0485800</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:29</td>\n",
       "<td> 2 min 11.339 sec</td>\n",
       "<td>145.0</td>\n",
       "<td>238.8975692</td>\n",
       "<td>161.8550723</td>\n",
       "<td>57072.0485800</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:29</td>\n",
       "<td> 2 min 11.398 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>238.8975692</td>\n",
       "<td>161.8550723</td>\n",
       "<td>57072.0485800</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-06-15 17:34:29</td>\n",
       "<td> 2 min 11.464 sec</td>\n",
       "<td>155.0</td>\n",
       "<td>238.8975692</td>\n",
       "<td>161.8550723</td>\n",
       "<td>57072.0485800</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration          number_of_trees    training_rmse       training_mae        training_deviance\n",
       "---  -------------------  ----------------  -----------------  ------------------  ------------------  -------------------\n",
       "     2019-06-15 17:34:12  1 min 54.089 sec  0.0                2764.3735851247416  2043.090371497679   7641761.318135416\n",
       "     2019-06-15 17:34:12  1 min 54.589 sec  5.0                2192.5248386317003  1630.4838124152927  4807165.168016964\n",
       "     2019-06-15 17:34:13  1 min 55.084 sec  10.0               1753.1547323314185  1309.1702723077126  3073551.5154960477\n",
       "     2019-06-15 17:34:13  1 min 55.685 sec  15.0               1419.2924544251491  1066.394179084924   2014391.0711881642\n",
       "     2019-06-15 17:34:14  1 min 56.205 sec  20.0               1164.6183108282964  878.2490783766067   1356335.8099165545\n",
       "---  ---                  ---               ---                ---                 ---                 ---\n",
       "     2019-06-15 17:34:29  2 min 11.230 sec  135.0              238.89756922152384  161.85507226509128  57072.048579952774\n",
       "     2019-06-15 17:34:29  2 min 11.283 sec  140.0              238.89756922152384  161.85507226509128  57072.048579952774\n",
       "     2019-06-15 17:34:29  2 min 11.339 sec  145.0              238.89756922152384  161.85507226509128  57072.048579952774\n",
       "     2019-06-15 17:34:29  2 min 11.398 sec  150.0              238.89756922152384  161.85507226509128  57072.048579952774\n",
       "     2019-06-15 17:34:29  2 min 11.464 sec  155.0              238.89756922152384  161.85507226509128  57072.048579952774"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>IsOpen</td>\n",
       "<td>2674854985728.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4788738</td></tr>\n",
       "<tr><td>Day of week (number)</td>\n",
       "<td>701704503296.0000000</td>\n",
       "<td>0.2623337</td>\n",
       "<td>0.1256247</td></tr>\n",
       "<tr><td>StoreID</td>\n",
       "<td>502043639808.0000000</td>\n",
       "<td>0.1876900</td>\n",
       "<td>0.0898798</td></tr>\n",
       "<tr><td>HasPromotions</td>\n",
       "<td>372018937856.0000000</td>\n",
       "<td>0.1390800</td>\n",
       "<td>0.0666018</td></tr>\n",
       "<tr><td>NearestCompetitor</td>\n",
       "<td>268197412864.0000000</td>\n",
       "<td>0.1002662</td>\n",
       "<td>0.0480148</td></tr>\n",
       "<tr><td>AssortmentType</td>\n",
       "<td>155297251328.0000000</td>\n",
       "<td>0.0580582</td>\n",
       "<td>0.0278026</td></tr>\n",
       "<tr><td>Region_GDP</td>\n",
       "<td>139054858240.0000000</td>\n",
       "<td>0.0519859</td>\n",
       "<td>0.0248947</td></tr>\n",
       "<tr><td>Region_PopulationK</td>\n",
       "<td>101733916672.0000000</td>\n",
       "<td>0.0380334</td>\n",
       "<td>0.0182132</td></tr>\n",
       "<tr><td>Region_AreaKM2</td>\n",
       "<td>100816928768.0000000</td>\n",
       "<td>0.0376906</td>\n",
       "<td>0.0180490</td></tr>\n",
       "<tr><td>Day of month</td>\n",
       "<td>99766272000.0000000</td>\n",
       "<td>0.0372978</td>\n",
       "<td>0.0178610</td></tr>\n",
       "<tr><td>StoreType</td>\n",
       "<td>92741853184.0000000</td>\n",
       "<td>0.0346717</td>\n",
       "<td>0.0166034</td></tr>\n",
       "<tr><td>Region</td>\n",
       "<td>89537159168.0000000</td>\n",
       "<td>0.0334736</td>\n",
       "<td>0.0160297</td></tr>\n",
       "<tr><td>Day of year</td>\n",
       "<td>81206362112.0000000</td>\n",
       "<td>0.0303592</td>\n",
       "<td>0.0145382</td></tr>\n",
       "<tr><td>IsHoliday</td>\n",
       "<td>66618073088.0000000</td>\n",
       "<td>0.0249053</td>\n",
       "<td>0.0119265</td></tr>\n",
       "<tr><td>Month (number)</td>\n",
       "<td>56944766976.0000000</td>\n",
       "<td>0.0212889</td>\n",
       "<td>0.0101947</td></tr>\n",
       "<tr><td>Week</td>\n",
       "<td>56670220288.0000000</td>\n",
       "<td>0.0211863</td>\n",
       "<td>0.0101456</td></tr>\n",
       "<tr><td>Year</td>\n",
       "<td>26512488448.0000000</td>\n",
       "<td>0.0099117</td>\n",
       "<td>0.0047465</td></tr></table></div>"
      ],
      "text/plain": [
       "variable              relative_importance    scaled_importance    percentage\n",
       "--------------------  ---------------------  -------------------  ------------\n",
       "IsOpen                2.67485e+12            1                    0.478874\n",
       "Day of week (number)  7.01705e+11            0.262334             0.125625\n",
       "StoreID               5.02044e+11            0.18769              0.0898798\n",
       "HasPromotions         3.72019e+11            0.13908              0.0666018\n",
       "NearestCompetitor     2.68197e+11            0.100266             0.0480148\n",
       "AssortmentType        1.55297e+11            0.0580582            0.0278026\n",
       "Region_GDP            1.39055e+11            0.0519859            0.0248947\n",
       "Region_PopulationK    1.01734e+11            0.0380334            0.0182132\n",
       "Region_AreaKM2        1.00817e+11            0.0376906            0.018049\n",
       "Day of month          9.97663e+10            0.0372978            0.017861\n",
       "StoreType             9.27419e+10            0.0346717            0.0166034\n",
       "Region                8.95372e+10            0.0334736            0.0160297\n",
       "Day of year           8.12064e+10            0.0303592            0.0145382\n",
       "IsHoliday             6.66181e+10            0.0249053            0.0119265\n",
       "Month (number)        5.69448e+10            0.0212889            0.0101947\n",
       "Week                  5.66702e+10            0.0211863            0.0101456\n",
       "Year                  2.65125e+10            0.00991175           0.00474648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<bound method ModelBase.varimp of >"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "aml.leader.varimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T13:50:34.305577Z",
     "start_time": "2019-06-18T13:50:34.302577Z"
    }
   },
   "source": [
    "# Load H2O Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T13:52:05.485777Z",
     "start_time": "2019-06-18T13:52:04.869742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ModelPath= \"C:/Benchmarking/Checkpoints/GBM_grid_1_AutoML_20190615_172108_model_1\"\n",
    "Loaded_model = h2o.load_model(ModelPath)\n",
    "Loaded_model.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size Study: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T17:46:39.955040Z",
     "start_time": "2019-06-18T17:46:39.852034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn_rate: 0.05\n",
      "learn_rate_annealing: 1.0\n",
      "min_split_improvement: 0.0001\n",
      "distribution: gaussian\n",
      "huber_alpha:  0.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>training_frame</th>\n",
       "      <th>validation_frame</th>\n",
       "      <th>nfolds</th>\n",
       "      <th>keep_cross_validation_models</th>\n",
       "      <th>keep_cross_validation_predictions</th>\n",
       "      <th>keep_cross_validation_fold_assignment</th>\n",
       "      <th>score_each_iteration</th>\n",
       "      <th>score_tree_interval</th>\n",
       "      <th>fold_assignment</th>\n",
       "      <th>...</th>\n",
       "      <th>min_split_improvement</th>\n",
       "      <th>histogram_type</th>\n",
       "      <th>max_abs_leafnode_pred</th>\n",
       "      <th>pred_noise_bandwidth</th>\n",
       "      <th>categorical_encoding</th>\n",
       "      <th>calibrate_model</th>\n",
       "      <th>calibration_frame</th>\n",
       "      <th>custom_metric_func</th>\n",
       "      <th>export_checkpoints_dir</th>\n",
       "      <th>monotone_constraints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>__meta</th>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "      <td>{'schema_version': 3, 'schema_name': 'ModelPar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_value</th>\n",
       "      <td>{'__meta': {'schema_version': 3, 'schema_name'...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>Modulo</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.79769e+308</td>\n",
       "      <td>0</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>C:/Benchmarking/Checkpoints</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default_value</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>...</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>1.79769e+308</td>\n",
       "      <td>0</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gridable</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>Destination id for this model; auto-generated ...</td>\n",
       "      <td>Id of the training data frame.</td>\n",
       "      <td>Id of the validation data frame.</td>\n",
       "      <td>Number of folds for K-fold cross-validation (0...</td>\n",
       "      <td>Whether to keep the cross-validation models.</td>\n",
       "      <td>Whether to keep the predictions of the cross-v...</td>\n",
       "      <td>Whether to keep the cross-validation fold assi...</td>\n",
       "      <td>Whether to score during each iteration of mode...</td>\n",
       "      <td>Score the model after every so many trees. Dis...</td>\n",
       "      <td>Cross-validation fold assignment scheme, if fo...</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum relative improvement in squared error ...</td>\n",
       "      <td>What type of histogram to use for finding opti...</td>\n",
       "      <td>Maximum absolute value of a leaf node prediction</td>\n",
       "      <td>Bandwidth (sigma) of Gaussian multiplicative n...</td>\n",
       "      <td>Encoding scheme for categorical features</td>\n",
       "      <td>Use Platt Scaling to calculate calibrated clas...</td>\n",
       "      <td>Calibration frame for Platt Scaling</td>\n",
       "      <td>Reference to custom evaluation function, forma...</td>\n",
       "      <td>Automatically export generated models to this ...</td>\n",
       "      <td>A mapping representing monotonic constraints. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_member_of_frames</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_mutually_exclusive_with</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>model_id</td>\n",
       "      <td>training_frame</td>\n",
       "      <td>validation_frame</td>\n",
       "      <td>nfolds</td>\n",
       "      <td>keep_cross_validation_models</td>\n",
       "      <td>keep_cross_validation_predictions</td>\n",
       "      <td>keep_cross_validation_fold_assignment</td>\n",
       "      <td>score_each_iteration</td>\n",
       "      <td>score_tree_interval</td>\n",
       "      <td>fold_assignment</td>\n",
       "      <td>...</td>\n",
       "      <td>min_split_improvement</td>\n",
       "      <td>histogram_type</td>\n",
       "      <td>max_abs_leafnode_pred</td>\n",
       "      <td>pred_noise_bandwidth</td>\n",
       "      <td>categorical_encoding</td>\n",
       "      <td>calibrate_model</td>\n",
       "      <td>calibration_frame</td>\n",
       "      <td>custom_metric_func</td>\n",
       "      <td>export_checkpoints_dir</td>\n",
       "      <td>monotone_constraints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <td>critical</td>\n",
       "      <td>critical</td>\n",
       "      <td>critical</td>\n",
       "      <td>critical</td>\n",
       "      <td>expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>...</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>secondary</td>\n",
       "      <td>expert</td>\n",
       "      <td>expert</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "      <td>secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>model_id</td>\n",
       "      <td>training_frame</td>\n",
       "      <td>validation_frame</td>\n",
       "      <td>nfolds</td>\n",
       "      <td>keep_cross_validation_models</td>\n",
       "      <td>keep_cross_validation_predictions</td>\n",
       "      <td>keep_cross_validation_fold_assignment</td>\n",
       "      <td>score_each_iteration</td>\n",
       "      <td>score_tree_interval</td>\n",
       "      <td>fold_assignment</td>\n",
       "      <td>...</td>\n",
       "      <td>min_split_improvement</td>\n",
       "      <td>histogram_type</td>\n",
       "      <td>max_abs_leafnode_pred</td>\n",
       "      <td>pred_noise_bandwidth</td>\n",
       "      <td>categorical_encoding</td>\n",
       "      <td>calibrate_model</td>\n",
       "      <td>calibration_frame</td>\n",
       "      <td>custom_metric_func</td>\n",
       "      <td>export_checkpoints_dir</td>\n",
       "      <td>monotone_constraints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>required</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>Key&lt;Model&gt;</td>\n",
       "      <td>Key&lt;Frame&gt;</td>\n",
       "      <td>Key&lt;Frame&gt;</td>\n",
       "      <td>int</td>\n",
       "      <td>boolean</td>\n",
       "      <td>boolean</td>\n",
       "      <td>boolean</td>\n",
       "      <td>boolean</td>\n",
       "      <td>int</td>\n",
       "      <td>enum</td>\n",
       "      <td>...</td>\n",
       "      <td>double</td>\n",
       "      <td>enum</td>\n",
       "      <td>double</td>\n",
       "      <td>double</td>\n",
       "      <td>enum</td>\n",
       "      <td>boolean</td>\n",
       "      <td>Key&lt;Frame&gt;</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>KeyValue[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>values</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AUTO, Random, Modulo, Stratified]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AUTO, UniformAdaptive, Random, QuantilesGloba...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AUTO, Enum, OneHotInternal, OneHotExplicit, B...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     model_id  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                {'__meta': {'schema_version': 3, 'schema_name'...   \n",
       "default_value                                                            None   \n",
       "gridable                                                                False   \n",
       "help                        Destination id for this model; auto-generated ...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                                model_id   \n",
       "level                                                                critical   \n",
       "name                                                                 model_id   \n",
       "required                                                                False   \n",
       "type                                                               Key<Model>   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                               training_frame  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             None   \n",
       "default_value                                                            None   \n",
       "gridable                                                                False   \n",
       "help                                           Id of the training data frame.   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                          training_frame   \n",
       "level                                                                critical   \n",
       "name                                                           training_frame   \n",
       "required                                                                False   \n",
       "type                                                               Key<Frame>   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                             validation_frame  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             None   \n",
       "default_value                                                            None   \n",
       "gridable                                                                 True   \n",
       "help                                         Id of the validation data frame.   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                        validation_frame   \n",
       "level                                                                critical   \n",
       "name                                                         validation_frame   \n",
       "required                                                                False   \n",
       "type                                                               Key<Frame>   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                                       nfolds  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                                5   \n",
       "default_value                                                               0   \n",
       "gridable                                                                False   \n",
       "help                        Number of folds for K-fold cross-validation (0...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                                  nfolds   \n",
       "level                                                                critical   \n",
       "name                                                                   nfolds   \n",
       "required                                                                False   \n",
       "type                                                                      int   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                 keep_cross_validation_models  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                            False   \n",
       "default_value                                                            True   \n",
       "gridable                                                                False   \n",
       "help                             Whether to keep the cross-validation models.   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                            keep_cross_validation_models   \n",
       "level                                                                  expert   \n",
       "name                                             keep_cross_validation_models   \n",
       "required                                                                False   \n",
       "type                                                                  boolean   \n",
       "values                                                                     []   \n",
       "\n",
       "                                            keep_cross_validation_predictions  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             True   \n",
       "default_value                                                           False   \n",
       "gridable                                                                False   \n",
       "help                        Whether to keep the predictions of the cross-v...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                       keep_cross_validation_predictions   \n",
       "level                                                                  expert   \n",
       "name                                        keep_cross_validation_predictions   \n",
       "required                                                                False   \n",
       "type                                                                  boolean   \n",
       "values                                                                     []   \n",
       "\n",
       "                                        keep_cross_validation_fold_assignment  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                            False   \n",
       "default_value                                                           False   \n",
       "gridable                                                                False   \n",
       "help                        Whether to keep the cross-validation fold assi...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                   keep_cross_validation_fold_assignment   \n",
       "level                                                                  expert   \n",
       "name                                    keep_cross_validation_fold_assignment   \n",
       "required                                                                False   \n",
       "type                                                                  boolean   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                         score_each_iteration  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                            False   \n",
       "default_value                                                           False   \n",
       "gridable                                                                False   \n",
       "help                        Whether to score during each iteration of mode...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                    score_each_iteration   \n",
       "level                                                               secondary   \n",
       "name                                                     score_each_iteration   \n",
       "required                                                                False   \n",
       "type                                                                  boolean   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                          score_tree_interval  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                                5   \n",
       "default_value                                                               0   \n",
       "gridable                                                                False   \n",
       "help                        Score the model after every so many trees. Dis...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                     score_tree_interval   \n",
       "level                                                               secondary   \n",
       "name                                                      score_tree_interval   \n",
       "required                                                                False   \n",
       "type                                                                      int   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                              fold_assignment  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                           Modulo   \n",
       "default_value                                                            AUTO   \n",
       "gridable                                                                 True   \n",
       "help                        Cross-validation fold assignment scheme, if fo...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                         fold_assignment   \n",
       "level                                                               secondary   \n",
       "name                                                          fold_assignment   \n",
       "required                                                                False   \n",
       "type                                                                     enum   \n",
       "values                                     [AUTO, Random, Modulo, Stratified]   \n",
       "\n",
       "                                                  ...                          \\\n",
       "__meta                                            ...                           \n",
       "actual_value                                      ...                           \n",
       "default_value                                     ...                           \n",
       "gridable                                          ...                           \n",
       "help                                              ...                           \n",
       "is_member_of_frames                               ...                           \n",
       "is_mutually_exclusive_with                        ...                           \n",
       "label                                             ...                           \n",
       "level                                             ...                           \n",
       "name                                              ...                           \n",
       "required                                          ...                           \n",
       "type                                              ...                           \n",
       "values                                            ...                           \n",
       "\n",
       "                                                        min_split_improvement  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                           0.0001   \n",
       "default_value                                                           1e-05   \n",
       "gridable                                                                 True   \n",
       "help                        Minimum relative improvement in squared error ...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                   min_split_improvement   \n",
       "level                                                               secondary   \n",
       "name                                                    min_split_improvement   \n",
       "required                                                                False   \n",
       "type                                                                   double   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                               histogram_type  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             AUTO   \n",
       "default_value                                                            AUTO   \n",
       "gridable                                                                 True   \n",
       "help                        What type of histogram to use for finding opti...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                          histogram_type   \n",
       "level                                                               secondary   \n",
       "name                                                           histogram_type   \n",
       "required                                                                False   \n",
       "type                                                                     enum   \n",
       "values                      [AUTO, UniformAdaptive, Random, QuantilesGloba...   \n",
       "\n",
       "                                                        max_abs_leafnode_pred  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                     1.79769e+308   \n",
       "default_value                                                    1.79769e+308   \n",
       "gridable                                                                 True   \n",
       "help                         Maximum absolute value of a leaf node prediction   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                   max_abs_leafnode_pred   \n",
       "level                                                                  expert   \n",
       "name                                                    max_abs_leafnode_pred   \n",
       "required                                                                False   \n",
       "type                                                                   double   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                         pred_noise_bandwidth  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                                0   \n",
       "default_value                                                               0   \n",
       "gridable                                                                 True   \n",
       "help                        Bandwidth (sigma) of Gaussian multiplicative n...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                    pred_noise_bandwidth   \n",
       "level                                                                  expert   \n",
       "name                                                     pred_noise_bandwidth   \n",
       "required                                                                False   \n",
       "type                                                                   double   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                         categorical_encoding  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             AUTO   \n",
       "default_value                                                            AUTO   \n",
       "gridable                                                                 True   \n",
       "help                                 Encoding scheme for categorical features   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                    categorical_encoding   \n",
       "level                                                               secondary   \n",
       "name                                                     categorical_encoding   \n",
       "required                                                                False   \n",
       "type                                                                     enum   \n",
       "values                      [AUTO, Enum, OneHotInternal, OneHotExplicit, B...   \n",
       "\n",
       "                                                              calibrate_model  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                            False   \n",
       "default_value                                                           False   \n",
       "gridable                                                                False   \n",
       "help                        Use Platt Scaling to calculate calibrated clas...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                         calibrate_model   \n",
       "level                                                                  expert   \n",
       "name                                                          calibrate_model   \n",
       "required                                                                False   \n",
       "type                                                                  boolean   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                            calibration_frame  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             None   \n",
       "default_value                                                            None   \n",
       "gridable                                                                False   \n",
       "help                                      Calibration frame for Platt Scaling   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                       calibration_frame   \n",
       "level                                                                  expert   \n",
       "name                                                        calibration_frame   \n",
       "required                                                                False   \n",
       "type                                                               Key<Frame>   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                           custom_metric_func  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                                             None   \n",
       "default_value                                                            None   \n",
       "gridable                                                                False   \n",
       "help                        Reference to custom evaluation function, forma...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                      custom_metric_func   \n",
       "level                                                               secondary   \n",
       "name                                                       custom_metric_func   \n",
       "required                                                                False   \n",
       "type                                                                   string   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                       export_checkpoints_dir  \\\n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...   \n",
       "actual_value                                      C:/Benchmarking/Checkpoints   \n",
       "default_value                                                            None   \n",
       "gridable                                                                False   \n",
       "help                        Automatically export generated models to this ...   \n",
       "is_member_of_frames                                                        []   \n",
       "is_mutually_exclusive_with                                                 []   \n",
       "label                                                  export_checkpoints_dir   \n",
       "level                                                               secondary   \n",
       "name                                                   export_checkpoints_dir   \n",
       "required                                                                False   \n",
       "type                                                                   string   \n",
       "values                                                                     []   \n",
       "\n",
       "                                                         monotone_constraints  \n",
       "__meta                      {'schema_version': 3, 'schema_name': 'ModelPar...  \n",
       "actual_value                                                             None  \n",
       "default_value                                                            None  \n",
       "gridable                                                                False  \n",
       "help                        A mapping representing monotonic constraints. ...  \n",
       "is_member_of_frames                                                        []  \n",
       "is_mutually_exclusive_with                                                 []  \n",
       "label                                                    monotone_constraints  \n",
       "level                                                               secondary  \n",
       "name                                                     monotone_constraints  \n",
       "required                                                                False  \n",
       "type                                                               KeyValue[]  \n",
       "values                                                                     []  \n",
       "\n",
       "[13 rows x 56 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(Loaded_model.get_params())\n",
    "print('learn_rate:',df.loc['actual_value','learn_rate']) #Defaut=0.1\n",
    "print('learn_rate_annealing:',df.loc['actual_value','learn_rate_annealing']) #Default=1\n",
    "print('min_split_improvement:',df.loc['actual_value','min_split_improvement'])\n",
    "print('distribution:',df.loc['actual_value','distribution'])\n",
    "print('huber_alpha: ',df.loc['actual_value','huber_alpha'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T03:20:08.643852Z",
     "start_time": "2019-04-26T03:16:39.675802Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set shape: (20110, 17)\n",
      "Measured Fit Time:  0.8934476375579834\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.826221\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.18125725 1.64521885 2.16161036 2.51963973 3.08020592]\n",
      "XVR_fit_time: 2.117586\n",
      "score_time Actuals:  [0.87560153 0.87660193 0.85982919 0.8892014  0.87360144]\n",
      "score_time: 0.874967\n",
      " \n",
      " \n",
      "Prediction Time:  0.2184004783630371\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.67\n",
      "MAE: 1021.464769\n",
      "MSE: 1963549.971611\n",
      "RMSE: 1401.267273\n",
      "Validation Set R2: 0.672327\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.67250382] 0.6035212700015493\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.393535)\n",
      "2. feature 4 HasPromotions (0.184543)\n",
      "3. feature 2 Day of week (number) (0.114266)\n",
      "4. feature 13 StoreID (0.044849)\n",
      "5. feature 8 NearestCompetitor (0.044226)\n",
      "6. feature 5 IsHoliday (0.038938)\n",
      "7. feature 12 Region_PopulationK (0.020104)\n",
      "8. feature 3 Day of year (0.019854)\n",
      "9. feature 9 Region (0.019032)\n",
      "10. feature 1 Day of month (0.018635)\n",
      "11. feature 11 Region_GDP (0.017179)\n",
      "12. feature 14 StoreType (0.017155)\n",
      "13. feature 0 AssortmentType (0.016919)\n",
      "14. feature 15 Week (0.016285)\n",
      "15. feature 10 Region_AreaKM2 (0.016111)\n",
      "16. feature 7 Month (number) (0.012802)\n",
      "17. feature 16 Year (0.005568)\n",
      "Training data set shape: (30165, 17)\n",
      "Measured Fit Time:  1.0424044132232666\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 24.015146\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.21560383 1.6720612  2.07441568 2.61320496 3.0420053 ]\n",
      "XVR_fit_time: 2.123458\n",
      "score_time Actuals:  [0.89520192 0.83681059 0.87360144 0.87360168 0.88920164]\n",
      "score_time: 0.873683\n",
      " \n",
      " \n",
      "Prediction Time:  0.2184004783630371\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.66\n",
      "MAE: 1133.752583\n",
      "MSE: 2131660.738144\n",
      "RMSE: 1460.020801\n",
      "Validation Set R2: 0.644273\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.65551759] 0.6006902317740543\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.413621)\n",
      "2. feature 4 HasPromotions (0.185221)\n",
      "3. feature 2 Day of week (number) (0.117410)\n",
      "4. feature 13 StoreID (0.044955)\n",
      "5. feature 8 NearestCompetitor (0.043371)\n",
      "6. feature 5 IsHoliday (0.039386)\n",
      "7. feature 9 Region (0.019020)\n",
      "8. feature 3 Day of year (0.018731)\n",
      "9. feature 1 Day of month (0.016567)\n",
      "10. feature 12 Region_PopulationK (0.015893)\n",
      "11. feature 15 Week (0.015257)\n",
      "12. feature 11 Region_GDP (0.014645)\n",
      "13. feature 10 Region_AreaKM2 (0.014323)\n",
      "14. feature 14 StoreType (0.013385)\n",
      "15. feature 0 AssortmentType (0.011752)\n",
      "16. feature 7 Month (number) (0.011399)\n",
      "17. feature 16 Year (0.005063)\n",
      "Training data set shape: (40221, 17)\n",
      "Measured Fit Time:  1.284008502960205\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.860477\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.15200973 1.65840483 2.06240654 2.66062164 3.01000786]\n",
      "XVR_fit_time: 2.108690\n",
      "score_time Actuals:  [0.87860179 0.8922019  0.89520168 0.8766017  0.87560177]\n",
      "score_time: 0.883642\n",
      " \n",
      " \n",
      "Prediction Time:  0.3238029479980469\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.65\n",
      "MAE: 1149.250023\n",
      "MSE: 2334718.467287\n",
      "RMSE: 1527.978556\n",
      "Validation Set R2: 0.610387\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.64582835] 0.5990753578470595\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.400620)\n",
      "2. feature 4 HasPromotions (0.171990)\n",
      "3. feature 2 Day of week (number) (0.118357)\n",
      "4. feature 13 StoreID (0.057739)\n",
      "5. feature 8 NearestCompetitor (0.050280)\n",
      "6. feature 5 IsHoliday (0.040483)\n",
      "7. feature 0 AssortmentType (0.019354)\n",
      "8. feature 3 Day of year (0.018149)\n",
      "9. feature 9 Region (0.018091)\n",
      "10. feature 10 Region_AreaKM2 (0.015366)\n",
      "11. feature 1 Day of month (0.015310)\n",
      "12. feature 15 Week (0.014842)\n",
      "13. feature 14 StoreType (0.014711)\n",
      "14. feature 12 Region_PopulationK (0.014115)\n",
      "15. feature 11 Region_GDP (0.012471)\n",
      "16. feature 7 Month (number) (0.012254)\n",
      "17. feature 16 Year (0.005869)\n",
      "Training data set shape: (50276, 17)\n",
      "Measured Fit Time:  1.4932060241699219\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.844252\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.10820532 1.66200423 2.12540555 2.60520482 3.02640533]\n",
      "XVR_fit_time: 2.105445\n",
      "score_time Actuals:  [0.87360168 0.89320183 0.88920164 0.8736012  0.89320159]\n",
      "score_time: 0.884562\n",
      " \n",
      " \n",
      "Prediction Time:  0.218400239944458\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.62\n",
      "MAE: 1186.198887\n",
      "MSE: 2380180.163868\n",
      "RMSE: 1542.783252\n",
      "Validation Set R2: 0.602801\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.62124636] 0.594978360849488\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.392009)\n",
      "2. feature 4 HasPromotions (0.166869)\n",
      "3. feature 2 Day of week (number) (0.117591)\n",
      "4. feature 13 StoreID (0.057730)\n",
      "5. feature 8 NearestCompetitor (0.056800)\n",
      "6. feature 5 IsHoliday (0.041692)\n",
      "7. feature 0 AssortmentType (0.021644)\n",
      "8. feature 14 StoreType (0.019613)\n",
      "9. feature 10 Region_AreaKM2 (0.018346)\n",
      "10. feature 12 Region_PopulationK (0.018216)\n",
      "11. feature 9 Region (0.017458)\n",
      "12. feature 3 Day of year (0.016136)\n",
      "13. feature 11 Region_GDP (0.015023)\n",
      "14. feature 1 Day of month (0.013859)\n",
      "15. feature 15 Week (0.012944)\n",
      "16. feature 7 Month (number) (0.010293)\n",
      "17. feature 16 Year (0.003776)\n",
      "Training data set shape: (60331, 17)\n",
      "Measured Fit Time:  1.6936051845550537\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.888854\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.15120792 1.60680294 2.10800385 2.57780623 3.00560689]\n",
      "XVR_fit_time: 2.089886\n",
      "score_time Actuals:  [0.89320159 0.87560177 0.87560153 0.87360168 0.8736012 ]\n",
      "score_time: 0.878322\n",
      " \n",
      " \n",
      "Prediction Time:  0.218400239944458\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.63\n",
      "MAE: 1149.650829\n",
      "MSE: 2278250.830947\n",
      "RMSE: 1509.387568\n",
      "Validation Set R2: 0.619810\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.62875679] 0.5962300986385579\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.412960)\n",
      "2. feature 4 HasPromotions (0.168109)\n",
      "3. feature 2 Day of week (number) (0.111870)\n",
      "4. feature 8 NearestCompetitor (0.050135)\n",
      "5. feature 13 StoreID (0.046933)\n",
      "6. feature 5 IsHoliday (0.039631)\n",
      "7. feature 0 AssortmentType (0.022779)\n",
      "8. feature 11 Region_GDP (0.019439)\n",
      "9. feature 14 StoreType (0.019349)\n",
      "10. feature 12 Region_PopulationK (0.019143)\n",
      "11. feature 10 Region_AreaKM2 (0.017554)\n",
      "12. feature 3 Day of year (0.015476)\n",
      "13. feature 9 Region (0.015404)\n",
      "14. feature 1 Day of month (0.013976)\n",
      "15. feature 15 Week (0.012752)\n",
      "16. feature 7 Month (number) (0.010479)\n",
      "17. feature 16 Year (0.004011)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set shape: (70387, 17)\n",
      "Measured Fit Time:  1.912212610244751\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.698446\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.16980362 1.57860303 2.10600376 2.49700451 3.02880645]\n",
      "XVR_fit_time: 2.076044\n",
      "score_time Actuals:  [0.87360168 0.8892014  0.8766017  0.89420176 0.87360168]\n",
      "score_time: 0.881442\n",
      " \n",
      " \n",
      "Prediction Time:  0.2184004783630371\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.62\n",
      "MAE: 1132.892700\n",
      "MSE: 2329374.467224\n",
      "RMSE: 1526.228838\n",
      "Validation Set R2: 0.611279\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.6192922 ] 0.5946526673653694\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.415181)\n",
      "2. feature 4 HasPromotions (0.171836)\n",
      "3. feature 2 Day of week (number) (0.115340)\n",
      "4. feature 8 NearestCompetitor (0.044115)\n",
      "5. feature 13 StoreID (0.042577)\n",
      "6. feature 5 IsHoliday (0.042488)\n",
      "7. feature 0 AssortmentType (0.029254)\n",
      "8. feature 12 Region_PopulationK (0.018064)\n",
      "9. feature 11 Region_GDP (0.017565)\n",
      "10. feature 14 StoreType (0.017121)\n",
      "11. feature 10 Region_AreaKM2 (0.016628)\n",
      "12. feature 3 Day of year (0.015325)\n",
      "13. feature 9 Region (0.014041)\n",
      "14. feature 1 Day of month (0.013621)\n",
      "15. feature 15 Week (0.012844)\n",
      "16. feature 7 Month (number) (0.010386)\n",
      "17. feature 16 Year (0.003612)\n",
      "Training data set shape: (80442, 17)\n",
      "Measured Fit Time:  2.2266056537628174\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.979246\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.26740408 1.62240291 2.10800362 2.62180448 3.05760527]\n",
      "XVR_fit_time: 2.135444\n",
      "score_time Actuals:  [0.87660146 0.87560129 0.87560177 0.87660193 0.87360168]\n",
      "score_time: 0.875602\n",
      " \n",
      " \n",
      "Prediction Time:  0.23400044441223145\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.64\n",
      "MAE: 1140.692240\n",
      "MSE: 2349348.979000\n",
      "RMSE: 1532.758617\n",
      "Validation Set R2: 0.607946\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.63544903] 0.5973454717154992\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.405038)\n",
      "2. feature 4 HasPromotions (0.171722)\n",
      "3. feature 2 Day of week (number) (0.116403)\n",
      "4. feature 5 IsHoliday (0.045028)\n",
      "5. feature 13 StoreID (0.042028)\n",
      "6. feature 8 NearestCompetitor (0.040620)\n",
      "7. feature 0 AssortmentType (0.032246)\n",
      "8. feature 14 StoreType (0.021267)\n",
      "9. feature 11 Region_GDP (0.019703)\n",
      "10. feature 10 Region_AreaKM2 (0.017813)\n",
      "11. feature 12 Region_PopulationK (0.017374)\n",
      "12. feature 9 Region (0.016238)\n",
      "13. feature 3 Day of year (0.014885)\n",
      "14. feature 1 Day of month (0.013727)\n",
      "15. feature 15 Week (0.012273)\n",
      "16. feature 7 Month (number) (0.010133)\n",
      "17. feature 16 Year (0.003501)\n",
      "Training data set shape: (90497, 17)\n",
      "Measured Fit Time:  2.4332163333892822\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 23.892272\n",
      "[0.68329983 0.55466659 0.65183528 0.66779217 0.39102993]\n",
      "EV: 0.589725\n",
      "[ 976.44519848 1292.50419848 1133.46605205 1044.31633422 1269.88681822]\n",
      "MAE: 1143.323720\n",
      "[2070151.61218798 3975607.97450234 2425820.47504332 2393693.14476846\n",
      " 6059182.7266187 ]\n",
      "MSE: 3384891.186624\n",
      "[1438.80214491 1993.89266875 1557.50456662 1547.15647068 2461.54072211]\n",
      "RMSE: 1839.807378\n",
      "XV R2 Actuals: [0.66277831 0.55438427 0.65164117 0.66778777 0.39081453]\n",
      "Cross Validation R2: 0.585481\n",
      "XVR_fit_time Actuals:  [1.14081073 1.60680294 2.10800385 2.57921171 3.11420774]\n",
      "XVR_fit_time: 2.109807\n",
      "score_time Actuals:  [0.88020992 0.88920164 0.89020181 0.87360168 0.88920164]\n",
      "score_time: 0.884483\n",
      " \n",
      " \n",
      "Prediction Time:  0.218400239944458\n",
      " \n",
      "Validation data set shape: (20405, 17)\n",
      "Validation Set Explained Variance (EV): 0.68\n",
      "MAE: 1118.532198\n",
      "MSE: 2114550.408269\n",
      "RMSE: 1454.149376\n",
      "Validation Set R2: 0.647128\n",
      "Total Mean EV:  [0.68329983 0.55466659 0.65183528 0.66779217 0.39102993 0.67901694] 0.6046067896890618\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.371401)\n",
      "2. feature 4 HasPromotions (0.153958)\n",
      "3. feature 2 Day of week (number) (0.098718)\n",
      "4. feature 13 StoreID (0.048849)\n",
      "5. feature 11 Region_GDP (0.047933)\n",
      "6. feature 14 StoreType (0.042108)\n",
      "7. feature 8 NearestCompetitor (0.041565)\n",
      "8. feature 0 AssortmentType (0.040047)\n",
      "9. feature 5 IsHoliday (0.038906)\n",
      "10. feature 10 Region_AreaKM2 (0.025018)\n",
      "11. feature 12 Region_PopulationK (0.021944)\n",
      "12. feature 9 Region (0.019908)\n",
      "13. feature 3 Day of year (0.013818)\n",
      "14. feature 1 Day of month (0.012168)\n",
      "15. feature 15 Week (0.011183)\n",
      "16. feature 7 Month (number) (0.009470)\n",
      "17. feature 16 Year (0.003006)\n"
     ]
    }
   ],
   "source": [
    "SampleSizes = [20, 30, 40, 50, 60, 70, 80, 90] \n",
    "for i in SampleSizes:\n",
    "    CrossEval(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:12:30.181247Z",
     "start_time": "2019-06-22T14:12:30.120244Z"
    }
   },
   "outputs": [],
   "source": [
    "def PatternSearch(param_dist, PercentToUse=20, Mdl=ExtraTreesRegressor(), alfa=2, verbose=False, metric=2):\n",
    "    \n",
    "    class Dimension():\n",
    "        def __init__(self, value):\n",
    "            #If value is a Tuple, divide the interval into \"length\" equaly spaced intervals:\n",
    "            if isinstance(value,tuple):\n",
    "               lower=value[0];upper=value[1];length=value[2]\n",
    "               value=[lower + x*(upper-lower)/(length-1) for x in range(length)]\n",
    "            self.value=value\n",
    "            self.value.sort()\n",
    "            self.min=self.value[0]\n",
    "            self.max=self.value[-1]\n",
    "            self.midptidx=int((len(self.value)/2)-0.5)\n",
    "            self.midpoint=self.value[self.midptidx]\n",
    "            self.Delta=(len(value)-1)-self.midptidx\n",
    "            self.BestValue=self.midpoint\n",
    "            self.CurrIndex=self.midptidx\n",
    "            self.BestIndex=self.midptidx\n",
    "        \n",
    "    Space={}\n",
    "    BestScore=[]; CurrScore=[]\n",
    "    for Dkey, Dval in param_dist.items():\n",
    "        Space[Dkey]=Dimension(Dval)\n",
    "        print(Dkey,\":\",Dval)\n",
    "    \n",
    "    #Episilon=0.001;\n",
    "    k=0.5\n",
    "    xc={}\n",
    "    #print(Space)\n",
    "    #builds the first exploratory point by collecting the midpoint of each dimension:\n",
    "    for Dkey, Dval in Space.items():\n",
    "        xc[Dkey]=Dval.midpoint\n",
    "    \n",
    "    print(xc)    \n",
    "    BestScore=CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "                                 PercentToUse, Mdl=Mdl,MdlParams=xc,verbose=verbose)\n",
    "    BestScore=list(BestScore)\n",
    "    #print(\"Type BestScore: \",type(BestScore))\n",
    "    cols=list(xc.keys())\n",
    "    cols.append('score')\n",
    "    df=pd.DataFrame(columns=cols)\n",
    "    xc.update({'score':BestScore[metric]})\n",
    "    df=df.append(xc, ignore_index=True)\n",
    "    print(df)\n",
    "\n",
    "    Ndimensions=len(Space);  \n",
    "    BestIdx=0; InitialExploration=0\n",
    "\n",
    "    # i=exploratory moves iterations; k=Overall Iterations\n",
    "    i=0; Continue=0\n",
    "\n",
    "    while Continue<3:\n",
    "        #Exploratory Search:\n",
    "        while i < Ndimensions:\n",
    "            k+=1\n",
    "            for Direction in [1,-1]:\n",
    "                xn={}; xd={}\n",
    "                for CurDim in range(0,Ndimensions): #Build the vextor xn:\n",
    "                    if i == CurDim:\n",
    "                        NewIndex=list(Space.values())[CurDim].CurrIndex + Direction*list(Space.values())[CurDim].Delta\n",
    "                        #print(NewIndex)\n",
    "                        if NewIndex>len(list(Space.values())[CurDim].value)-1: NewIndex=len(list(Space.values())[CurDim].value)-1\n",
    "                        if NewIndex<0: NewIndex=0\n",
    "                        xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].value[NewIndex]\n",
    "                    else:\n",
    "                        xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].BestValue\n",
    "                if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                    #print(\"Vetor\",xn,\" nao esta em df. Executando Random Forest\")\n",
    "                    print(Direction, xn)            \n",
    "                    CurrScore=CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "                                                 PercentToUse, Mdl=Mdl,MdlParams=xn,verbose=verbose)\n",
    "                    #cols=list(xn.keys())\n",
    "                    #cols.append('score')\n",
    "                    xd=xn.copy()\n",
    "                    xd.update({'score':CurrScore[metric]})\n",
    "                    df=df.append(xd, ignore_index=True)\n",
    "                    print(df)\n",
    "                    #print(CurrScore[metric])\n",
    "                    if CurrScore[metric] > BestScore[metric]: \n",
    "                        BestScore[metric]=CurrScore[metric]\n",
    "                        list(Space.values())[i].BestValue=list(Space.values())[i].value[NewIndex]\n",
    "                        list(Space.values())[i].BestIndex=NewIndex\n",
    "                        xc=xn.copy()\n",
    "                        BestIdx=k\n",
    "                        break\n",
    "            list(Space.values())[i].CurrIndex=list(Space.values())[i].BestIndex \n",
    "            i+=1\n",
    "\n",
    "        #xc={}\n",
    "        #for Dkey, Dval in Space.items():\n",
    "        #    xc[Dkey]=Dval.BestValue\n",
    "        #print(xc)\n",
    "\n",
    "        #pattern move:\n",
    "        BestIdx=int(BestIdx+0.5)\n",
    "        pm=df.values[BestIdx]+(df.values[BestIdx]-df.values[InitialExploration])\n",
    "        pm=pm[0:(len(pm)-1)]\n",
    "        print(\"Theoretical Pattern Move: \",pm)\n",
    "\n",
    "        #picks the closest elements in the lists to the ideal point\n",
    "        n=0\n",
    "        for Dkey, Dval in Space.items():\n",
    "            xn[Dkey]=min(Dval.value, key=lambda x:abs(x-pm[n])) \n",
    "            n+=1\n",
    "\n",
    "        #Evaluates pattern move it it has not been evaluated already:\n",
    "        if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "            print(xn)\n",
    "            print(\"Executing Pattern Move...\")\n",
    "            k+=1\n",
    "            CurrScore=CrossEval3(trainDataset_X,trainDataset_y,validBench_X,validBench_y,\n",
    "                                 PercentToUse, Mdl=Mdl,MdlParams=xn,verbose=verbose)\n",
    "            xd=xn.copy()\n",
    "            xd.update({'score':CurrScore[metric]})\n",
    "            df=df.append(xd, ignore_index=True)\n",
    "            print(df)\n",
    "            if CurrScore[metric] > BestScore[metric]:\n",
    "                BestScore[metric]=CurrScore[metric]\n",
    "                xc=xn.copy()\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(Space.values())[CurDim].BestIndex=list(Space.values())[CurDim].value.index(list(xc.values())[CurDim])\n",
    "                    list(Space.values())[CurDim].CurrIndex=list(Space.values())[CurDim].BestIndex\n",
    "                    list(Space.values())[CurDim].BestValue=list(xc.values())[CurDim]\n",
    "                BestIdx=k\n",
    "                InitialExploration=BestIdx\n",
    "                BestScore[metric]=CurrScore[metric]\n",
    "            else:\n",
    "                #divide delta by 2\n",
    "                DeltaVector=[]\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "                    DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "        else:\n",
    "            #divide delta by 2\n",
    "            print(\"Closest point in the Space\",xn,\" has been evaluated\")\n",
    "            DeltaVector=[]\n",
    "            for CurDim in range(0,Ndimensions):\n",
    "                list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "                if list(Space.values())[CurDim].Delta==0: list(Space.values())[CurDim].Delta=1\n",
    "                DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "\n",
    "        print(\"Current Delta values for each dimension: \",DeltaVector)\n",
    "        i=0 \n",
    "        print(Ndimensions)\n",
    "        if DeltaVector==list(np.ones(len(Space))): Continue+=1\n",
    "\n",
    "    print(\"Best Parameters Found:\")\n",
    "    xc=dict(df.loc[df['score'].idxmax()])\n",
    "    del xc[\"score\"]\n",
    "    \n",
    "    Mdl.set_params(**xc)\n",
    "    print(Mdl.get_params(deep=True))\n",
    "    print(\" \");print(xc);print(\" \")\n",
    "    print(df.loc[df['score'].idxmax()])\n",
    "    df.sort_values('score', axis=0, ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T22:14:43.749557Z",
     "start_time": "2019-06-18T22:14:01.678150Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators : [4, 5, 6]\n",
      "max_depth : [4, 5, 6, 10, 15, 20, 30, 33]\n",
      "{'n_estimators': 5, 'max_depth': 10}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 2.395137\n",
      "Type BestScore:  <class 'list'>\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1 {'n_estimators': 6, 'max_depth': 10}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 2.806160\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "1 {'n_estimators': 6, 'max_depth': 33}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 7.112407\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "-1 {'n_estimators': 6, 'max_depth': 4}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 1.682096\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "Theoretical Pattern Move:  [ 7. 10.]\n",
      "Closest point in the Space {'n_estimators': 6, 'max_depth': 10}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 3]\n",
      "2\n",
      "1 {'n_estimators': 6, 'max_depth': 30}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 6.680382\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "4           6.0       30.0  0.547394\n",
      "Theoretical Pattern Move:  [ 7. 10.]\n",
      "Closest point in the Space {'n_estimators': 6, 'max_depth': 10}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 2]\n",
      "2\n",
      "1 {'n_estimators': 6, 'max_depth': 20}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 5.869336\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "4           6.0       30.0  0.547394\n",
      "5           6.0       20.0  0.509679\n",
      "-1 {'n_estimators': 6, 'max_depth': 5}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 1.582090\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "4           6.0       30.0  0.547394\n",
      "5           6.0       20.0  0.509679\n",
      "6           6.0        5.0  0.569176\n",
      "Theoretical Pattern Move:  [7. 0.]\n",
      "Closest point in the Space {'n_estimators': 6, 'max_depth': 4}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1]\n",
      "2\n",
      "-1 {'n_estimators': 5, 'max_depth': 5}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 1.438082\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "4           6.0       30.0  0.547394\n",
      "5           6.0       20.0  0.509679\n",
      "6           6.0        5.0  0.569176\n",
      "7           5.0        5.0  0.566254\n",
      "1 {'n_estimators': 6, 'max_depth': 6}\n",
      "Current Model:  ExtraTreesRegressor\n",
      "100553\n",
      "Pandas Value: True\n",
      "Training data set shape: (100553, 17)\n",
      "(100553, 17) (100553, 1)\n",
      "Cross Validation Time: 2.011115\n",
      "   n_estimators  max_depth     score\n",
      "0           5.0       10.0  0.550999\n",
      "1           6.0       10.0  0.557374\n",
      "2           6.0       33.0  0.516668\n",
      "3           6.0        4.0  0.556537\n",
      "4           6.0       30.0  0.547394\n",
      "5           6.0       20.0  0.509679\n",
      "6           6.0        5.0  0.569176\n",
      "7           5.0        5.0  0.566254\n",
      "8           6.0        6.0  0.550972\n",
      "Theoretical Pattern Move:  [7. 0.]\n",
      "Closest point in the Space {'n_estimators': 6, 'max_depth': 4}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1]\n",
      "2\n",
      "Theoretical Pattern Move:  [7. 0.]\n",
      "Closest point in the Space {'n_estimators': 6, 'max_depth': 4}  has been evaluated\n",
      "Current Delta values for each dimension:  [1, 1]\n",
      "2\n",
      "Best Parameters Found:\n",
      "{'bootstrap': False, 'criterion': 'mse', 'max_depth': 5.0, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 6.0, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_estimators    6.000000\n",
       "max_depth       5.000000\n",
       "score           0.569176\n",
       "Name: 6, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dist={'n_estimators':[4,5,6],\n",
    "            'max_depth':[4,5,6,10,15,20,30,33]}\n",
    "PatternSearch(param_dist, 100, alfa=1.4, metric=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T18:13:05.387137Z",
     "start_time": "2019-06-18T18:13:05.371136Z"
    }
   },
   "outputs": [],
   "source": [
    "PercentToUse=100\n",
    "alfa=2;Episilon=0.001;k=0\n",
    "\n",
    "class Dimension():\n",
    "    def __init__(self, value=[]):\n",
    "        self.value=value\n",
    "        self.value.sort()\n",
    "        self.min=self.value[0]\n",
    "        self.max=self.value[len(value)-1]\n",
    "        self.midptidx=int((len(self.value)/2)-0.5)\n",
    "        self.midpoint=self.value[self.midptidx]\n",
    "        self.Delta=(len(value)-1)-self.midptidx\n",
    "        self.BestValue=self.midpoint\n",
    "        self.CurrIndex=self.midptidx\n",
    "        self.BestIndex=self.midptidx\n",
    "\n",
    "Space={'n_estimators':Dimension([230,240,250]),\n",
    "       'max_features':Dimension([2,3]),\n",
    "       'max_depth':Dimension([16,17,30])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T18:48:28.137967Z",
     "start_time": "2019-06-18T18:48:28.134967Z"
    }
   },
   "source": [
    "# Pattern SearchCV Sklearn Style (Hooke-Jeeves Method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:11:17.621905Z",
     "start_time": "2019-06-22T14:11:17.610904Z"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import defaultdict\n",
    "from collections.abc import Mapping, Sequence, Iterable\n",
    "from functools import partial, reduce\n",
    "from itertools import product\n",
    "import operator\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from sklearn.base import BaseEstimator, is_classifier, clone\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.model_selection._validation import _fit_and_score\n",
    "from sklearn.model_selection._validation import _aggregate_score_dicts\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.utils._joblib import Parallel, delayed\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.fixes import MaskedArray\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils.validation import indexable, check_is_fitted\n",
    "from sklearn.utils.metaestimators import if_delegate_has_method\n",
    "from sklearn.metrics.scorer import _check_multimetric_scoring\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "from sklearn.model_selection._search import BaseSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:11:19.029373Z",
     "start_time": "2019-06-22T14:11:18.898767Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatternSearchCV(BaseSearchCV):\n",
    "    _required_parameters = [\"estimator\", \"param_distributions\"]\n",
    "\n",
    "    def __init__(self, estimator, param_distributions, scoring=None, #n_iter=10,\n",
    "                 n_jobs=None, iid='warn', refit=True,\n",
    "                 cv='warn', verbose=0, pre_dispatch='2*n_jobs',\n",
    "                 random_state=None, error_score='raise-deprecating',\n",
    "                 return_train_score=False):\n",
    "        self.param_distributions = param_distributions\n",
    "        #self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        super().__init__(\n",
    "            estimator=estimator, scoring=scoring,\n",
    "            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,\n",
    "            pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "            return_train_score=return_train_score)\n",
    "        self.ResultDf = pd.DataFrame()\n",
    "\n",
    "#     param_dist={'n_estimators':[230,240,250],\n",
    "#            'max_features':[2,3],\n",
    "#            'max_depth':[16,17,30]}        \n",
    "        \n",
    "        class Dimension():\n",
    "            def __init__(self, value):\n",
    "                #If value is a Tuple, divide the interval into \"length\" equaly spaced intervals:\n",
    "                if isinstance(value,tuple):\n",
    "                   lower=value[0];upper=value[1];length=value[2]\n",
    "                   value=[lower + x*(upper-lower)/(length-1) for x in range(length)]\n",
    "                self.value=value\n",
    "                self.value.sort()\n",
    "                self.min=self.value[0]\n",
    "                self.max=self.value[-1]\n",
    "                self.midptidx=int((len(self.value)/2)-0.5)\n",
    "                self.midpoint=self.value[self.midptidx]\n",
    "                self.Delta=(len(value)-1)-self.midptidx\n",
    "                self.BestValue=self.midpoint\n",
    "                self.CurrIndex=self.midptidx\n",
    "                self.BestIndex=self.midptidx\n",
    "        \n",
    "        self.Space={}\n",
    "        for Dkey, Dval in param_dist.items():\n",
    "            self.Space[Dkey]=Dimension(Dval)\n",
    "            print(Dkey,\":\",Dval) \n",
    "\n",
    "    def _run_search(self, evaluate_candidates):\n",
    "        \"\"\"Search best parameters using Pattern Search Method\"\"\"\n",
    "        #        PercentToUse=20\n",
    "        alfa=2; Episilon=0.001; k=0\n",
    "        Ndimensions=len(self.Space); \n",
    "\n",
    "        xc={}; xd={}\n",
    "        #builds the first exploratory point by collecting the midpoint of each dimension:\n",
    "        for Dkey, Dval in self.Space.items():\n",
    "            xc[Dkey]=Dval.midpoint   \n",
    "\n",
    "\n",
    "        DeltaVector=[]\n",
    "        for CurDim in range(0,Ndimensions):\n",
    "            DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "        print(\" \")\n",
    "        print(\"Current Delta values for each dimension: \", DeltaVector)\n",
    "\n",
    "        BestScore = evaluate_candidates([xc])['mean_test_score'][-1]\n",
    "        print(\" \")\n",
    "        print(xc)\n",
    "\n",
    "        cols=list(xc.keys())\n",
    "        cols.append('score')\n",
    "        df=pd.DataFrame(columns=cols)\n",
    "        xd=xc.copy()\n",
    "        xd.update({'score':BestScore})\n",
    "        df=df.append(xd, ignore_index=True)\n",
    "        print(df)\n",
    "\n",
    "        BestIdx=0; InitialExploration=0\n",
    "\n",
    "        # i=exploratory moves iterations; k=Overall Iterations\n",
    "        i=0; Continue=0\n",
    "\n",
    "        while Continue<3:\n",
    "            #Exploratory Search:\n",
    "            while i < Ndimensions:\n",
    "                k+=1\n",
    "                for Direction in [1,-1]:\n",
    "                    xn={}; xd={}\n",
    "                    for CurDim in range(0,Ndimensions): #Build the vector xn:\n",
    "                        if i == CurDim:\n",
    "                            NewIndex=list(self.Space.values())[CurDim].CurrIndex + Direction*list(self.Space.values())[CurDim].Delta\n",
    "                            #print(NewIndex)\n",
    "                            if NewIndex>len(list(self.Space.values())[CurDim].value)-1: NewIndex=len(list(self.Space.values())[CurDim].value)-1\n",
    "                            if NewIndex<0: NewIndex=0\n",
    "                            xn[list(self.Space.keys())[CurDim]]=list(self.Space.values())[CurDim].value[NewIndex]\n",
    "                        else:\n",
    "                            xn[list(self.Space.keys())[CurDim]]=list(self.Space.values())[CurDim].BestValue\n",
    "                    if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                        #print(\"Vetor\",xn,\" nao esta em df. Executando Random Forest\")\n",
    "                        print(Direction, xn)            \n",
    "                        CurrScore = evaluate_candidates([xn])['mean_test_score'][-1]\n",
    "                        #cols=list(xn.keys())\n",
    "                        #cols.append('score')\n",
    "                        xd=xn.copy()\n",
    "                        xd.update({'score':CurrScore})\n",
    "                        df=df.append(xd, ignore_index=True)\n",
    "                        print(df)\n",
    "                        #print(CurrScore)\n",
    "                        if CurrScore > BestScore: \n",
    "                            BestScore=CurrScore\n",
    "                            list(self.Space.values())[i].BestValue=list(self.Space.values())[i].value[NewIndex]\n",
    "                            list(self.Space.values())[i].BestIndex=NewIndex\n",
    "                            #xc=xn.copy()\n",
    "                            BestIdx=k-1\n",
    "                            break\n",
    "                list(self.Space.values())[i].CurrIndex=list(self.Space.values())[i].BestIndex \n",
    "                i+=1\n",
    "\n",
    "            #pattern move:\n",
    "            pm=df.values[BestIdx]+(df.values[BestIdx]-df.values[InitialExploration])\n",
    "            pm=pm[0:(len(pm)-1)]\n",
    "            print(\"Theoretical Pattern Move: \",pm)\n",
    "\n",
    "            #picks the closest elements in the lists to the ideal point\n",
    "            n=0\n",
    "            for Dkey, Dval in self.Space.items():\n",
    "                xn[Dkey]=min(Dval.value, key=lambda x:abs(x-pm[n])) \n",
    "                n+=1\n",
    "\n",
    "            #Evaluates pattern move it it has not been evaluated already:\n",
    "            if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                print(xn)\n",
    "                print(\"Executing Pattern Move...\")\n",
    "                k+=1\n",
    "                CurrScore = evaluate_candidates([xn])['mean_test_score'][-1]\n",
    "                xd=xn.copy()\n",
    "                xd.update({'score':CurrScore})\n",
    "                df=df.append(xd, ignore_index=True)\n",
    "                print(df)\n",
    "                if CurrScore > BestScore:\n",
    "                    BestScore=CurrScore\n",
    "                    xc=xn.copy()\n",
    "                    for CurDim in range(0,Ndimensions):\n",
    "                        list(self.Space.values())[CurDim].BestIndex=list(self.Space.values())[CurDim].value.index(list(xc.values())[CurDim])\n",
    "                        list(self.Space.values())[CurDim].CurrIndex=list(self.Space.values())[CurDim].BestIndex\n",
    "                        list(self.Space.values())[CurDim].BestValue=list(xc.values())[CurDim]\n",
    "                    BestIdx=k\n",
    "                    InitialExploration=BestIdx\n",
    "                    BestScore=CurrScore\n",
    "                else:\n",
    "                    #divide delta by 2\n",
    "                    DeltaVector=[]\n",
    "                    for CurDim in range(0,Ndimensions):\n",
    "                        list(self.Space.values())[CurDim].Delta=int(0.5+list(self.Space.values())[CurDim].Delta/alfa)\n",
    "                        DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "            else:\n",
    "                #divide delta by 2\n",
    "                print(\"Nearest Pattern move point\",xn,\" has been evaluated\")\n",
    "                DeltaVector=[]\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(self.Space.values())[CurDim].Delta=int(0.5+list(self.Space.values())[CurDim].Delta/alfa)\n",
    "                    if list(self.Space.values())[CurDim].Delta==0: list(self.Space.values())[CurDim].Delta=1\n",
    "                    DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "\n",
    "            print(\"Current Delta values for each dimension: \",DeltaVector)\n",
    "            i=0 \n",
    "            if DeltaVector==list(np.ones(Ndimensions)): Continue+=1\n",
    "\n",
    "        print(\" \");print(\" \")\n",
    "        print(\"Best Parameters Found:\")\n",
    "        print(df.loc[df['score'].idxmax()])\n",
    "        print(\" \");print(\" \")\n",
    "        self.ResultDf=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T14:11:25.848858Z",
     "start_time": "2019-06-22T14:11:22.714528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Time Series Cross Validation\n",
      "max_depth : [3, 4, 5, 6, 7, 8]\n",
      "n_estimators : [47, 48, 49, 50, 51, 52, 53, 54, 55, 57]\n",
      "learning_rate : [0.097, 0.098, 0.099, 0.1, 0.101]\n",
      "colsample_bylevel : [0.49, 0.5, 0.51, 1]\n",
      "colsample_bytree : [0.1, 0.25, 0.5, 0.75, 1]\n",
      "gamma : [0, 0.1, 5, 10, 20]\n",
      " \n",
      "Current Delta values for each dimension:  [3, 5, 2, 2, 2, 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter learning_rate for estimator ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=5,\n          max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n          min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=-1,\n          oob_score=False, random_state=0, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 514, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"c:\\program files\\python36\\lib\\site-packages\\sklearn\\base.py\", line 213, in set_params\n    (key, self))\nValueError: Invalid parameter learning_rate for estimator ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=5,\n          max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n          min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=-1,\n          oob_score=False, random_state=0, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e08095160ca1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mpattern_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PatternSearchCV took %.2f seconds \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5ae8124908fa>\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current Delta values for each dimension: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeltaVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mBestScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter learning_rate for estimator ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=5,\n          max_features=2, max_leaf_nodes=None, min_impurity_decrease=0.0,\n          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n          min_weight_fraction_leaf=0.0, n_estimators=51, n_jobs=-1,\n          oob_score=False, random_state=0, verbose=0, warm_start=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "#Execute Pattern Search:\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Load data\n",
    "SplitPercent=20\n",
    "Nrows,_=trainDataset_X.shape\n",
    "SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "X, y = trainDataset_X.iloc[:SplitPoint, :], trainDataset_y.iloc[:SplitPoint]\n",
    "\n",
    "\n",
    "# build model\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=240, \n",
    "                                 max_features = int(X.columns.size - 15),\n",
    "                                 max_depth = 16,\n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=0)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "\n",
    "#param_dist={'n_estimators':[5,6,7],\n",
    "#           'max_features':[2,3,4,5]}\n",
    "           #'n_estimators': list(np.linspace(0, 250, num=26, endpoint=True,dtype=int)+10),\n",
    "           #'lr':list(np.logspace(-2, -5, num=4, endpoint=True, base=10.0)),\n",
    "           #'max_depth':list(range(5,18,1))}\n",
    "            \n",
    "param_dist={'max_depth':[3,4,5,6,7,8],\n",
    "            'n_estimators': [47, 48, 49, 50,51,52,53,54,55,57],\n",
    "            'learning_rate': [0.097,0.098, 0.099,0.1,0.101],\n",
    "            'colsample_bylevel': [0.49,0.5,0.51,1],\n",
    "            'colsample_bytree': [0.1,0.25,0.5,0.75,1],\n",
    "            'gamma': [0,0.1,5,10,20]\n",
    "           }            \n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "print(\"Using Time Series Cross Validation\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "#scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "\n",
    "\n",
    "# run randomized search\n",
    "pattern_search = PatternSearchCV(clf, param_distributions=param_dist,\n",
    "                                    cv=tscv, n_jobs=-1, verbose=0) \n",
    "\n",
    "start = time()\n",
    "pattern_search.fit(X, y.values.ravel())\n",
    "print(\"PatternSearchCV took %.2f seconds \" % ((time() - start)))\n",
    "report(pattern_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-18T19:01:50.097327Z",
     "start_time": "2019-06-18T19:01:50.082326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.534920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.547069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.538547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.542613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.555353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.548891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_features     score\n",
       "0           6.0           3.0  0.534920\n",
       "1           7.0           3.0  0.547069\n",
       "2           7.0           5.0  0.538547\n",
       "3           7.0           2.0  0.542613\n",
       "4           7.0           4.0  0.555353\n",
       "5           6.0           4.0  0.548891"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_search.ResultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Search Matlab Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:20:26.245583Z",
     "start_time": "2019-06-19T14:20:26.188579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Run This cell and then the \"#Execute Pattern Search MATLAB:\" cell\n",
    "#Pattern Search MATLAB Style:\n",
    "class PatternSearchCV(BaseSearchCV):\n",
    "    _required_parameters = [\"estimator\", \"param_distributions\"]\n",
    "\n",
    "    def __init__(self, estimator, param_distributions, scoring=None, #n_iter=10,\n",
    "                 n_jobs=None, iid='warn', refit=True,\n",
    "                 cv='warn', verbose=0, pre_dispatch='2*n_jobs',\n",
    "                 random_state=None, error_score='raise-deprecating',\n",
    "                 return_train_score=False):\n",
    "        self.param_distributions = param_distributions\n",
    "        #self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        super().__init__(\n",
    "            estimator=estimator, scoring=scoring,\n",
    "            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,\n",
    "            pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "            return_train_score=return_train_score)\n",
    "        self.ResultDf = pd.DataFrame()\n",
    "\n",
    "#     param_dist={'n_estimators':[230,240,250],\n",
    "#            'max_features':[2,3],\n",
    "#            'max_depth':[16,17,30]}        \n",
    "        \n",
    "        class Dimension():\n",
    "            def __init__(self, value):\n",
    "                #If value is a Tuple, divide the interval into \"length\" equaly spaced intervals:\n",
    "                if isinstance(value,tuple):\n",
    "                   lower=value[0];upper=value[1];length=value[2]\n",
    "                   value=[lower + x*(upper-lower)/(length-1) for x in range(length)]\n",
    "                self.value=value\n",
    "                self.value.sort()\n",
    "                self.min=self.value[0]\n",
    "                self.max=self.value[-1]\n",
    "                self.midptidx=int((len(self.value)/2)-0.5)\n",
    "                self.midpoint=self.value[self.midptidx]\n",
    "                self.Delta=(len(value)-1)-self.midptidx\n",
    "                self.BestValue=self.midpoint\n",
    "                self.CurrIndex=self.midptidx\n",
    "                self.BestIndex=self.midptidx\n",
    "        \n",
    "        self.Space={}\n",
    "        for Dkey, Dval in param_dist.items():\n",
    "            self.Space[Dkey]=Dimension(Dval)\n",
    "            print(Dkey,\":\",Dval) \n",
    "\n",
    "    def _run_search(self, evaluate_candidates):\n",
    "        \"\"\"Search best parameters using Pattern Search Method\"\"\"\n",
    "        alfa=2;Episilon=0.001 ;k=0\n",
    "        Ndimensions=len(self.Space); \n",
    "\n",
    "        xc={}; xd={}; xb={}\n",
    "        #builds the first exploratory point by collecting the midpoint of each dimension:\n",
    "        for Dkey, Dval in self.Space.items():\n",
    "            xc[Dkey]=Dval.midpoint   \n",
    "\n",
    "        for CurDim in range(0,Ndimensions): #divide Delta by 2\n",
    "            #print(\"Dividing delta by 2:\")\n",
    "            list(self.Space.values())[CurDim].Delta=int(0.5+list(self.Space.values())[CurDim].Delta/alfa)\n",
    "\n",
    "        DeltaVector=[]\n",
    "        for CurDim in range(0,Ndimensions):\n",
    "            DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "        print(\" \")\n",
    "        print(\"Current Delta values for each dimension: \", DeltaVector)\n",
    "\n",
    "        #Evaluate the first point as the midpoint of the search space:\n",
    "        BestScore = evaluate_candidates([xc])['mean_test_score'][-1]\n",
    "        print(\" \")\n",
    "        print(xc)\n",
    "\n",
    "        cols=list(xc.keys())\n",
    "        cols.append('score')\n",
    "        df=pd.DataFrame(columns=cols)\n",
    "        xd=xc.copy()\n",
    "        xd.update({'score':BestScore})\n",
    "        df=df.append(xd, ignore_index=True)\n",
    "        print(df)\n",
    "\n",
    "        BestIdx=0; InitialExploration=0\n",
    "\n",
    "\n",
    "        # i=exploratory moves iterations; k=Overall Iterations\n",
    "        i=0; Continue=0\n",
    "\n",
    "        while Continue<3:\n",
    "            #Exploratory Search:\n",
    "            while i < Ndimensions:\n",
    "                k+=1\n",
    "                #print(\"Iteration k:\",k)\n",
    "                print(\"Dimension: \",i+1)\n",
    "                for Direction in [1,-1]:\n",
    "                    xn={}; xd={}\n",
    "                    for CurDim in range(0,Ndimensions): #Build the vextor xn:\n",
    "                        if i == CurDim:\n",
    "                            NewIndex=list(self.Space.values())[CurDim].CurrIndex + Direction*list(self.Space.values())[CurDim].Delta\n",
    "                            if NewIndex>len(list(self.Space.values())[CurDim].value)-1: NewIndex=len(list(self.Space.values())[CurDim].value)-1\n",
    "                            if NewIndex<0: NewIndex=0\n",
    "                            xn[list(self.Space.keys())[CurDim]]=list(self.Space.values())[CurDim].value[NewIndex]\n",
    "                        else:\n",
    "                            CurrInd=list(self.Space.values())[CurDim].CurrIndex\n",
    "                            if CurrInd>len(list(self.Space.values())[CurDim].value)-1: CurrInd=len(list(self.Space.values())[CurDim].value)-1\n",
    "                            if CurrInd<0: CurrInd =0\n",
    "                            xn[list(self.Space.keys())[CurDim]]=list(self.Space.values())[CurDim].value[CurrInd]\n",
    "                    if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                        print(Direction, xn)            \n",
    "                        CurrScore = evaluate_candidates([xn])['mean_test_score'][-1]\n",
    "                        #cols=list(xn.keys())\n",
    "                        #cols.append('score')\n",
    "                        xd=xn.copy()\n",
    "                        xd.update({'score':CurrScore})\n",
    "                        df=df.append(xd, ignore_index=True)\n",
    "                        print(df)\n",
    "                        #print(CurrScore)\n",
    "                        if CurrScore > BestScore: \n",
    "                            BestScore=CurrScore\n",
    "                            list(self.Space.values())[i].BestValue=list(self.Space.values())[i].value[NewIndex]\n",
    "                            list(self.Space.values())[i].BestIndex=NewIndex\n",
    "                            xb=xn.copy()\n",
    "                            BestIdx=k-1\n",
    "\n",
    "                i+=1\n",
    "            xd={}    \n",
    "            NotEqual=False\n",
    "            for Dkey, Dval in self.Space.items():\n",
    "                #print(Dval.CurrIndex,Dval.BestIndex)\n",
    "                if Dval.CurrIndex != Dval.BestIndex: NotEqual=True\n",
    "            print(\" \");print(\" \");\n",
    "            #print(\"NotEqual =\",NotEqual)\n",
    "            DeltaVector=[]\n",
    "            if NotEqual:\n",
    "                for CurDim in range(0,Ndimensions): #update Current index to Best Point:\n",
    "                    list(self.Space.values())[CurDim].CurrIndex=list(self.Space.values())[CurDim].BestIndex\n",
    "\n",
    "                print(\"Best vector so far (xb):\",xb) \n",
    "\n",
    "                #Multiply delta by 2\n",
    "                print(\"multiplying delta by 2:\")\n",
    "                for CurDim in range(0,Ndimensions):\n",
    "                    list(self.Space.values())[CurDim].Delta=int(0.5+list(self.Space.values())[CurDim].Delta*alfa)\n",
    "                    DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "            else:\n",
    "                print(\"Dividing delta by 2:\")\n",
    "                for CurDim in range(0,Ndimensions): #Stay in current point and divide Delta by 2\n",
    "                    list(self.Space.values())[CurDim].Delta=int(0.5+list(self.Space.values())[CurDim].Delta/alfa)\n",
    "                    DeltaVector.append(list(self.Space.values())[CurDim].Delta)\n",
    "\n",
    "            print(\"Current Delta values for each dimension: \",DeltaVector)\n",
    "            i=0 \n",
    "            if DeltaVector==list(np.ones(Ndimensions)): Continue+=1\n",
    "\n",
    "        print(\" \");print(\" \")\n",
    "        print(\"Best Parameters Found:\")\n",
    "        print(df.loc[df['score'].idxmax()])\n",
    "        print(\" \");print(\" \")\n",
    "        self.ResultDf=df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:20:31.499883Z",
     "start_time": "2019-06-19T14:20:31.477882Z"
    }
   },
   "outputs": [],
   "source": [
    "#Execute Pattern Search MATLAB:\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "      \n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "# run Pattern search:\n",
    "\n",
    "def PatternSearchMatLab(param_dist,PercentToUse=20,Mdl=ExtraTreesRegressor()):\n",
    "    # Load data\n",
    "    SplitPercent=PercentToUse\n",
    "    Nrows,_=trainDataset_X.shape\n",
    "    SplitPoint=int(Nrows*(SplitPercent/100))\n",
    "    X, y = trainDataset_X.iloc[:SplitPoint, :], trainDataset_y.iloc[:SplitPoint]\n",
    "    print(\"Using Time Series Cross Validation\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    #scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "    \n",
    "    pattern_search = PatternSearchCV(Mdl, param_distributions=param_dist,\n",
    "                                        cv=tscv, n_jobs=-1, verbose=0) \n",
    "\n",
    "    start = time()\n",
    "    pattern_search.fit(X, y.values.ravel())\n",
    "    print(\"PatternSearchCV took %.2f seconds \" % ((time() - start)))\n",
    "    report(pattern_search.cv_results_)\n",
    "    return pattern_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:39:45.303851Z",
     "start_time": "2019-06-19T14:27:14.385927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Time Series Cross Validation\n",
      "max_depth : [3, 4, 5, 6, 7, 8]\n",
      "n_estimators : [47, 48, 49, 50, 51, 52, 53, 54, 55, 57]\n",
      "learning_rate : [0.097, 0.098, 0.099, 0.1, 0.101]\n",
      "colsample_bylevel : [0.49, 0.5, 0.51, 1]\n",
      "colsample_bytree : [0.1, 0.25, 0.5, 0.75, 1]\n",
      "gamma : [0, 0.1, 5, 10, 20]\n",
      " \n",
      "Current Delta values for each dimension:  [2, 3, 1, 1, 1, 1]\n",
      " \n",
      "{'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "Dimension:  1\n",
      "1 {'max_depth': 7, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "-1 {'max_depth': 3, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "2        3.0          51.0          0.099                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "Dimension:  2\n",
      "1 {'max_depth': 5, 'n_estimators': 54, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "2        3.0          51.0          0.099                0.5   \n",
      "3        5.0          54.0          0.099                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "-1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "2        3.0          51.0          0.099                0.5   \n",
      "3        5.0          54.0          0.099                0.5   \n",
      "4        5.0          48.0          0.099                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "4               0.5    5.0  0.561159  \n",
      "Dimension:  3\n",
      "1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.1, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "2        3.0          51.0          0.099                0.5   \n",
      "3        5.0          54.0          0.099                0.5   \n",
      "4        5.0          48.0          0.099                0.5   \n",
      "5        5.0          51.0          0.100                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "4               0.5    5.0  0.561159  \n",
      "5               0.5    5.0  0.554233  \n",
      "-1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.098, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099                0.5   \n",
      "1        7.0          51.0          0.099                0.5   \n",
      "2        3.0          51.0          0.099                0.5   \n",
      "3        5.0          54.0          0.099                0.5   \n",
      "4        5.0          48.0          0.099                0.5   \n",
      "5        5.0          51.0          0.100                0.5   \n",
      "6        5.0          51.0          0.098                0.5   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "4               0.5    5.0  0.561159  \n",
      "5               0.5    5.0  0.554233  \n",
      "6               0.5    5.0  0.548879  \n",
      "Dimension:  4\n",
      "1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.51, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099               0.50   \n",
      "1        7.0          51.0          0.099               0.50   \n",
      "2        3.0          51.0          0.099               0.50   \n",
      "3        5.0          54.0          0.099               0.50   \n",
      "4        5.0          48.0          0.099               0.50   \n",
      "5        5.0          51.0          0.100               0.50   \n",
      "6        5.0          51.0          0.098               0.50   \n",
      "7        5.0          51.0          0.099               0.51   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "4               0.5    5.0  0.561159  \n",
      "5               0.5    5.0  0.554233  \n",
      "6               0.5    5.0  0.548879  \n",
      "7               0.5    5.0  0.559353  \n",
      "-1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099               0.50   \n",
      "1        7.0          51.0          0.099               0.50   \n",
      "2        3.0          51.0          0.099               0.50   \n",
      "3        5.0          54.0          0.099               0.50   \n",
      "4        5.0          48.0          0.099               0.50   \n",
      "5        5.0          51.0          0.100               0.50   \n",
      "6        5.0          51.0          0.098               0.50   \n",
      "7        5.0          51.0          0.099               0.51   \n",
      "8        5.0          51.0          0.099               0.49   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0               0.5    5.0  0.559353  \n",
      "1               0.5    5.0  0.528280  \n",
      "2               0.5    5.0  0.548709  \n",
      "3               0.5    5.0  0.557150  \n",
      "4               0.5    5.0  0.561159  \n",
      "5               0.5    5.0  0.554233  \n",
      "6               0.5    5.0  0.548879  \n",
      "7               0.5    5.0  0.559353  \n",
      "8               0.5    5.0  0.544236  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.75, 'gamma': 5}\n",
      "   max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0        5.0          51.0          0.099               0.50   \n",
      "1        7.0          51.0          0.099               0.50   \n",
      "2        3.0          51.0          0.099               0.50   \n",
      "3        5.0          54.0          0.099               0.50   \n",
      "4        5.0          48.0          0.099               0.50   \n",
      "5        5.0          51.0          0.100               0.50   \n",
      "6        5.0          51.0          0.098               0.50   \n",
      "7        5.0          51.0          0.099               0.51   \n",
      "8        5.0          51.0          0.099               0.49   \n",
      "9        5.0          51.0          0.099               0.50   \n",
      "\n",
      "   colsample_bytree  gamma     score  \n",
      "0              0.50    5.0  0.559353  \n",
      "1              0.50    5.0  0.528280  \n",
      "2              0.50    5.0  0.548709  \n",
      "3              0.50    5.0  0.557150  \n",
      "4              0.50    5.0  0.561159  \n",
      "5              0.50    5.0  0.554233  \n",
      "6              0.50    5.0  0.548879  \n",
      "7              0.50    5.0  0.559353  \n",
      "8              0.50    5.0  0.544236  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9              0.75    5.0  0.543442  \n",
      "-1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 10}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "-1 {'max_depth': 5, 'n_estimators': 51, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 0.1}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [4, 6, 2, 2, 2, 2]\n",
      "Dimension:  1\n",
      "1 {'max_depth': 8, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "-1 {'max_depth': 3, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14              0.50    5.0  0.551587  \n",
      "Dimension:  2\n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "Dimension:  3\n",
      "1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.101, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "-1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.097, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "Dimension:  4\n",
      "1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18              0.50    5.0  0.575505  \n",
      "-1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "-1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22              0.50   20.0  0.561159  \n",
      "-1 {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 5, 'n_estimators': 48, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [8, 12, 4, 4, 4, 4]\n",
      "Dimension:  1\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "-1 {'max_depth': 3, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25              0.50    5.0  0.575677  \n",
      "Dimension:  2\n",
      "1 {'max_depth': 5, 'n_estimators': 57, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "Dimension:  3\n",
      "1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28              0.50    5.0  0.563909  \n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "Dimension:  4\n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31              1.00    5.0  0.549812  \n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "-1 {'max_depth': 5, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34              0.50    0.0  0.576045  \n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [16, 24, 8, 8, 8, 8]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "Dimension:  3\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36              0.50    5.0  0.595450  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38              0.50    5.0  0.534193  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40              0.10    5.0  0.234621  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42              0.50    0.0  0.593296  \n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [32, 48, 16, 16, 16, 16]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 3, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44              0.50    5.0  0.592783  \n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46              1.00    5.0  0.510440  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48              0.50   20.0  0.595450  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [16, 24, 8, 8, 8, 8]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [8, 12, 4, 4, 4, 4]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [4, 6, 2, 2, 2, 2]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 4, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50              0.50    5.0  0.584368  \n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52              0.50    5.0  0.553430  \n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [2, 3, 1, 1, 1, 1]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 6, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 50, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54              0.50    5.0  0.594732  \n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.1, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 0.51, 'colsample_bytree': 0.5, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56              0.50    5.0  0.553430  \n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.75, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58              0.25    5.0  0.601372  \n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 10}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "30        5.0          47.0          0.099               0.49   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "30              0.50    5.0  0.546131  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.5, 'gamma': 0.1}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "31        5.0          47.0          0.099               1.00   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "31              1.00    5.0  0.549812  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61 rows x 7 columns]\n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [4, 6, 2, 2, 2, 2]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 4, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "32        5.0          47.0          0.099               1.00   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "32              0.10    5.0  0.229203  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "\n",
      "[62 rows x 7 columns]\n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "33        5.0          47.0          0.099               1.00   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "33              0.50   20.0  0.576045  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63 rows x 7 columns]\n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "34        5.0          47.0          0.099               1.00   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "34              0.50    0.0  0.576045  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "\n",
      "[64 rows x 7 columns]\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "35        8.0          57.0          0.099               1.00   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "35              0.50    5.0  0.591890  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65 rows x 7 columns]\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "36        8.0          47.0          0.101               1.00   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "36              0.50    5.0  0.595450  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "\n",
      "[66 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 47, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "37        8.0          47.0          0.097               1.00   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "37              0.50    5.0  0.593986  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67 rows x 7 columns]\n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [8, 12, 4, 4, 4, 4]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 3, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "38        8.0          47.0          0.099               0.49   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "38              0.50    5.0  0.534193  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "\n",
      "[68 rows x 7 columns]\n",
      "Dimension:  2\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "39        8.0          47.0          0.099               1.00   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "39              1.00    5.0  0.521339  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69 rows x 7 columns]\n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "40        8.0          47.0          0.099               1.00   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "40              0.10    5.0  0.234621  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "\n",
      "[70 rows x 7 columns]\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "41        8.0          47.0          0.099               1.00   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "41              0.50   20.0  0.593296  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71 rows x 7 columns]\n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "42        8.0          47.0          0.099               1.00   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "42              0.50    0.0  0.593296  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "\n",
      "[72 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "43        3.0          47.0          0.101               1.00   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "43              0.50    5.0  0.582562  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73 rows x 7 columns]\n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "44        8.0          57.0          0.101               1.00   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "44              0.50    5.0  0.592783  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "\n",
      "[74 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 53, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "45        8.0          47.0          0.101               0.49   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "45              0.50    5.0  0.541648  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75 rows x 7 columns]\n",
      " \n",
      " \n",
      "Best vector so far (xb): {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "multiplying delta by 2:\n",
      "Current Delta values for each dimension:  [16, 24, 8, 8, 8, 8]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 3, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "46        8.0          47.0          0.101               1.00   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "46              1.00    5.0  0.510440  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "\n",
      "[76 rows x 7 columns]\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.097, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "47        8.0          47.0          0.101               1.00   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "47              0.10    5.0  0.237382  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77 rows x 7 columns]\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 0.49, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "48        8.0          47.0          0.101               1.00   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "48              0.50   20.0  0.595450  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "\n",
      "[78 rows x 7 columns]\n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "49        8.0          47.0          0.101               1.00   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "49              0.50    0.0  0.595450  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.1, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "50        4.0          47.0          0.101               1.00   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "50              0.50    5.0  0.584368  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "\n",
      "[80 rows x 7 columns]\n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 20}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "51        8.0          53.0          0.101               1.00   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "51              0.50    5.0  0.595018  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "52        8.0          47.0          0.101               0.50   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "52              0.50    5.0  0.553430  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "\n",
      "[82 rows x 7 columns]\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [8, 12, 4, 4, 4, 4]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [4, 6, 2, 2, 2, 2]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 4, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "53        6.0          47.0          0.101               1.00   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "53              0.50    5.0  0.581707  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83 rows x 7 columns]\n",
      "Dimension:  2\n",
      "-1 {'max_depth': 8, 'n_estimators': 50, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "54        8.0          50.0          0.101               1.00   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "54              0.50    5.0  0.594732  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "\n",
      "[84 rows x 7 columns]\n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.099, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "55        8.0          47.0          0.100               1.00   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "55              0.50    5.0  0.593702  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85 rows x 7 columns]\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 0.5, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "56        8.0          47.0          0.101               0.51   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "56              0.50    5.0  0.553430  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "\n",
      "[86 rows x 7 columns]\n",
      "Dimension:  5\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.75, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "57        8.0          47.0          0.101               1.00   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "57              0.75    5.0  0.569316  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87 rows x 7 columns]\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [2, 3, 1, 1, 1, 1]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 6, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "58        8.0          47.0          0.101               1.00   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "58              0.25    5.0  0.601372  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "\n",
      "[88 rows x 7 columns]\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.1, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "59        8.0          47.0          0.101               1.00   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "59              0.50   10.0  0.595450  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89 rows x 7 columns]\n",
      "Dimension:  4\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 0.51, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "60        8.0          47.0          0.101               1.00   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "60              0.50    0.1  0.595450  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "\n",
      "[90 rows x 7 columns]\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      "1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 10}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "61        4.0          47.0          0.101               1.00   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "90        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "61              0.25    5.0  0.567646  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "90              0.25   10.0  0.610089  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91 rows x 7 columns]\n",
      "-1 {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0.1}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "62        8.0          53.0          0.101               1.00   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "90        8.0          57.0          0.101               1.00   \n",
      "91        8.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "62              0.25    5.0  0.605145  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "90              0.25   10.0  0.610089  \n",
      "91              0.25    0.1  0.610089  \n",
      "\n",
      "[92 rows x 7 columns]\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [1, 2, 1, 1, 1, 1]\n",
      "Dimension:  1\n",
      "-1 {'max_depth': 7, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "63        8.0          47.0          0.099               1.00   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "90        8.0          57.0          0.101               1.00   \n",
      "91        8.0          57.0          0.101               1.00   \n",
      "92        7.0          57.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "63              0.25    5.0  0.598215  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "90              0.25   10.0  0.610089  \n",
      "91              0.25    0.1  0.610089  \n",
      "92              0.25    5.0  0.591961  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93 rows x 7 columns]\n",
      "Dimension:  2\n",
      "-1 {'max_depth': 8, 'n_estimators': 54, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "64        8.0          47.0          0.101               0.50   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "90        8.0          57.0          0.101               1.00   \n",
      "91        8.0          57.0          0.101               1.00   \n",
      "92        7.0          57.0          0.101               1.00   \n",
      "93        8.0          54.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "64              0.25    5.0  0.557896  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "90              0.25   10.0  0.610089  \n",
      "91              0.25    0.1  0.610089  \n",
      "92              0.25    5.0  0.591961  \n",
      "93              0.25    5.0  0.606547  \n",
      "\n",
      "[94 rows x 7 columns]\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "-1 {'max_depth': 8, 'n_estimators': 55, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "    max_depth  n_estimators  learning_rate  colsample_bylevel  \\\n",
      "0         5.0          51.0          0.099               0.50   \n",
      "1         7.0          51.0          0.099               0.50   \n",
      "2         3.0          51.0          0.099               0.50   \n",
      "3         5.0          54.0          0.099               0.50   \n",
      "4         5.0          48.0          0.099               0.50   \n",
      "5         5.0          51.0          0.100               0.50   \n",
      "6         5.0          51.0          0.098               0.50   \n",
      "7         5.0          51.0          0.099               0.51   \n",
      "8         5.0          51.0          0.099               0.49   \n",
      "9         5.0          51.0          0.099               0.50   \n",
      "10        5.0          51.0          0.099               0.50   \n",
      "11        5.0          51.0          0.099               0.50   \n",
      "12        5.0          51.0          0.099               0.50   \n",
      "13        8.0          48.0          0.099               0.50   \n",
      "14        3.0          48.0          0.099               0.50   \n",
      "15        5.0          47.0          0.099               0.50   \n",
      "16        5.0          48.0          0.101               0.50   \n",
      "17        5.0          48.0          0.097               0.50   \n",
      "18        5.0          48.0          0.099               1.00   \n",
      "19        5.0          48.0          0.099               0.49   \n",
      "20        5.0          48.0          0.099               0.50   \n",
      "21        5.0          48.0          0.099               0.50   \n",
      "22        5.0          48.0          0.099               0.50   \n",
      "23        5.0          48.0          0.099               0.50   \n",
      "24        8.0          47.0          0.099               1.00   \n",
      "25        3.0          47.0          0.099               1.00   \n",
      "26        5.0          57.0          0.099               1.00   \n",
      "27        5.0          47.0          0.099               1.00   \n",
      "28        5.0          47.0          0.101               1.00   \n",
      "29        5.0          47.0          0.097               1.00   \n",
      "..        ...           ...            ...                ...   \n",
      "65        8.0          47.0          0.101               1.00   \n",
      "66        8.0          47.0          0.101               1.00   \n",
      "67        3.0          53.0          0.101               1.00   \n",
      "68        8.0          57.0          0.101               1.00   \n",
      "69        8.0          53.0          0.097               1.00   \n",
      "70        8.0          53.0          0.101               0.49   \n",
      "71        8.0          53.0          0.101               1.00   \n",
      "72        8.0          53.0          0.101               1.00   \n",
      "73        8.0          53.0          0.101               1.00   \n",
      "74        8.0          53.0          0.101               1.00   \n",
      "75        3.0          57.0          0.101               1.00   \n",
      "76        8.0          57.0          0.097               1.00   \n",
      "77        8.0          57.0          0.101               0.49   \n",
      "78        8.0          57.0          0.101               1.00   \n",
      "79        8.0          57.0          0.101               1.00   \n",
      "80        8.0          57.0          0.101               1.00   \n",
      "81        8.0          57.0          0.101               1.00   \n",
      "82        4.0          57.0          0.101               1.00   \n",
      "83        8.0          50.0          0.101               1.00   \n",
      "84        8.0          57.0          0.099               1.00   \n",
      "85        8.0          57.0          0.101               0.50   \n",
      "86        8.0          57.0          0.101               1.00   \n",
      "87        6.0          57.0          0.101               1.00   \n",
      "88        8.0          57.0          0.100               1.00   \n",
      "89        8.0          57.0          0.101               0.51   \n",
      "90        8.0          57.0          0.101               1.00   \n",
      "91        8.0          57.0          0.101               1.00   \n",
      "92        7.0          57.0          0.101               1.00   \n",
      "93        8.0          54.0          0.101               1.00   \n",
      "94        8.0          55.0          0.101               1.00   \n",
      "\n",
      "    colsample_bytree  gamma     score  \n",
      "0               0.50    5.0  0.559353  \n",
      "1               0.50    5.0  0.528280  \n",
      "2               0.50    5.0  0.548709  \n",
      "3               0.50    5.0  0.557150  \n",
      "4               0.50    5.0  0.561159  \n",
      "5               0.50    5.0  0.554233  \n",
      "6               0.50    5.0  0.548879  \n",
      "7               0.50    5.0  0.559353  \n",
      "8               0.50    5.0  0.544236  \n",
      "9               0.75    5.0  0.543442  \n",
      "10              0.25    5.0  0.551622  \n",
      "11              0.50   10.0  0.559353  \n",
      "12              0.50    0.1  0.559353  \n",
      "13              0.50    5.0  0.543876  \n",
      "14              0.50    5.0  0.551587  \n",
      "15              0.50    5.0  0.562698  \n",
      "16              0.50    5.0  0.557367  \n",
      "17              0.50    5.0  0.560823  \n",
      "18              0.50    5.0  0.575505  \n",
      "19              0.50    5.0  0.545019  \n",
      "20              1.00    5.0  0.522722  \n",
      "21              0.10    5.0  0.228387  \n",
      "22              0.50   20.0  0.561159  \n",
      "23              0.50    0.0  0.561159  \n",
      "24              0.50    5.0  0.593296  \n",
      "25              0.50    5.0  0.575677  \n",
      "26              0.50    5.0  0.566185  \n",
      "27              0.50    5.0  0.576045  \n",
      "28              0.50    5.0  0.563909  \n",
      "29              0.50    5.0  0.578359  \n",
      "..               ...    ...       ...  \n",
      "65              0.25   20.0  0.601372  \n",
      "66              0.25    0.0  0.601372  \n",
      "67              0.25    5.0  0.569072  \n",
      "68              0.25    5.0  0.610089  \n",
      "69              0.25    5.0  0.600693  \n",
      "70              0.25    5.0  0.547768  \n",
      "71              1.00    5.0  0.505799  \n",
      "72              0.10    5.0  0.253616  \n",
      "73              0.25   20.0  0.605145  \n",
      "74              0.25    0.0  0.605145  \n",
      "75              0.25    5.0  0.572235  \n",
      "76              0.25    5.0  0.605986  \n",
      "77              0.25    5.0  0.549587  \n",
      "78              1.00    5.0  0.502783  \n",
      "79              0.10    5.0  0.304320  \n",
      "80              0.25   20.0  0.610089  \n",
      "81              0.25    0.0  0.610089  \n",
      "82              0.25    5.0  0.579669  \n",
      "83              0.25    5.0  0.603269  \n",
      "84              0.25    5.0  0.607658  \n",
      "85              0.25    5.0  0.555027  \n",
      "86              0.75    5.0  0.568056  \n",
      "87              0.25    5.0  0.586537  \n",
      "88              0.25    5.0  0.608763  \n",
      "89              0.25    5.0  0.555027  \n",
      "90              0.25   10.0  0.610089  \n",
      "91              0.25    0.1  0.610089  \n",
      "92              0.25    5.0  0.591961  \n",
      "93              0.25    5.0  0.606547  \n",
      "94              0.25    5.0  0.607816  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95 rows x 7 columns]\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1]\n",
      "Dimension:  1\n",
      "Dimension:  2\n",
      "Dimension:  3\n",
      "Dimension:  4\n",
      "Dimension:  5\n",
      "Dimension:  6\n",
      " \n",
      " \n",
      "Dividing delta by 2:\n",
      "Current Delta values for each dimension:  [1, 1, 1, 1, 1, 1]\n",
      " \n",
      " \n",
      "Best Parameters Found:\n",
      "max_depth             8.000000\n",
      "n_estimators         57.000000\n",
      "learning_rate         0.101000\n",
      "colsample_bylevel     1.000000\n",
      "colsample_bytree      0.250000\n",
      "gamma                 5.000000\n",
      "score                 0.610089\n",
      "Name: 68, dtype: float64\n",
      " \n",
      " \n",
      "PatternSearchCV took 750.85 seconds \n",
      "Model with rank: 1\n",
      "Mean validation score: 0.610 (std: 0.051)\n",
      "Parameters: {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 5}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.610 (std: 0.051)\n",
      "Parameters: {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 20}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.610 (std: 0.051)\n",
      "Parameters: {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 10}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.610 (std: 0.051)\n",
      "Parameters: {'max_depth': 8, 'n_estimators': 57, 'learning_rate': 0.101, 'colsample_bylevel': 1, 'colsample_bytree': 0.25, 'gamma': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "\n",
    "# build model\n",
    "# Mdl = ExtraTreesRegressor(n_estimators=240, \n",
    "#                                  max_features = int(X.columns.size - 15),\n",
    "#                                  max_depth = 16,\n",
    "#                                  n_jobs=-1, \n",
    "#                                  random_state=0)\n",
    "param_dist={'max_depth':[3,4,5,6,7,8],\n",
    "            'n_estimators': [47, 48, 49, 50,51,52,53,54,55,57],\n",
    "            'learning_rate': [0.097,0.098, 0.099,0.1,0.101],\n",
    "            'colsample_bylevel': [0.49,0.5,0.51,1],\n",
    "            'colsample_bytree': [0.1,0.25,0.5,0.75,1],\n",
    "            'gamma': [0,0.1,5,10,20]\n",
    "           } \n",
    "\n",
    "Mdl=xgb.XGBRegressor(random_state=0, n_jobs=-1)\n",
    "#PatternSearch(param_dist, 20, Mdl=Mdl, alfa=1.45, metric=6, verbose=False)\n",
    "PatternSearchMatLab(param_dist,100,Mdl=Mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatternSearchMatLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T03:54:58.057845Z",
     "start_time": "2019-05-14T03:49:01.681462Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      " \n",
      " \n",
      "{'n_estimators': 240, 'max_features': 2, 'max_depth': 17}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 34.411968\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1 {'n_estimators': 250, 'max_features': 2, 'max_depth': 17}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 33.190898\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "-1 {'n_estimators': 230, 'max_features': 2, 'max_depth': 17}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 31.665811\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "1 {'n_estimators': 240, 'max_features': 3, 'max_depth': 17}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 33.954942\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "1 {'n_estimators': 240, 'max_features': 2, 'max_depth': 30}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 58.419342\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "4         240.0           2.0       30.0  0.654425\n",
      "-1 {'n_estimators': 240, 'max_features': 2, 'max_depth': 16}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 29.122665\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "4         240.0           2.0       30.0  0.654425\n",
      "5         240.0           2.0       16.0  0.680345\n",
      "[240.   4.  17.]\n",
      "Pattern move point {'n_estimators': 240, 'max_features': 3, 'max_depth': 17}  has been evaluated\n",
      "[1, 1, 1]\n",
      "1 {'n_estimators': 250, 'max_features': 2, 'max_depth': 16}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 30.621752\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "4         240.0           2.0       30.0  0.654425\n",
      "5         240.0           2.0       16.0  0.680345\n",
      "6         250.0           2.0       16.0  0.680327\n",
      "-1 {'n_estimators': 230, 'max_features': 2, 'max_depth': 16}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 29.693699\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "4         240.0           2.0       30.0  0.654425\n",
      "5         240.0           2.0       16.0  0.680345\n",
      "6         250.0           2.0       16.0  0.680327\n",
      "7         230.0           2.0       16.0  0.680028\n",
      "1 {'n_estimators': 240, 'max_features': 3, 'max_depth': 16}\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 31.423798\n",
      "   n_estimators  max_features  max_depth     score\n",
      "0         240.0           2.0       17.0  0.679917\n",
      "1         250.0           2.0       17.0  0.679335\n",
      "2         230.0           2.0       17.0  0.679552\n",
      "3         240.0           3.0       17.0  0.673869\n",
      "4         240.0           2.0       30.0  0.654425\n",
      "5         240.0           2.0       16.0  0.680345\n",
      "6         250.0           2.0       16.0  0.680327\n",
      "7         230.0           2.0       16.0  0.680028\n",
      "8         240.0           3.0       16.0  0.671156\n",
      "[240.   4.  17.]\n",
      "Pattern move point {'n_estimators': 240, 'max_features': 3, 'max_depth': 17}  has been evaluated\n",
      "[1, 1, 1]\n",
      "[240.   4.  17.]\n",
      "Pattern move point {'n_estimators': 240, 'max_features': 3, 'max_depth': 17}  has been evaluated\n",
      "[1, 1, 1]\n",
      "Best Parameters Found:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n_estimators    240.000000\n",
       "max_features      2.000000\n",
       "max_depth        16.000000\n",
       "score             0.680345\n",
       "Name: 5, dtype: float64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pattern Serch old version:\n",
    "\n",
    "#len(Space)\n",
    "#print(list(Space.keys())[0])\n",
    "#list(Space.values())[0].midpoint\n",
    "\n",
    "#print(list(Space.values())[0].min)\n",
    "#print(list(Space.values())[0].midpoint)\n",
    "#print(list(Space.values())[0].max)\n",
    "#print(list(Space.values())[0].midptidx)\n",
    "print(list(Space.values())[0].Delta,list(Space.values())[1].Delta,list(Space.values())[2].Delta)\n",
    "print(\" \");print(\" \")\n",
    "\n",
    "xc={}\n",
    "#builds the first exploratory point by collecting the midpoint of each dimension:\n",
    "for Dkey, Dval in Space.items():\n",
    "    xc[Dkey]=Dval.midpoint\n",
    "print(xc)    \n",
    "BestScore,_,_=CrossEval(PercentToUse,xc)\n",
    "cols=list(xc.keys())\n",
    "cols.append('score')\n",
    "df=pd.DataFrame(columns=cols)\n",
    "xc.update({'score':BestScore})\n",
    "df=df.append(xc, ignore_index=True)\n",
    "print(df)\n",
    "\n",
    "Ndimensions=len(Space);  \n",
    "BestIdx=0; InitialExploration=0\n",
    "\n",
    "# i=exploratory moves iterations; k=Overall Iterations\n",
    "i=0; Continue=0\n",
    "\n",
    "while Continue<3:\n",
    "    #Exploratory Search:\n",
    "    while i < Ndimensions:\n",
    "        k+=1\n",
    "        for Direction in [1,-1]:\n",
    "            xn={}; xd={}\n",
    "            for CurDim in range(0,Ndimensions): #Build the vextor xn:\n",
    "                if i == CurDim:\n",
    "                    NewIndex=list(Space.values())[CurDim].CurrIndex + Direction*list(Space.values())[CurDim].Delta\n",
    "                    #print(NewIndex)\n",
    "                    if NewIndex>len(list(Space.values())[CurDim].value)-1: NewIndex=len(list(Space.values())[CurDim].value)-1\n",
    "                    if NewIndex<0: NewIndex=0\n",
    "                    xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].value[NewIndex]\n",
    "                else:\n",
    "                    xn[list(Space.keys())[CurDim]]=list(Space.values())[CurDim].BestValue\n",
    "            if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "                #print(\"Vetor\",xn,\" nao esta em df. Executando Random Forest\")\n",
    "                print(Direction, xn)            \n",
    "                CurrScore,_,_=CrossEval(PercentToUse,xn)\n",
    "                #cols=list(xn.keys())\n",
    "                #cols.append('score')\n",
    "                xd=xn.copy()\n",
    "                xd.update({'score':CurrScore})\n",
    "                df=df.append(xd, ignore_index=True)\n",
    "                print(df)\n",
    "                #print(CurrScore)\n",
    "                if CurrScore > BestScore: \n",
    "                    BestScore=CurrScore\n",
    "                    list(Space.values())[i].BestValue=list(Space.values())[i].value[NewIndex]\n",
    "                    list(Space.values())[i].BestIndex=NewIndex\n",
    "                    xc=xn.copy()\n",
    "                    BestIdx=k\n",
    "                    break\n",
    "        list(Space.values())[i].CurrIndex=list(Space.values())[i].BestIndex \n",
    "        i+=1\n",
    "\n",
    "    #xc={}\n",
    "    #for Dkey, Dval in Space.items():\n",
    "    #    xc[Dkey]=Dval.BestValue\n",
    "    #print(xc)\n",
    "\n",
    "    #pattern move:\n",
    "    pm=df.values[BestIdx]+(df.values[BestIdx]-df.values[InitialExploration])\n",
    "    pm=pm[0:(len(pm)-1)]\n",
    "    print(pm)\n",
    "\n",
    "    #picks the closest elements in the lists to the ideal point\n",
    "    n=0\n",
    "    for Dkey, Dval in Space.items():\n",
    "        xn[Dkey]=min(Dval.value, key=lambda x:abs(x-pm[n])) \n",
    "        n+=1\n",
    "\n",
    "    #Evaluates pattern move it it has not been evaluated already:\n",
    "    if list(xn.values()) not in df.drop(['score'], axis=1).values.tolist():\n",
    "        print(xn)\n",
    "        print(\"Executing Pattern Move...\")\n",
    "        k+=1\n",
    "        CurrScore,_,_=CrossEval(PercentToUse,xn)\n",
    "        xd=xn.copy()\n",
    "        xd.update({'score':CurrScore})\n",
    "        df=df.append(xd, ignore_index=True)\n",
    "        print(df)\n",
    "        if CurrScore > BestScore:\n",
    "            BestScore=CurrScore\n",
    "            xc=xn.copy()\n",
    "            for CurDim in range(0,Ndimensions):\n",
    "                list(Space.values())[CurDim].BestIndex=list(Space.values())[CurDim].value.index(list(xc.values())[CurDim])\n",
    "                list(Space.values())[CurDim].CurrIndex=list(Space.values())[CurDim].BestIndex\n",
    "                list(Space.values())[CurDim].BestValue=list(xc.values())[CurDim]\n",
    "            BestIdx=k\n",
    "            InitialExploration=BestIdx\n",
    "            BestScore=CurrScore\n",
    "        else:\n",
    "            #divide delta by 2\n",
    "            DeltaVector=[]\n",
    "            for CurDim in range(0,Ndimensions):\n",
    "                list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "                DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "    else:\n",
    "        #divide delta by 2\n",
    "        print(\"Pattern move point\",xn,\" has been evaluated\")\n",
    "        DeltaVector=[]\n",
    "        for CurDim in range(0,Ndimensions):\n",
    "            list(Space.values())[CurDim].Delta=int(0.5+list(Space.values())[CurDim].Delta/alfa)\n",
    "            if list(Space.values())[CurDim].Delta==0: list(Space.values())[CurDim].Delta=1\n",
    "            DeltaVector.append(list(Space.values())[CurDim].Delta)\n",
    "\n",
    "    print(DeltaVector)\n",
    "    i=0 \n",
    "    if DeltaVector==list(np.ones(3)): Continue+=1\n",
    "\n",
    "print(\"Best Parameters Found:\")\n",
    "df.loc[df['score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T06:43:29.324458Z",
     "start_time": "2019-05-06T06:43:29.308858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 240, 'max_features': 2, 'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "pm=df.values[k]+(df.values[k]-df.values[k-i])\n",
    "pm=pm[0:3]\n",
    "n=0\n",
    "for Dkey, Dval in Space.items():\n",
    "    xc[Dkey]=min(Dval.value, key=lambda x:abs(x-pm[n])) #picks the closes elements in the lists to the ideal point\n",
    "    n+=1\n",
    "print(xc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomize Search:\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load data\n",
    "X, y = trainDataset_X, trainDataset_y\n",
    "#del(trainDataset_X, trainDataset_y)\n",
    "\n",
    "# build model\n",
    "n_iter_search = 20\n",
    "clf = GradientBoostingRegressor(verbose=1,\n",
    "                                loss=\"ls\",\n",
    "                                n_iter_no_change=3,\n",
    "                                tol=0.1,\n",
    "                                random_state=0)\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"n_estimators\": [31,37], #sp_randint(37),\n",
    "              \"max_depth\": [4,5],\n",
    "              \"max_features\": [12],\n",
    "              \"min_samples_split\": [5,10],\n",
    "              \"min_samples_leaf\" : [1,2]\n",
    "              }\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "print(\"Using Time Series Cross Validation\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "#scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "\n",
    "\n",
    "# run randomized search\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=tscv, n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKopt Baysian Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T12:35:59.964969Z",
     "start_time": "2019-05-15T12:28:10.390111Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240, 3, 30]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 63.298620\n",
      "0.6458001773754825\n",
      "[250, 3, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 36.172069\n",
      "0.6741501541783825\n",
      "[230, 2, 16]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 26.676526\n",
      "0.6800284818335245\n",
      "[240, 3, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 32.522860\n",
      "0.6738686885201622\n",
      "[240, 3, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 33.090893\n",
      "0.6738686885201622\n",
      "[240, 2, 30]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 54.812135\n",
      "0.6544249628704046\n",
      "[230, 3, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 32.401853\n",
      "0.6729350987370989\n",
      "[250, 3, 30]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 58.195328\n",
      "0.6467656594479332\n",
      "[250, 3, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 34.974000\n",
      "0.6741501541783825\n",
      "[250, 2, 17]\n",
      "Training data set shape: (100553, 17)\n",
      "Cross Validation Time: 32.701870\n",
      "0.6793347665121312\n"
     ]
    }
   ],
   "source": [
    "PercentToUse=100\n",
    "def objective(params):\n",
    "    print(params)\n",
    "    keys=['n_estimators','max_features','max_depth']\n",
    "    xc=dict(zip(keys,params))\n",
    "    Score,EV,_,_,R2=CrossEval(PercentToUse, xc)\n",
    "    print(R2)\n",
    "    return -R2\n",
    "\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# Space=[[230,240,250],  #'n_estimators'\n",
    "#        [2,3], #'max_features'\n",
    "#        [16,17,30]] #'max_depth'\n",
    "\n",
    "Space=[[10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260],\n",
    "       [2,3,4],\n",
    "       [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]]\n",
    "\n",
    "#gp_minimize requires a minimum of 10 function calls\n",
    "r=gp_minimize(objective, Space, n_calls=15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T15:08:25.382414Z",
     "start_time": "2019-05-14T15:08:25.376414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230, 2, 16]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.6800284818335245"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(r.x)\n",
    "r.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T12:36:00.030973Z",
     "start_time": "2019-05-15T12:36:00.024973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.64580018, -0.67415015, -0.68002848, -0.67386869, -0.67386869,\n",
       "       -0.65442496, -0.6729351 , -0.64676566, -0.67415015, -0.67933477])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.func_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T12:46:10.771906Z",
     "start_time": "2019-05-15T12:46:10.501890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bcaea58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEYCAYAAACHoivJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXFWd///XO+mkIQmBrJ0ACYEkkKDDYppNQUNYvoILi7jiGGZkwGUGUVGiuCDKiMOM4/hzA0EJIwMo+zioCZEeYBQlrIHsCQmEJJ2FbJ2Q/fP7495OVzfV6a50Vd3q7vfz8ahH3Xvq3FufOkp/cu499xxFBGZmZuXQI+sAzMys+3DSMTOzsnHSMTOzsnHSMTOzsnHSMTOzsnHSMTOzsnHSMbMOkzRKUkiqyjoWq2xOOtblSfqYpJmSGiStkPQ7SadmHVd3JelaSb/KOg7LhpOOdWmSvgD8APhnoAYYCfwEOC/LuHK5d2DdiZOOdVmSDgSuAz4bEfdFxOaI2BER/x0RX0rrVEv6gaTl6esHkqrTzyZKWibpi5JWpb2kv0s/O1nSSkk9c77vAkkvpNs9JE2RtEjSWkm/ljQw/azxUtQnJb0C/DEt/4SkpWn9r0taIunMAs43WdIrktZIuiYnrp6Svpoeu0nS05JGpJ+NkzRd0uuS5kn60F7as07SdyX9VdIGSQ82xpCn7sGSHkrPu1DSP6Tl7wa+Cnw47Xk+v0//41qn5aRjXdkpwH7A/Xupcw1wMnAccCxwIvC1nM+HAQcChwCfBH4saUBEPAlsBibl1P0Y8F/p9hXA+cC7gIOBdcCPW3z3u4DxwP+TdDRJD+xiYHjOdzZqz/lOBY4CzgC+IWl8Wv4F4KPAuUB/4O+BLZL6AtPTmIemdX4i6S2tthZ8Ij3+YGAn8MNW6t0JLEvrXQT8s6QzIuL3JL3OuyOiX0Qcu5fvsq4oIvzyq0u+SP6Ar2yjziLg3Jz9/wcsSbcnAm8AVTmfrwJOTre/A/wi3T6AJAkdlu7PAc7IOW44sAOoAkYBARyR8/k3gDtz9vsA24EzCzjfoTmf/xX4SLo9Dzgvz2//MPB4i7KbgG+20lZ1wA05+0enMfbMiaEKGAHsAg7Iqftd4LZ0+1rgV1n//8OvbF6+lmxd2VpgsKSqiNjZSp2DgaU5+0vTsj3naHHsFqBfuv1fwJ8kfRq4EHgmIhrPdRhwv6TdOcfuIrmv1OjVFnHs2Y+ILZLW5nzenvOtbCXOESTJtaXDgJMkrc8pqwL+M0/dfDEvBXoBg1vUORh4PSI2tahbu5fzWjfhy2vWlf0Z2EpyWao1y0n++DYamZa1KSJmk/wxPYfml9Yg+eN8TkQclPPaLyJeyz1FzvYK4NDGHUn7A4MKPF9rXgVGt1L+vy3O2S8iPr2Xc43I2R5J0tta06LOcmCgpANa1G2M1VPbd2NOOtZlRcQGkstWP5Z0vqQ+knpJOkfSv6TV7gS+JmmIpMFp/UKG8/4Xyf2WdwK/ySn/GXC9pMMA0vPvbcTcPcD7JL1dUm/gW4A6cL5ctwDfljRWiWMkDQJ+Cxwp6W/Tdukl6YSce0H5fFzS0ZL6kAzSuCciduVWiIhXgT8B35W0n6RjSO6H3ZFWqQdGSfLfn27I/6NblxYR3ye5kf41YDXJv+7/EXggrfIdYCbwAjALeCYta687Se79/DEicv/F/x/AQ8A0SZuAJ4GT9hLnS8A/AXeR9Ho2kdw/2rYv52vh+8CvgWnARuBWYP/08tfZwEdIeicrge8B1Xs5138Ct6V19yNJuPl8lOQ+z3KSgRzfjIjp6WeNyXmtpGfa+Rusi1CEe7pmlUZSP2A9MDYiXs46HkiGTJMMALgl61is83JPx6xCSHpfegmwL/CvJD2vJdlGZVZcTjpmleM8kstRy4GxJEOefSnCuhRfXjMzs7JxT8fMzMrGD4e2MHjw4Bg1alTWYXTI5s2b6du3b9ZhVAy3R3NujyZui+Y60h5PP/30mogY0lY9J50WRo0axcyZM7MOo0Pq6uqYOHFi1mFUDLdHc26PJm6L5jrSHpKWtl2rAi6vSRqYznK7IH0f0Eq9kZKmSZojabakUS0+//8kNeTsXyJptaTn0telpf0lZmbWlsyTDjAFmBERY4EZ6X4+twM3RsR4kpmAVzV+IKkWOCjPMXdHxHHpy88WmJllrBKSznnA1HR7KnnmyUqnfa9qfKI5IhoiYkv6WU/gRuDL5QnXzMz2VSUknZqIWAGQvg/NU+dIYL2k+yQ9K+lGNS2e9Y/AQ43naOEDkl6QdE/jolVmZpadsjynI+kRksWwWroGmBoRB+XUXRcRze7rSLqIZL6o44FXgLuBh4HfkcwpNTEidkpqiIh+6TGDgIaI2CbpU8CHIiJ3wa3c818GXAZQU1Mz4a677urYD85YQ0MD/fr1a7tiN+H2aM7t0cRt0VxH2uP0009/OiLaXL4i84dDJc0jSRorJA0H6iLiqBZ1TiZZPGpiuv+3JKs9PkySjLamVUcCiyNiTIvje5Ks73FgW/HU1tZGoaPXpj02m5vueIJVazcydFB/Lr/4VM5+59EFnaOYPCKnObdHc26PJm6L5jo4eq1dSacSLq89BExOtycDD+ap8xQwQFLjGPBJwOyI+J+IGBYRoyJiFLClMeGkCazR+0lWXiy6aY/N5ns/m0b9mo1EQP2ajXzvZ9OY9tjsUnydmVmnVglJ5wbgLEkLgLPSfSTVSroFIF2v4ypghqRZJOuM/LyN814h6SVJz5NMv35JKYK/6Y4n2Lat+aKU27bt5KY7nijF15mZdWqZPxwaEWuBM/KUzwQuzdmfDhzTxrn65Wx/BfhK8SLNb9XajQWVm5l1Z5XQ0+nUhg7qX1C5mVl35qTTQZdffCrV1c07jNW9q7j84lMzisjMrHJlfnmts2scpXbTHY9Tv2YTAJ/6+GmZjl4zM6tU7ukUwdnvPJp7b7qcE449DIChg31pzcwsHyedIho3Onn+de7ClRlHYmZWmZx0imj8mCTpzHHSMTPLy0mniBqTztxFK8l6pgczs0rkpFNEQwYdwKABfWnYvI1lK9ZnHY6ZWcVx0imyPZfYFvkSm5lZS046RTZujAcTmJm1xkmnyBpHsHkwgZnZmznpFNm40TUAzF9cz85duzOOxsyssjjpFNlB/fswfOiBbNu+k6XL1mYdjplZRXHSKQE/r2Nmlp+TTgl4MIGZWX5OOiXgYdNmZvk56ZTAUUfUIMGipavZvmNn2weYmXUTTjol0Gf/3hx2yCB27tzNwiWrsw7HzKxiOOmUyLgxydBpDyYwM2vipFMi473MgZnZm2SedCQNlDRd0oL0fUAr9UZKmiZpjqTZkkal5bdJelnSc+nruLRckn4oaaGkFyS9rXy/CsaNGQ4kM06bmVki86QDTAFmRMRYYEa6n8/twI0RMR44EViV89mXIuK49PVcWnYOMDZ9XQb8tCTRt2LMqCH07NmDpa+9zpY3tpfzq83MKlYlJJ3zgKnp9lTg/JYVJB0NVEXEdICIaIiILe047+2ReBI4SNLwIsa9V9W9qxh92BB27w7mL64v19eamVW0qqwDAGoiYgVARKyQNDRPnSOB9ZLuAw4HHgGmRMSu9PPrJX2DtKcUEduAQ4BXc86xLC1b0fLkki4j6Q1RU1NDXV1dUX7YgX2Sudd++4f/Y/3qYUU5Z3s0NDQU7Td0BW6P5tweTdwWzZWjPcqSdCQ9AuT7q3tNO09RBZwGHA+8AtwNXALcCnwFWAn0Bm4GrgauA5TnPHmX84yIm9Njqa2tjYkTJ7YzrL1r2DmLp178AzvoS7HO2R51dXVl/b5K5/Zozu3RxG3RXDnaoyxJJyLObO0zSfWShqe9nOE0v1fTaBnwbEQsTo95ADgZuLWxlwRsk/RL4KqcY0bknONQYHkHf0pBxnkONjOzZirhns5DwOR0ezLwYJ46TwEDJA1J9ycBswEa79NIEsn9oBdzzvuJdBTbycCGnARVFqNGDKK6dxXL6zewYdMb5fxqM7OKVAlJ5wbgLEkLgLPSfSTVSroFIL13cxUwQ9IskktnP0+PvyMtmwUMBr6Tlj8MLAYWpnU/U56f06SqZw+OPDy5ReXndczMKmAgQUSsBc7IUz4TuDRnfzpwTJ56k1o5bwCfLV6k+2bcmGHMmrecuYvqOen4w7MOx8wsU5XQ0+nSxnuZAzOzPZx0SmyclzkwM9vDSafEDh02gH59qlnzegNrXm/IOhwzs0w56ZRYjx7iqNGecdrMDJx0ymK8n9cxMwOcdMpinJc5MDMDnHTKonEwwdxFK0lGcpuZdU9OOmVQM/gABhzYh40NW1levyHrcMzMMuOkUwaSmp7X8dBpM+vGnHTKxJN/mpk56ZTNeA8mMDNz0imXxp7OvMX17Nq1O+NozMyy4aRTJgMO7MOwIf15Y+sOlr72etbhmJllwkmnjMalMxP4EpuZdVdOOmXkyT/NrLtz0imj8WOGA+7pmFn35aRTRkcdkVxeW7hkNTt27Mo4GjOz8nPSKaN+fasZefBAduzcxaJXVmcdjplZ2TnplJlXEjWz7sxJp8w8M4GZdWeZJx1JAyVNl7QgfR/QSr2RkqZJmiNptqRRafltkl6W9Fz6Oi4tnyhpQ075N8r3q1rnno6ZdWeZJx1gCjAjIsYCM9L9fG4HboyI8cCJwKqcz74UEcelr+dyyh/PKb+uJNEXaMyoIfTsIV5etpY3tm7POhwzs7KqhKRzHjA13Z4KnN+ygqSjgaqImA4QEQ0RsaV8IRbPftW9OHzkYHbvDua/vKrtA8zMuhC1d1ExSR8Efh8RmyR9DXgb8J2IeKZDAUjrI+KgnP11ETGgRZ3zgUuB7cDhwCPAlIjYJek24BRgG2lPKSK2SZoI3AssA5YDV0XES63EcBlwGUBNTc2Eu+66qyM/qU33z1jC07PXcM6ph/KO44cV/fwNDQ3069ev6OftrNwezbk9mrgtmutIe5x++ulPR0RtmxUjol0v4IX0/VTgcZIeyl/aeewjwIt5XucB61vUXZfn+IuADcARQBVJMvlk+tlwQEA1SU/pG2l5f6Bfun0usKA9sU6YMCFK7YE/PBfvuPDGuPbff1uS8z/66KMlOW9n5fZozu3RxG3RXEfaA5gZ7fgbW8jltcanGd8D/DQiHgR6t+fAiDgzIt6a5/UgUC9pOED6nu+a0zLg2YhYHBE7gQdIelpExIr0N28Dfklyv4eI2BgRDen2w0AvSYML+L0lM94j2Mysmyok6bwm6Wbgw8DDkqoLPL41DwGT0+3JwIN56jwFDJA0JN2fBMyGPYkKSSK5H/Riuj8sLUPSiWmsa4sQb4cdMXIwvXtXsWzFOjZt3pp1OGZmZVNI0vgg8Dvg7IhYDwwAripCDDcAZ0laAJyV7iOpVtItABGxK/2uGZJmkVxO+3l6/B1p2SxgMPCdtPwi4EVJzwM/BD6SdgEzV1XVk7Gjkvw5d2F9xtGYmZVPVVsVJG0CGv9YC4jGDkRa3r8jAUTEWuCMPOUzSQYPNO5PB47JU29SK+f9EfCjjsRWSuPHDOOl+SuYu2glJxx7WNbhmJmVRZtJJyIOKEcg3c1Ro31fx8y6n0p4Tqdb8swEZtYdFXJ5TXk+jojo0OW17mrkwQPps39vVq3dxNp1mxk0oG/WIZmZlVybPZ2IOCAi+qfvLV9OOPuoRw/tWV/Hl9jMrLso6PKapAGSTpT0zsZXqQLrDhovsc3z8tVm1k20eXmtkaRLgc8BhwLPAScDfyZ5Zsb2gZc5MLPuppCezueAE4ClEXE6cDzg5S87YM9ggkUrqZBHiMzMSqqQpLM1IrYCSKqOiLnAUaUJq3sYNqQ/B/Xfn/Ub32Dl6o1Zh2NmVnKFJJ1lkg4imfdsuqQHSWZvtn0kiaNGezCBmXUf7U46EXFBRKyPiGuBrwO3kmftGyvM+NF+XsfMuo92DyTIFRH/W+xAuqtxY4YD7umYWffQ7p6OpKnp5bXG/QGSflGasLqPPcOmF9eze7cHE5hZ11bIPZ1j0tmlAYiIdSQj2KwDBg3oy9BBB7Dlje28uvz1rMMxMyupQpJOD0l7lpGWNJB9vDxnzfl5HTPrLgpJOv8G/EnStyVdB/wJ+JfShNW95D6vY2bWlbW7pxIRt0uaSTIDgYALI2J2ySLrRsZ5mQMz6yYKujyWJhknmiJrfFZnwZLV7Ny5i6qqnhlHZGZWGl5PpwL077cfhw47iO3bd7L4lTVZh2NmVjJOOhXCgwnMrDso5DmdSZJulfRvkv5O0gRJ1aUMrjvxYAIz6w4K6en8Cvgt8CRwBPAN4KWOBiBpoKTpkhak7wNaqTdS0jRJcyTNljQqLZek6yXNTz+7Iqf8h5IWSnpB0ts6GmspjduTdOozjsTMrHQKGUiwMCLuT7d/U8QYpgAzIuIGSVPS/avz1LsduD4ipkvqB+xOyy8BRgDjImK3pKFp+TnA2PR1EvDT9L0iHXn4UHr0EIuXrmbbth1UV/fKOiQzs6IrpKfzv5I+L0lFjuE8YGq6PZU8k4hKOhqoiojpABHREBFb0o8/DVwXEbvTz1blnPf2SDwJHCRpeJFjL5r99+vN4YcOYtfuYMESL1NkZl1TIT2dtwBvBa6W9DTJ6qHPRURHez01EbECICJW5PRUch0JrJd0H3A48AgwJSJ2AaOBD0u6gGRRuSsiYgFwCPBqzjmWpWUrWp5c0mXAZQA1NTXU1dV18CftmwP7Je///fvHWbOiZp/P09DQkNlvqERuj+bcHk3cFs2Voz0KeTj0QgBJ+9OUgE6iHZfaJD0CDMvz0TXt/Poq4DSSud5eAe4muax2K1BNssBcraQLgV+kdfP1yPLOqBkRNwM3A9TW1sbEiRPbGVZxrdv6HM/MfoSdHEBHYqirq+vQ8V2N26M5t0cTt0Vz5WiPgudOi4g3gJnpq73HnNnaZ5LqJQ1PeznDgVV5qi0Dno2IxekxDwAnkySdZcC9ab37gV/mHDMi5xyHUuGLzo33sGkz6+Iq4Tmdh4DJ6fZk4ME8dZ4CBkgaku5PomlmhAfSfYB3AfNzzvuJdBTbycCGxst4lWr0YUPoVdWTV5a/TsPmbVmHY2ZWdJWQdG4AzpK0ADgr3UdSraRbANJ7N1cBMyTNIrl09vOc4z+Qln8XuDQtfxhYDCxM636mPD9n3/Xq1ZMxo5K8Om+xh06bWdfTrstr6Yi1QyPi1TYrFygi1gJn5CmfSVMCIR25dkyeeuuB9+QpD+CzRQ22DMaNGcachSuZs3AFE/5mZNbhmJkVVbt6Oukf8AdKHIsB40f7IVEz67oKubz2pKQTShaJATkzE3gwgZl1QYUkndNJEs+idFqZWZJeKFVg3dVhhwxk//16sXL1RtZt2NL2AWZmnUghQ6bPKVkUtkfPnj048vAanp+zjLkLV3LKhCOyDsnMrGgK6em8QvLQ5eSIWEryoOW+PzZvrRo3JmnWOZ5x2sy6mEKSzk+AU4CPpvubgB8XPSJj/Jhkijjf1zGzrqaQy2snRcTbJD0LEBHrJPUuUVzdWu7MBBFB8edYNTPLRiE9nR2SepLOX5bODrB774fYvji45kD699uPdRu2UL9mU9bhmJkVTSFJ54ckc5sNlXQ98ATJDABWZJIYlz6vM8/3dcysC2l30omIO4AvkySaFcD5EfHrUgXW3Y3z5J9m1gW1+56OpO9FxNXA3DxlVmSecdrMuqJCLq+dlafMz+6UyLjRybDpeYvq2b077zJAZmadTptJR9Kn0xmcj0pnImh8vQx4RoISGTLoAAYN6EvDlm0sW7ku63DMzIqiPZfXzgXeC8wD3pdTvikiXi9JVAYkl9ieeGoRcxeuZOTBA7MOx8ysw9pzeW10+j4P2EjyUOgmAEn+S1hCHkxgZl1Ne3o6PwN+DxwOPE2ygFqjADw5WInsWebAScfMuog2ezoR8cOIGA/8MiKOiIjDc15OOCXU2NOZ//Iqdu7yc7hm1vm1e8h0RHxa0gBgLLBfTvljpQjM4MAD9ufgmgNZXr+BJa+u3bOUtZlZZ9XuIdOSLgUeA/4AfCt9v7Y0YVmj8V7Uzcy6kEKe0/kccAKwNCJOB44HVpckKtvjqPS+jpc5MLOuoJCkszUitgJIqo6IucBRHQ1A0kBJ0yUtSN8HtFJvpKRpkuZImi1pVFouSddLmp9+dkVaPlHSBknPpa9vdDTWLHhmAjPrSgpZ2mCZpIOAB4DpktYBy4sQwxRgRkTcIGlKup9vap3bgesjYrqkfjTNcH0JMAIYFxG7JQ3NOebxiHhvEWLMzFFH1CDBoqWr2bZ9J9W9C/mfzMysshQy4ecFEbE+Iq4Fvg7cCpxfhBjOA6am21PznVPS0UBVRExPY2mIiC3px58GrouI3elnq4oQU8Xos39vDjtkELt27WbhEl/NNLPOTRHZzuslaX1EHJSzvy4iBrSocz5wKbCd5HmhR4ApEbFL0lrg+8AFJPeYroiIBZImAvcCy0h6ZFdFxEutxHAZcBlATU3NhLvuuqvIv7Jj7p3+Ms/OXct73zmSk48d2mb9hoYG+vXrV4bIOge3R3NujyZui+Y60h6nn3760xFR21a9slyrkfQIMCzPR9e08xRVwGkkgxdeAe4muax2K1BNcr+pVtKFwC/Sus8Ah0VEg6RzSS4Ljs138oi4GbgZoLa2NiZOnNjOsMpj7RsH8uzcGezs0Y/2xFZXV9euet2F26M5t0cTt0Vz5WiPsiSdiDiztc8k1UsaHhErJA0H8l0eWwY8GxGL02MeAE4mSTrLSHo0kCwy98v0OzfmfP/Dkn4iaXBErCnKjyqjpmHT9RlHYmbWMYWMXgNAUt902epieQiYnG5PBh7MU+cpYEC6RDbAJGB2uv1Aug/wLmB+GucwSUq3TyT5rWuLGHfZjBk1hKqqHix9bS1b3tiedThmZvusPUsb9JD0MUn/I2kVySJuKyS9JOlGSXkvWRXgBuAsSQtI1uy5If3eWkm3AETELuAqYEa6zIKAn+cc/4G0/Lsk934ALgJelPQ8yVLbH4msb2Dto969qhh92BAiYN5i93bMrPNqz+W1R0lu3H8FeLFxlFg6w/TpwA2S7o+IX+1LABGxFjgjT/lMmhII6ci1Y/LUWw+8J0/5j4Af7UtMlWjc6GHMW1TPnIUrOf4tI7IOx8xsn7Qn6ZwZETtaFqZr6dwL3CupV9Ejs2bGjx7Ggzzv6XDMrFNrzyzTOwAk/aDxHklrdax0vLaOmXUFhQwkaAAektQXQNLZkv6vNGFZS6NGDKK6dxUrVm1g/cYtbR9gZlaBCpmR4GvAnUCdpCeAL5JMWWNlUNWzB0ceUQPA3EUeTGBmnVMhSxucAfwDsBkYQvLk/+OlCszebM/zOp5x2sw6qUIur10DfD0iJpIMR75b0qS9H2LFNM5r65hZJ1fIyqGTcrZnSTqHZPTa20sRmL3Z+NEeTGBmnVt7Hg5tbcTaCtLna1qrY8V1yLCD6NenmrXrNrN67aaswzEzK1h7Lq89KumfJI3MLZTUGzhF0lSaprGxEurRQxw1OhlM4N6OmXVG7Uk67wZ2AXdKWp6u2rkYWAB8FPj3iLithDFaDq8kamadWXvu6XwvIj4n6TZgBzAYeCOdfsbKzCPYzKwza09Pp3FetMcjYkdErHDCyc6eEWyL6umk85eaWTfWnqTze0l/BoZJ+ntJEyTtV+rALL+hgw5g4EF92NSwleX1G7IOx8ysIO2Ze+0q4GKS+zqHA18HZqVLG9xd4visBUmM89BpM+uk2vWcTkQslnRmRMxvLJPUD3hrySKzVo0fM4w/Pb2YOQtXcOap47IOx8ys3QpZrnqppI8Bo1oc92RRI7I2NfZ0vHy1mXU2hSSdB4ENwNPAttKEY+3ROJhg/sv17Nq1m549C1513MwsE4UknUMj4t0li8TabcCBfRg2pD8rV29k6WtrOWLkkKxDMjNrl0L+ifwnSX9TskisIF7Uzcw6o0KSzqnA05LmSXpB0ixJL5QqMNs7z0xgZp1RIZfXzilFAJIGAneTDFBYAnwoItblqTcSuAUYAQRwbkQskfQ4cEBabSjw14g4P52E9D+Ac4EtwCUR8UwpfkMWxo/2Mgdm1vkUsnLo0nyvIsQwBZgREWOBGbS+GuntwI0RMR44EViVxnVaRBwXEccBfwbuS+ufA4xNX5cBPy1CrBXjqNE1SLBw6Wq279iZdThmZu3SnqUNnkjfN0namL43vjYWIYbzgKnp9lTg/DwxHA1URcR0gIhoiIgtLeocAEwCHsg57+2ReBI4SNLwIsRbEfr2qWbkwQPZuXM3i5euyTocM7N2afPyWkScmr4f0FbdfVSTrs1DRKyQNDRPnSOB9ZLuI5kV4RFgSkTsyqlzAUmPqTERHgK8mvP5srRsRcuTS7qMpDdETU0NdXV1HftFZXJQP1gKPPi7x1j5N03N1tDQ0Gl+Qzm4PZpzezRxWzRXjvZo9z0dSbXAV2nxcGhEHNOOYx8BhuX56Jp2fn0VcBpwPPAKyT2gS4Bbc+p8lOSez56vzXOevDNkRsTNwM0AtbW1MXHixHaGla3Vm/vz/Lw/sqtHf3Jjrquro7P8hnJwezTn9mjitmiuHO1RyECCO4AvAbOA3YV8SUSc2dpnkuolDU97OcNJ79W0sAx4NiIWp8c8AJxMmnQkDSK5z3NBi2NG5OwfCiwvJO5Kt2eZAw8mMLNOopAh06sj4qGIeLnIAwkeomnl0ckkMx+09BQwQFLjU5CTgNk5n38Q+G1EbG1x3k8ocTKwofEyXlcxdtQQevbswZJla3lj6/aswzEza1MhSeebkm6R9FFJFza+ihDDDcBZkhYAZ6X7SKqVdAtAeu/mKmCGpFkkl85+nnOOjwB3tjjvw8BiYGFa9zNFiLWiVFf34oiRg9m9O5i/OF8H0cysshRyee3vgHFAL5ourwVNQ5T3SUSspWmhuNzymcClOfvTgbz3jyJiYp6yAD7bkdg6g3Gjh7Hg5VXMWbiSY48+NOtwzMz2qpCkc2xEeBqcCjN+zDD++5EXvHy1mXUKhVxeezJ9XsYqiAcTmFlnUkhP51RgsqSXSZY2EMlVrDaHTFvpHD5iEL17V7Fs5Xo2Nmylfz+vJG5RIuEcAAAP2UlEQVRmlauQpONlDSpQVVVPxo4awkvzVzBv0UpOOHZU1iGZmbWqEuZesw7yjNNm1ll4yckuoHH5aicdM6t0TjpdgAcTmFln4aTTBYw4eCB9+/Rm9esNrFnXkHU4ZmatctLpAnr0EEcdUQPA3IX1GUdjZtY6J50uYtyeS2xdano5M+tinHS6iD2DCTwzgZlVMCedLqJpMEE9ybRzZmaVx0mnixg2pD8H9d+fDZveYN1GL3NgZpXJSaeLkLTnEttrqzZnHI2ZWX5OOl1I4yW21+qddMysMjnpdCFbtu0A4Iln6/nA5Tcz7bHZbRxhZlZeTjpdxLTHZvPA75/bs1+/ZiPf+9k0Jx4zqyhOOl3ETXc8wbbtO5uVbdu2k5vueCKjiMzM3sxJp4tYtXZjQeVmZllw0ukihg7qn7d8wIF9yhyJmVnrMk86kgZKmi5pQfo+oJV6IyVNkzRH0mxJo9LyxyU9l76WS3ogLZ8oaUPOZ98o368qv8svPpXq6jevybepYSuz5r6WQURmZm+WedIBpgAzImIsMCPdz+d24MaIGA+cCKwCiIjTIuK4iDgO+DNwX84xjzd+FhHXle4nZO/sdx7N1Z86m5rBSY+nZvABHD12ODt27uaL37mXF+ctzzhCM7PKSDrnAVPT7anA+S0rSDoaqIqI6QAR0RARW1rUOQCYBDxQ2nAr19nvPJp7b7qM7/xTLffedDk/uf6jnPGOcWx5Yztf+PY9TjxmljllPU+XpPURcVDO/rqIGNCizvnApcB24HDgEWBKROzKqfMJ4P0RcVG6PxG4F1gGLAeuioiXWonhMuAygJqamgl33XVX8X5gBhoaGujXrx8Au3YH90xbzKwF66ju1YNLzj+SEcP6ZRxheeW2h7k9crktmutIe5x++ulPR0RtmxUjouQvkiTxYp7XecD6FnXX5Tn+ImADcARQRZJMPtmizu+AD+Ts9wf6pdvnAgvaE+uECROis3v00Ueb7e/YuSu+/q8PxTsuvDHO/vh/xIvzlmcTWEZatkd35/Zo4rZoriPtAcyMdvyNLcvltYg4MyLemuf1IFAvaThA+r4qzymWAc9GxOKI2ElyCe1tjR9KGkRyn+d/cr5zY0Q0pNsPA70kDS7Zj6xgVT178I0r38Oktx/F5i3b+cK3f8NL873ujpmVXyXc03kImJxuTwYezFPnKWCApCHp/iQg91H7DwK/jYitjQWShklSun0iyW9dW+TYO43GxHP6KUfuSTyzFzjxmFl5VULSuQE4S9IC4Kx0H0m1km4BiOTezVXADEmzAAE/zznHR4A7W5z3IuBFSc8DPwQ+knYBu62qnj345pXvYWJj4rnuHuZ4pVEzK6M3P9hRZhGxFjgjT/lMksEDjfvTgWNaOcfEPGU/An5UtEC7iKqqnlx75Xu4NoK6Jxfw+W/dw79/8yLGjxmedWhm1g1UQk/HyqyqqifXfv69TDx5LA1btvH5b93D3IVe5trMSs9Jp5tqTDzvOilJPFde9xvmLnLiMbPSctLpxqqqevKtL7yXd540lobN27jyW048ZlZaTjrdXFVVT771+fdy2oljaNi8jc9fdw/zFtdnHZaZdVFOOkavXj257gvv47QTxrCpYStXfus3TjxmVhJOOgakieeLzRPPfCceMysyJx3bozHxnHrC6D2JZ8HL+SaIMDPbN0461kyvXj359hffzztqR7OxYSufu/bXTjxmVjROOvYmvXr15NtXvY+3TziiKfEsceIxs45z0rG8eveq4jtfev+exHPltb9x4jGzDnPSsVblJp4Nm97gymt/w8Ilq7MOy8w6MScd26vGxHPK2w5PEs+3fs2ipU48ZrZvnHSsTUniOY9T3nY46ze+weeudeIxs33jpGPtUt07STwnH9+UeBa/4sRjZoVx0rF2q+5dxfVfPo+Tjh/F+o1vcMU3nXjMrDBOOlaQ6t5V/POXz2+ReNZkHZaZdRJOOlawxsRz4nGj9lxqe/lVJx4za5uTju2T6t5VfPfq8znx2FGs27CFz137a5YsW5t1WGZW4Zx0bJ8liec8Tjj2MF5fv4Urvnm3E4+Z7ZWTjnVIdXUvbrj6/GaJZ6kTj5m1QhGRdQxIGgjcDYwClgAfioh1eeqNBG4BRgABnBsRSySdAdxIkkQbgEsiYqGkauB2YAKwFvhwRCzZWyy1tbUxc+bMIv2ybNTV1TFx4sSyfue2bTu4+oYHmPnCUvru35v99+vF2vWbGTqoP5dffCpnv/PossYDMO2x2dx0xxPUr9lIzeDs41i1dmO3bw+3Rf44ukJ7SHo6ImrbrFchSedfgNcj4gZJU4ABEXF1nnp1wPURMV1SP2B3RGyRNB84LyLmSPoMcGJEXJJuHxMRn5L0EeCCiPjw3mJx0tl3W7ft4NIv/ydLlr3erLy6dxX/OHkiE085smyx1P15Pj+aWse27TsdR4XEUQkxOI52xlFdxdWfOrugxNPZks48YGJErJA0HKiLiKNa1DkauDkiTm3l+E9ExF8kfQU4ICK+KukPwLUR8WdJVcBKYEjs5Uc76XTMhZffxKo1mzL5bjMrnprB/bn3psvaXb+9SaeqQ1EVT01ErABIE8/QPHWOBNZLug84HHgEmBIRu4BLgYclvQFsBE5OjzkEeDU9705JG4BBQLPxvZIuAy4DqKmpoa6ursg/r7waGhoy+w17Szh99ivf/922bN3Z6meOI5s4KiEGx9H+OOrXbCzJ35Gy/TJJjwDD8nx0TTtPUQWcBhwPvEJyD+gS4Fbg8yT3d/4i6UvA90kSkfKc5029nIi4GbgZkp5OVr2EYsmyp1Nz53zq12x8c3mB/2rqqA9cfrPjqLA4KiEGx1FYHKX4O1K20WsRcWZEvDXP60GgPr2sRvqeb+GWZcCzEbE4InYCDwBvkzQEODYi/pLWuxt4e84xI9LzVgEHAs1vOFhRXX7xqVRXN/+3THV1FZdf/Karoo6jm8VRCTE4juzjqJTLaw8Bk4Eb0vcH89R5ChggaUhErAYmATOBdcCBko6MiPnAWcCcFuf9M3AR8Me93c+xjmu88Zj1iJzcOLIcoeT2yB9Dd2+LlnF0q/aIiMxfJPdZZgAL0veBaXktcEtOvbOAF4BZwG1A77T8grTseaAOOCIt3w/4DbAQ+Gtj+d5eEyZMiM7u0UcfzTqEiuL2aM7t0cRt0VxH2gOYGe34e18RPZ2IWAuckad8Jsm9mcb96cAxeerdD9yfp3wr8MGiBmtmZvvMMxKYmVnZOOmYmVnZOOmYmVnZOOmYmVnZVMQ0OJVE0mpgadZxdNBgWsy60M25PZpzezRxWzTXkfY4LCKGtFXJSacLkjQz2jEHUnfh9mjO7dHEbdFcOdrDl9fMzKxsnHTMzKxsnHS6ppuzDqDCuD2ac3s0cVs0V/L28D0dMzMrG/d0zMysbJx0zMysbJx0uhBJIyQ9KmmOpJckfS7rmLImqaekZyX9NutYsibpIEn3SJqb/n/klKxjypKkz6f/nbwo6U5J+2UdUzlJ+oWkVZJezCkbKGm6pAXp+4Bif6+TTteyE/hiRIwnWbL7s5LKuzhH5fkcTesrdXf/Afw+IsYBx9KN20XSIcAVQG1EvBXoCXwk26jK7jbg3S3KpgAzImIsyTIzU4r9pU46XUhErIiIZ9LtTSR/VA7JNqrsSDoUeA9wS9axZE1Sf+CdJMu7ExHbI2J9tlFlrgrYP11VuA+wPON4yioiHuPNKymfB0xNt6cC5xf7e510uihJo4Djgb/svWaX9gPgy8DurAOpAEcAq4Ffppcbb5HUN+ugshIRrwH/CrwCrAA2RMS0bKOqCDURsQKSf8QCQ4v9BU46XZCkfsC9wJURsTHreLIg6b3Aqoh4OutYKkQV8DbgpxFxPLCZElw66SzSexXnAYcDBwN9JX0826i6ByedLkZSL5KEc0dE3Jd1PBl6B/B+SUuAu4BJkn6VbUiZWgYsi4jGnu89JEmouzoTeDkiVkfEDuA+4O0Zx1QJ6iUNB0jfVxX7C5x0uhBJIrlmPycivp91PFmKiK9ExKERMYrkBvEfI6Lb/ks2IlYCr0o6Ki06A5idYUhZewU4WVKf9L+bM+jGAytyPARMTrcnAw8W+wuqin1Cy9Q7gL8FZkl6Li37akQ8nGFMVjn+CbhDUm9gMfB3GceTmYj4i6R7gGdIRn0+SzebEkfSncBEYLCkZcA3gRuAX0v6JEli/mDRv9fT4JiZWbn48pqZmZWNk46ZmZWNk46ZmZWNk46ZmZWNk46ZmZWNk46ZmZWNk46ZmZWNk451e5JC0r/l7F8l6doinHdU7lolpSTpinSNnDs6eJ6GfNtmxeKkYwbbgAslDc46kFxKtPe/0c8A50bExaWMyayjnHTMkmlQbgY+n1vYsqfS2ANKy+emywO8KOkOSWdK+r90xcUTc05TJWmqpBfSVTv7pOf6uKS/SnpO0k2SeuZ85xxJPyGZomVEi5i+kH7ni5KuTMt+RrJ0wUOSmv2G9PNPpN//vKT/TMsekPR0unLmZXtrHEl9Jf1PevyLkj6cp879kr4j6XFJKyWdubdzWvflpGOW+DFwsaQD21l/DMlKnMcA44CPAacCVwFfzal3FHBzRBwDbAQ+I2k88GHgHRFxHLALuLjFMbdHxPERsbSxUNIEkvnSTiJZGfYfJB0fEZ8iWYDs9Ij499wgJb0FuAaYFBHHkqykCvD3ETEBqAWukDRoL7/13cDyiDg2XWXz93nqvBVYHxGnkfS63OOyvJx0zIB03aHbSZYwbo+XI2JWROwGXiJZ4jeAWcConHqvRsT/pdu/IklMZwATgKfSiVnPIOmpNFoaEU/m+c5TgfsjYnNENJBMx39aG3FOAu6JiDXp72xcKfIKSc8DT5L0psbu5RyzgDMlfU/SaRGxIffDtPd2INCY8KqA7r4qqbXCs0ybNfkBySWtX6b7O2n+D7P9cra35WzvztnfTfP/rlrOqBuAgKkR8ZVW4tjcSrlaKd8btYxB0kSS9WROiYgtkupo/tuaiYj5aS/rXOC7kqZFxHU5Vd4CPB0Ru9L9Y4CyDKCwzsc9HbNU2gv4NfDJtKgeGCppkKRq4L37cNqRkk5Jtz8KPAHMAC6SNBRA0kBJh7XjXI8B56drwPQFLgAeb+OYGcCHGi+fSRpI0itZlyaccSSX6lol6WBgS0T8imSJ55aLv70VeC5n/xjghXb8HuuG3NMxa+7fgH8EiIgdkq4D/gK8DMzdh/PNASZLuglYQLJc9BZJXwOmpaPTdgCfBZbu5TxExDOSbgP+mhbdEhHPtnHMS5KuB/5X0i6SdWMuBz4l6QVgHskltr35G+BGSbvTWD+d5/O/5Oy/Ffd0rBVeT8fMzMrGl9fMzKxsnHTMzKxsnHTMzKxsnHTMzKxsnHTMzKxsnHTMzKxsnHTMzKxs/n+bL5UZHKYO5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(r) #, yscale=\"log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T15:09:34.480366Z",
     "start_time": "2019-05-14T15:09:33.212294Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "230.25641025641025",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-2cc7da021b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_objective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\plots.py\u001b[0m in \u001b[0;36mplot_objective\u001b[1;34m(result, levels, n_points, n_samples, size, zscale, dimensions)\u001b[0m\n\u001b[0;32m    332\u001b[0m                                             \u001b[0mj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m                                             \u001b[0msample_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrvs_transformed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m                                             n_points=n_points)\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\plots.py\u001b[0m in \u001b[0;36mpartial_dependence\u001b[1;34m(space, model, i, j, sample_points, n_samples, n_points)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# XXX use linspace(*bounds, n_points) after python2 support ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mxi_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\space\\space.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;34m\"\"\"Transform samples form the original space to a warped space.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\space\\transformers.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mhot\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\skopt\\space\\transformers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mone\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mhot\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 230.25641025641025"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skopt.plots import plot_objective\n",
    "plot_objective(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn.feature_selection RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T18:25:59.169852Z",
     "start_time": "2019-05-17T18:24:13.465807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Measured Fit Time:  2.8331620693206787\n",
      " \n",
      " \n",
      "Model Parameters: \n",
      "{'bootstrap': False, 'criterion': 'mse', 'max_depth': 15, 'max_features': 2, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 250, 'n_jobs': -1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
      " \n",
      " \n",
      "Feature ranking:\n",
      "1. feature 6 IsOpen (0.380287)\n",
      "2. feature 4 HasPromotions (0.155310)\n",
      "3. feature 2 Day of week (number) (0.104303)\n",
      "4. feature 13 StoreID (0.045440)\n",
      "5. feature 11 Region_GDP (0.043561)\n",
      "6. feature 14 StoreType (0.042415)\n",
      "7. feature 5 IsHoliday (0.039981)\n",
      "8. feature 8 NearestCompetitor (0.039229)\n",
      "9. feature 0 AssortmentType (0.034189)\n",
      "10. feature 10 Region_AreaKM2 (0.023997)\n",
      "11. feature 12 Region_PopulationK (0.022469)\n",
      "12. feature 9 Region (0.019379)\n",
      "13. feature 3 Day of year (0.013333)\n",
      "14. feature 1 Day of month (0.012330)\n",
      "15. feature 15 Week (0.011289)\n",
      "16. feature 7 Month (number) (0.009551)\n",
      "17. feature 16 Year (0.002939)\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Optimal number of features : 17\n",
      "Cross Validation scores:\n",
      "[0.50902468 0.51332164 0.52919985 0.53358259 0.5450329  0.55599046\n",
      " 0.55423666 0.57229944 0.57475315 0.57465382 0.58177904 0.58092576\n",
      " 0.58548121]\n",
      "                 Feature  Importance  Rank\n",
      "0                 IsOpen    0.380287     1\n",
      "1          HasPromotions    0.155310     2\n",
      "2   Day of week (number)    0.104303     3\n",
      "3                StoreID    0.045440     4\n",
      "4             Region_GDP    0.043561     5\n",
      "5              StoreType    0.042415     6\n",
      "6              IsHoliday    0.039981     7\n",
      "7      NearestCompetitor    0.039229     8\n",
      "8         AssortmentType    0.034189     9\n",
      "9         Region_AreaKM2    0.023997    10\n",
      "10    Region_PopulationK    0.022469    11\n",
      "11                Region    0.019379    12\n",
      "12           Day of year    0.013333    13\n",
      "13          Day of month    0.012330    14\n",
      "14                  Week    0.011289    15\n",
      "15        Month (number)    0.009551    16\n",
      "16                  Year    0.002939    17\n",
      "(100553, 17)\n",
      "(100553, 17)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAK0CAYAAAD73JvMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xu4bWVZN/7vLYiWor4qmQoKmVrkqUC0tFolFnhAM3vT1LIstSL1tSyzflqU72tWWpqamKc0z5ahUmjqtjy7SdSQSCIMohLzfCgE798fYyxZbhbstdbem2fNuT+f61rXXuOw5rzH2nOuOb7jecbzVHcHAAAARrja6AIAAADYfwmlAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlALAFlXVH1fV/ze6DgBYZGWeUgCualV1XpIbJbl0zepbdfeFe/CYK0le2t2H7ll1i6mqXpTkgu7+9dG1AMBmaCkFYJR7d/e113xtOZDuDVV14Mjn3xNVdcDoGgBgq4RSALaVqrpzVb2rqj5dVR+cW0BXt/1kVZ1VVZ+rqnOr6hHz+msl+askN6mqz89fN6mqF1XVb6/5+ZWqumDN8nlV9StV9aEkX6iqA+efe21VXVRV/1JVj7qSWr/6+KuPXVW/XFUfr6p/r6r7VtU9quqfquqTVfWENT/7G1X1mqp65Xw8f19Vt1+z/Vurasf8ezizqk7Y5XmfU1WnVtUXkjwsyYOS/PJ87K+f93t8Vf3z/PgfqaofWvMYD62qd1TV71XVp+ZjPX7N9utX1Qur6sJ5++vWbLtXVZ0x1/auqrrdmm2/UlX/Nj/n2VV1tw38twOwHxNKAdg2quqmSd6Y5LeTXD/JLyV5bVUdMu/y8ST3SnKdJD+Z5OlV9R3d/YUkxye5cAstrw9Mcs8k10vylSSvT/LBJDdNcrckj6mqH9zgY31jkmvOP/vEJM9L8uAkRyX57iRPrKpvWrP/fZK8ej7WlyV5XVVdvaquPtfxpiTfkOQXkvxZVd16zc/+WJInJzk4yZ8m+bMkT52P/d7zPv88P+91k/xmkpdW1Y3XPMadkpyd5IZJnprk+VVV87aXJPn6JN821/D0JKmq70jygiSPSHKDJM9NckpVXWOu78Qkd+zug5P8YJLzNvi7A2A/JZQCMMrr5pa2T69phXtwklO7+9Tu/kp3vznJziT3SJLufmN3/3NP3p4ptH33HtbxjO4+v7u/lOSOSQ7p7pO6++LuPjdTsHzABh/ry0me3N1fTvKKTGHvD7v7c919ZpIzk9xuzf6nd/dr5v2flinQ3nn+unaSp8x1vDXJGzIF6FV/2d3vnH9P/71eMd396u6+cN7nlUk+muSYNbt8rLuf192XJnlxkhsnudEcXI9P8sju/lR3f3n+fSfJzyR5bne/t7sv7e4XJ/mfueZLk1wjyZFVdfXuPq+7/3mDvzsA9lNCKQCj3Le7rzd/3Xded/MkP7ImrH46yV0zhaVU1fFV9Z65K+ynM4XVG+5hHeev+f7mmboAr33+J2QalGkj/msOeEnypfnf/1yz/UuZwublnru7v5LkgiQ3mb/On9et+limFtj16l5XVf34mm62n05ym3zt7+s/1jz/F+dvr53ksCSf7O5PrfOwN0/yi7v8jg5LcpPuPifJY5L8RpKPV9Urquomu6sTgP2bUArAdnJ+kpesCavX6+5rdfdTquoaSV6b5PeS3Ki7r5fk1CSr3U3XG07+C5m6oK76xnX2Wftz5yf5l12e/+DuvsceH9n6Dlv9pqquluTQJBfOX4fN61bdLMm/XUHdl1uuqptnauU9MckN5t/XP+Sy39eVOT/J9avqelew7cm7/I6+vrtfniTd/bLuvmum8NpJfmcDzwfAfkwoBWA7eWmSe1fVD1bVAVV1zXkAoUOTHJSpa+hFSS6ZB+X5gTU/+59JblBV112z7owk95gH7fnGTK14V+Z9ST47D9bzdXMNt6mqO+61I/xaR1XV/Woa+fcxmbrBvifJezMF6l+e7zFdSXLvTF2Cr8h/Jll7v+q1MoXCi5JpkKhMLaW71d3/nmngqGdX1f+aa/ieefPzkjyyqu5Uk2tV1T2r6uCqunVVff98AeG/M7UMX3oFTwMASYRSALaR7j4/0+A/T8gUps5P8rgkV+vuzyV5VJJXJflUpoF+Tlnzs/+Y5OVJzp27ld4k02A9H8w02M6bkrxyN89/aabwd4ck/5LkE0n+JNNAQfvCXyb50UzH85Ak95vv37w4yQmZ7uv8RJJnJ/nx+RivyPMz3cv56ap6XXd/JMnvJ3l3psB62yTv3ERtD8l0j+w/Zhpg6jFJ0t07M91X+kdz3eckeej8M9dI8pS55v/INEDSEwIAV6K61+vtBADsS1X1G0m+ubsfPLoWABhJSykAAADDCKUAAAAMo/suAAAAw2gpBQAAYBihFAAAgGEOHPXEN7zhDfvwww8f9fQAAADsQ6effvonuvuQ3e03LJQefvjh2blz56inBwAAYB+qqo9tZD/ddwEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSvfAyspKVlZWRpcBAACwsIRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIbZUCitquOq6uyqOqeqHr/O9odW1UVVdcb89dN7v1QAAACWzYG726GqDkjyrCR3T3JBkvdX1Snd/ZFddn1ld5+4D2oEAABgSW2kpfSYJOd097ndfXGSVyS5z74tCwAAgP3BRkLpTZOcv2b5gnndrn64qj5UVa+pqsPWe6CqenhV7ayqnRdddNEWygUAAGCZbCSU1jrrepfl1yc5vLtvl+Rvkrx4vQfq7pO7++juPvqQQw7ZXKUAAAAsnY2E0guSrG35PDTJhWt36O7/6u7/mRefl+SovVMeAAAAy2wjofT9SW5ZVUdU1UFJHpDklLU7VNWN1yyekOSsvVciAAAAy2q3o+929yVVdWKS05IckOQF3X1mVZ2UZGd3n5LkUVV1QpJLknwyyUP3Yc0AAAAsid2G0iTp7lOTnLrLuieu+f5Xk/zq3i0NAACAZbeR7rsAAACwTwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAyzoVBaVcdV1dlVdU5VPf5K9rt/VXVVHb33SgQAAGBZ7TaUVtUBSZ6V5PgkRyZ5YFUduc5+Byd5VJL37u0iAQAAWE4baSk9Jsk53X1ud1+c5BVJ7rPOfr+V5KlJ/nsv1gcAAMAS20govWmS89csXzCv+6qq+vYkh3X3G67sgarq4VW1s6p2XnTRRZsuFgAAgOWykVBa66zrr26sulqSpyf5xd09UHef3N1Hd/fRhxxyyMarBAAAYCltJJRekOSwNcuHJrlwzfLBSW6TZEdVnZfkzklOMdgRAAAAu7ORUPr+JLesqiOq6qAkD0hyyurG7v5Md9+wuw/v7sOTvCfJCd29c59UDAAAwNLYbSjt7kuSnJjktCRnJXlVd59ZVSdV1Qn7ukAAAACW14Eb2am7T01y6i7rnngF+67seVkAAADsDzbSfRcAAAD2CaEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYQ4cXcBVrmp7P2b33nssAACAbU5LKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADDMhkJpVR1XVWdX1TlV9fh1tj+yqj5cVWdU1Tuq6si9XyoAAADLZrehtKoOSPKsJMcnOTLJA9cJnS/r7tt29x2SPDXJ0/Z6pQAAACydjbSUHpPknO4+t7svTvKKJPdZu0N3f3bN4rWS9N4rEQAAgGV14Ab2uWmS89csX5DkTrvuVFU/n+SxSQ5K8v17pToAAACW2kZaSmuddZdrCe3uZ3X3LZL8SpJfX/eBqh5eVTuraudFF120uUoBAABYOhsJpRckOWzN8qFJLryS/V+R5L7rbejuk7v76O4++pBDDtl4lQAAACyljYTS9ye5ZVUdUVUHJXlAklPW7lBVt1yzeM8kH917JQIAALCsdntPaXdfUlUnJjktyQFJXtDdZ1bVSUl2dvcpSU6sqmOTfDnJp5L8xL4sGgAAgOWwkYGO0t2nJjl1l3VPXPP9o/dyXQAAAOwHNtJ9FwAAAPYJoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhjlwdAGLbMfoAgAAABacllIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKN3PraysZGVlZXQZAADAfkooBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhtlQKK2q46rq7Ko6p6oev872x1bVR6rqQ1X1lqq6+d4vFQAAgGWz21BaVQckeVaS45McmeSBVXXkLrt9IMnR3X27JK9J8tS9XSgAAADLZyMtpcckOae7z+3ui5O8Isl91u7Q3W/r7i/Oi+9JcujeLRMAAIBltJFQetMk569ZvmBed0UeluSv9qQoAAAA9g8HbmCfWmddr7tj1YOTHJ3ke69g+8OTPDxJbnazm22wRAAAAJbVRlpKL0hy2JrlQ5NcuOtOVXVskl9LckJ3/896D9TdJ3f30d199CGHHLKVegEAAFgiGwml709yy6o6oqoOSvKAJKes3aGqvj3JczMF0o/v/TIBAABYRrsNpd19SZITk5yW5Kwkr+ruM6vqpKo6Yd7td5NcO8mrq+qMqjrlCh4OAAAAvmoj95Smu09Ncuou65645vtj93JdsCkrKytJkh07dgytAwAA2JyNdN8FAACAfUIoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGEOHF0AW1C1vR+ze+89FgAAsNS0lAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADCMUAoAAMAwB44ugP1Y1fZ+zO6991gAAMC6NtRSWlXHVdXZVXVOVT1+ne3fU1V/X1WXVNX9936ZAAAALKPdhtKqOiDJs5Icn+TIJA+sqiN32e1fkzw0ycv2doEAAAAsr4103z0myTndfW6SVNUrktwnyUdWd+ju8+ZtX9kHNQIAALCkNtJ996ZJzl+zfMG8btOq6uFVtbOqdl500UVbeQgAAACWyEZC6Xojx2xpBJjuPrm7j+7uow855JCtPAQAAABLZCOh9IIkh61ZPjTJhfumHAAAAPYnGwml709yy6o6oqoOSvKAJKfs27IAAADYH+w2lHb3JUlOTHJakrOSvKq7z6yqk6rqhCSpqjtW1QVJfiTJc6vqzH1ZNAAAAMthI6PvprtPTXLqLuueuOb792fq1gsAAAAbtpHuuwAAALBPCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDBCKQAAAMMIpQAAAAwjlAIAADDMgaMLYKwdowsAAAD2a1pKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIAAACGEUoBAAAYRigFAABgGKEUAACAYYRSYK9aWVnJysrK6DIAAFgQQikAAADDCKUAAAAMI5QCAAAwjFAKsI5luDd2GY4BAFh+QilsEwIEAAD7I6EUAACAYYRSAAAAhhFKAQAAGEYoBQAAYBihFAAAgGGEUgAAAIYRSgEAABhGKAUAAGAYoRQAAIBhhFIA2MdWVlaysrIyugwA2JaEUgC2NYEOAJabUAoAAMAwQikAAADDCKUAAAAMI5QCAAAwjFAKAADAMEIpAAAAwwilAAAADCOUAgAAMMyBowuAhVe1fR+ve+89FgAA7ANaSgEAABhGSymw91t79/ZjavEFAFhaQimwPIRrAICFo/suALBfWVlZycrKyugyAJgJpQAAAAyj+y7AdrIsXZCX5TgAgH1OSykAsCG6vQKwLwilAAAADCOUAgAAMIxQCgAAwDAGOgKAK7K3B2wyWBMAXI6WUgAAAIYRSgEAABhGKAUAWDCm5wGWiVAKAADAMEIpAAAAwwilAAAADCOUAgAAMIxQCgAAwDAHji4AmOwYXQAAAAwglALAMqva3o/ZvfceC4CFJJQCsK3tGF0AALBPCaUAwPanxRdgaRnoCAAAgGGEUgAAAIbRfRcA4KqgCzLAuoRSgHXsGF0AAMB+QvddAACGWFlZycrKyugygMG0lAJ71Y7RBQDAVWg1VO/YsWNoHbDItJQCAAAwjFAKAAD7OV2pGUn3XYAltWN0AcByMoowsJcJpSyFHaMLALgSO0YXAFze3g7XgvW24B7fxSSUAgDAItJqzZJwTykAAADDaCkFADZkx+gCWDo7RhfA9qDFd78nlAIAwBbtGF0ALAGhFABgwewYXQBLZ8foAtivCaUAAMBS2DG6ALbEQEcAAAAMI5QCAABsEysrK1+db3V/saHuu1V1XJI/THJAkj/p7qfssv0aSf40yVFJ/ivJj3b3eXu3VACAPbdjdAHA8jGC8B7ZbUtpVR2Q5FlJjk9yZJIHVtWRu+z2sCSf6u5vTvL0JL+ztwsFAABYdjuy/10820j33WOSnNPd53b3xUlekeQ+u+xznyQvnr9/TZK7Ve2LywUAAAAsk410371pkvPXLF+Q5E5XtE93X1JVn0lygySfWLtTVT08ycPnxc9X1dlbKXqbuWF2Oc49MibLL8MxJMtxHMtwDInjWJ/X1J5YhuNYhmNIHMf6vKb2xDIcxzIcQ+I41uc1tSduvpGdNhJK1zuCXTslb2SfdPfJSU7ewHMujKra2d1Hj65jTyzDMSTLcRzLcAyJ49hOluEYkuU4jmU4hsRxbCfLcAzJchzHMhxD4ji2k2U4hs3YSPfdC5Ictmb50CQXXtE+VXVgkusm+eTeKBAAAIDltZFQ+v4kt6yqI6rqoCQPSHLKLvuckuQn5u/vn+St3dt8iCcAAACG22333fke0ROTnJZpSpgXdPeZVXVSkp3dfUqS5yd5SVWdk6mF9AH7suhtZhm6Iy/DMSTLcRzLcAyJ49hOluEYkuU4jmU4hsRxbCfLcAzJchzHMhxD4ji2k2U4hg0rDZoAAACMspHuuwAAALBPCKUAAAAMI5QCAAAMUlUHVNUPj65jJPeUspCq6ppJ7pXku5PcJMmXkvxDkjd295kja2PxVdU1uvt/RtfB8vCagklVHZDktO4+dnQtLJeq+l+57JzwvO7+yuCSNqWq/q67v3t0HaNoKd2CqrpVVT2vqt5UVW9d/Rpd12ZU1S2q6hrz9ytV9aiqut7oujaiqn4jybuSfFeS9yZ5bpJXJbkkyVOq6s1VdbtxFW5OVd22qn5k/rrN6Hq2oqq+oar+oKreUFX/r6quM7qmraiqY6rqw0k+Oi/fvqqeObisTauqe1XVQv99n68av3R0HXtqiV5T31JVd6uqa++y/rhRNW1GVf3EFay/elW9/KquZ08s+jlId1+a5ItVdd3RteyJqrpfVX20qj5TVZ+tqs9V1WdH17VZVXX8OuseOaKWraiq61bVE+a/s+/JZeeEH6uqV1fV942tcFNOq6rHVNWNq+o6q1+ji7qqaCndgqr6YJI/TnJ6kktX13f36cOK2qSqOiPJ0UkOzzTdzylJbt3d9xhZ10ZU1T27+41Xsv0bktysu3dehWVt2vyB/JdJDkvyoSSV5LZJ/jXJfbp7YT7cquqvM70f/jZTC/bB3f3QoUVtQVW9J8mPJnldd3/7vO4funuhLhbMYe47k7w2yQu7+6zBJW1JVZ2W5N7dffHoWrZqGV5TVfWoJD+f5Kwkd0jy6O7+y3nb33f3d4ysbyOq6u+T/HF3n7xm3bWSvC7Jv3b3w4YVt0lLcg7yqiR3TvLmJF9YXd/djxpW1CbN0yDee1H/vq6qqncl+fXufuu8/CtJVrr7cmF1O6qqNyf50ySv7+5P77LtqCQPSfLh7n7+iPo2o6rOX2d1d/fNrvJiBtjtPKWs65Lufs7oIvbQV+Y5aH8oyR909zOr6gOji9qI7n7j3P3nKd39uHW2fzzJx6/6yjbtt5LsTPL9q11M5tatpyR5cpJfGFjbZn1jd//a/P1p8wngIrpad3+sqtauu/SKdt6uuvvB89XVByZ5YVV1khcmeXl3f25sdZtyXpJ3VtUp+doT16cNq2jzluE19TNJjuruz1fV4UleU1WHd/cfZrqYtgiOTfLXVXXN7n5GVR2S5NQkb+nuxw+ubbOW4RzkjfPXIvvPRQ+ksxOSvKGqHpfkuCTfMq9bCN1995r+wB6a5NO7bDs908WbhdDdh42uYSShdGteX1U/l+Qvknz1HqHu/uS4kjbty1X1wCR1X7diAAAgAElEQVQ/keTe87qrD6xnU7r70qo6qqqqF7e5/9gkt1t7z0N3f6WqnpDkw+PK2pKa7+VYPUE9YO3yAr03zq+qY5L0fOHjF5L80+CatqS7P1tVr03ydUkek+SHkjyuqp7R3YvSffTC+etqSQ4eXMtWLcNr6oDu/nySdPd5VbWSKZjePAsSSrv7k1V1bJK/qqqbJLlPkud09zMGl7YVC38O0t0vHl3DVlXV/eZvd1bVKzO1tq/9f/jzIYVtUXd/oqpOSPI3mQLc/RftvKq7u6pel+So0bXsqar6liRHJrnm6rruftm4iq46uu9uQVX9yzqru7u/6SovZouq6sgkj0zy7u5+eVUdkeRHu/spg0vbsKr6/SS3TPLqfG0rykJ8IFTVGd19h81u246q6rwkX8n6J6gL896Yu34/I9MFg8rUtezE7v7E0MI2qaruneSnktwiyUuSvLi7P15VX5/krO6++dACN6mqDs70Ovr86Fo2axleU/P9io/t7jPWrDswyQuSPKi7DxhW3AatCRIHJ3lakrckecXq9kX53EiW5hzklkn+Xy5/8r3tj6GqXnglm7u7f+oqK2YPVNXnkqwNAQdlGpujMx3HQt3LWFXPSvKi7n7/6Fq2qqp+PckPZGqtPi3JDyZ5R3ff70p/cEkIpSysK/hgWKQPhH/M1L1y1yBXSV7a3d961VfFMqiqP03yJ939t+tsu1t3v2VAWZtW08BfL0ly/XnVJ5L8uBG2r1pVdWimLqP/sc62u3T3OweUtSnLEiSWRVW9I8mTkjw9U2+tn8x0TvqkoYVtwnqv/UV5PyyjqvpIkltnuu3jC5nOpbq7F2ngyw9num//77v79lV14yTP7e6F6U69J4TSLZhbGx6baTCdh89X/G7d3W8YXNqGVdVdkvxGkptn6sa9+ubd9lcpl0VV7cjXXqX8Gt29SCPGrbacHJ/pCl+SfCTTsP+XjKtqc+b75Z6eaZCgJHlnkl/s7vMGlbRfmwfg+LXuftu8vJLk/3b3dw0tbBOW4TVVVde/su2L1G10GVTV1ZP8bJLvmVftyHTi+uVhRW1SVZ3e3UdV1Ye7+7bzuoWaDmO9Qb4WZeCvteb7MR+U5Iju/q2qOizJjbv7fYNL25T5doLL6e6PXdW1bFVVva+7j6mq05OsJPl8pkGaFmZgvD3hntKteWGmfverJ0YXZOpCujChNMnzk/yf7DJ63yKpqlsleU6SG3X3bWqaBuaE7v7twaVtSHevjK5hb5nv0Xpbkn9P8oFMFznuleRpVfV93X3hyPo24eVJTs40WmqS/Ni87juv8Ce2oaq6c5JnJvnWTF2yDkjyhUXrjpXkWquBNEm6e8c8YuoiWYbX1Ccyfc6tXmBa27ujkyzMxcyapkL74Uwjz3/1HKi7TxpV0xY8J9MYEM+elx8yr/vpYRVt3n/PA/t9tKpOTPJvSb5hcE0bUlXfmen875CqeuyaTdfJ9Ld20Tw70+03359pAMbPJ3lWkjuOLGqz5gHl7prklt39wnkws2vv7ue2mQ/UND3jCzINhPnZJIs6cOSmaSndgqra2d1HV9UH+rIh/j/Y3bcfXdtGVdV7u/tOo+vYE1X19iSPy3SFeOGmWlhzj9O6FuwepxclOaO7/2CX9Y/KNGrnunMEbjfrvS8W8b1SVTuTPCDTxbKjk/x4km9eM0LyQqiqv8j0gfySedWDkxzd3fcdV9XmLMNrqqr+MNNV+3dmCtTvWLSBUFbVNH3VZ3L56VR+f1hRm7Te+cYCnoPcMdMUQ9fLFISuk+R3u/s9QwvbgKr63kzvh0dmmppn1ecyTUvy0RF1bdVq6+4in9MmSVU9KdPn3a27+1bzxfJXd/ddBpe2JVX1zUmu0937TSjVUro1F1fV12XuellVt8iakdcWxNuq6neT/Hm+dtS4RXrxf313v6++dqqFhekqmstGPV5PZ/q/WRR37nXmJe1p6oWzB9SzVW+tql/KNABKZ2rden3Nk1f3As0d293nVNUBPU1U/8K5K+yi+akkv5nL3gt/m+Shw6rZmoV/TXX3o+cufiuZWuWeWVVvyjR67XqD7mxnh3b3caOL2EOXVtUtuvufk6SqvikL1uNpdTCamgbQ/8nR9WxGd789ydur6kWL1DX0Sny5ppHBV89pD8nUcrpofijJt2duWezuC+dB8hZKVT0gyS26+8lVdVhVHdULNAfxnhBKt+ZJSf46yWFV9WdJ7pLFO1FavUp/9Jp1nan7xqL4xHxBYPUP6f0zdR9dCIv2QbwbX7qSbV+8yqrYcw+e/330Lusfkel1tigTWH+xqg5KckZVPTXT+2LRur0mybHd/ai1K6rqRzK1AC+KpXhNzS2jb6tpPusHZGrd+miS5w0tbPPeVVW37e5Fm3Zrrcdl+r84N1NX6ptnGihoYcxdYJ+fqXvlzarq9kke0d0/N7ayTblGVZ2cy3cFX6TzqGQaHfwvktyoqp6c5P5Jfn1sSVtycXd3TfNyZwFv9UhV/VGmrvnfk2m++i9kao1fqK7UW6X77hZV1Q2S3DnTB8J7Fml4/2UxXx0+OdO9HZ9K8i+ZpidYqCuXVXWjJP83yU26+/iapuv5zu5+/uDSNmw+Ofql9TYleWp33+IqLmm/Ng/48PFMH27/J8l1kzy7u88ZWtgmLctAIotuPrm7T6ZW3kMytVy/srvPH1rYFswjdH5zps+L/8kCjtCZfPXe2Ftnqv8fu3uhemtV1XszhZ9TFvH2m2Tq4popMOzaFXzhWrVqmhvzbpleT2/p7rMGl7Rpc4+UWya5e6bphn4qyct6ceblXpqu1FulpXTrvjfJXTNd6b56pqtMC6OqrpupxXd19L63Jzmpuz8zrqrN6e5zkxw7nzBdrbs/N7qmLXpRpsGzVu/3+6ckr8x0FXlRvD1X3B35ctOSbFdV9Z5MAwy8fIFfT2tHG/xSpu6vC6Wqjk9yjyQ3rapnrNl0nSxWF/1leU19PFOr6MuTnJPpc++O832BC3X/e6YRwhdSVX1/d791nfEIblFVi/b/kO4+f5fbbxaqC3KmaZKeM7qIveSGSb64OkBQVR2xaF3zu/v3qurumQYHulWSJ3b3mweXtVlfngcAW23tvUEWsyv1lgilW1BVz850pfXl86pHVNWx3f3zA8varBck+Yck/3tefkimYLQwE/TOb9YnZb44UNO8Zyd193+NrWzTbtjdr6qqX02S7r6kqhbqw3mJuiI/NFM3uA/O92C+sBdkTs/kq3OcXdk0Q4vSGnRhppEHT8jUCrHqc5lafhfJQ7PAr6nZqzO9rr4ll035tGqh7n+fR+i8fZLVqUf+rrs/OLKmTfjeJG/N+hcAF+r/Icn5VfVdmT67D0ryqEwDHy2S11fVz2VqlFg7NsdCTZG0doCgTOeBV0/y0ky3pi2aDydZHfNlEbvoPyvJazON7Pybmc7RF+7C8lbpvrsFVXVmktusjj44X9X4cHd/29jKNq6qzujuO+xu3XZWVW/O1Ar30nnVg5KsdPex46ravJrmK/3hJG+eu23cOcnvdPf3jq1s46rq0CSHd/c75uXH5rKh2F+2gN1GD8gUiP4oycWZLuI8s7s/PbSw3ajL5mlbvUC2OmrtgzJdBV+kaS9SVQf2As1ze2UW9TWVTD1rrqgXTVXdcXXQmkVQVY9O8jO5LMD9UJKTF6mL3zKoqhsm+cMkx2bqMvqmJI9epIvKVbVeS2L3gs33XlVnZB4gaE2X0Q8t0EXMJElV/XSSJ2a6cFOZLuKc1N0vGFrYBlTVqUl+rrvPq6pvy2Xvi7/p7n8YW91VR0vp1pydaXCK1S5yhyX50LhytuRLVXXXNSHiLrnywWq2o+t392+tWf7tqlqYqSLWeGySUzJ1wXpnpnu27j+2pE373SR/tmb5EZnu9/36TFf5HjSiqK2Y7+n9yUytEX+Z6bjumumDblvfy7jabbeq7rLLMPiPn19bCxFKq+pV3f2/M83Zdrkrpwt4srSwr6nZW6rq7t39qbUr565yL8j0GbgoHpbkTt39hSSpqt9J8u5M8/ouhPrauTFXfSbJ6d19xlVdzxZ9pbsX5nNhPd19xOga9pKFHyBo9rgk3756YWPuTfeuTH+jtrsXJXlTVb040zgcZw6uZwihdGtukOSsqnrfvHzHJO+uqlOSpLtPGFbZxv1skhfP95ZWkk9m8UYQfts8dPar5uX7J3njwHo2bW5lv2amK3qrg1ac3d1fHlrY5t26u9+wZvmLPc/7V1V/N6imTZsH3/hSpg+xJ3b36oWad84XbhbFtXa56PRdWazRd1dHqr3X0Cr2giV5TT0309/bu3f3RUlSVT+WaXTIew6tbPMqX3vv4qXzukVy9Pz1+nn5nknen+SRVfXq7n7qsMo27r1zC90Lkvz1as+zRVJVV890LrU6NseOTPOmL9rn96uq6rlJrldVP5NpgKBFG1U7SS7IdIvHqs8lWYjB2OZbuN6YqaV3Z1W9JGvuJe3upw0r7iqk++4W1DRx8hWa57BaCLVAc+WtqqrPZbpfoDKdaK++ca+W5PPdfZ1RtW1FVb27u79zdB17oqo+0t1Hrlm+/up9Nbtu246q6n7d/edVdavu/qfR9eypqjoq08nededVn07yU71Y8xCnqn6nu39ld+u2oyV8TT0kyS8n+YFMo/A+Mslx3X3eyLo2a25l/IlcNjjhfZO8qLv/YFxVm1NVpyX54e7+/Lx87SSvydQV+fTt/vc2SWoa4ejYTAHomEyD+71okd4rVfUnme6/fPG86iFJLu3unx5X1cZV1WOSvDPJB5J8X6b3diU5bZEGCFrTc+AOSW6bqTdKZxox/H3d/chRtW3GfG/145P8WKb3w9pQul/cVyqUbkFVHZHk2zK96M+aR4FdCFX14O5+6RV0/9lvrsZsJ/PN7B9K8ueLeLU4+Wpr0EN2PaGoaZj5P+3uY8ZUtjG1pNOMzBedapFG1V5rvf+XRbnXaRlfUzXNEfvMJP+a5PhFuv9vrar6jkzdpyvJ33b3BwaXtClVdVaS23f3xfPyNZKc0d3fWmumklgUVfV9mcaGuFaSDyZ5fHe/e2xVu1frTNWx3rrtqqp+L9OUet+S6RzkXZlC6rsXabCmeaCmK7QIga6qjkvytEy3cp3U3Ys0v/teo/vuJswneH+S5KhMfzgrye2r6vQkD1uQ1sbVLnwHr7Nt4QJRVd0ul5+4epFGIEyme0qvleTSqvpSLps3b5FafJ+U5A01Tby92hp3VJIn5LKumOxjV3TRqeZpFxblolNV/WySn0vyTVW19n79g/P/t3fnUXZVZfrHvw/pYGQIcyvYgiQMgiBTUBFEBdFuZZCpAWWQQRyZtPNDRRltFVARoiACRkSILdD8gBYZZEYGIRAIiiBEWkWRBBQiQSDh6T/2vqlbRVVS91bIvvve97NWreScU7XWA6l779ln7/2+6cYpLEZNVZ1F2ie+Emk5bzU9PiWNtf2MpBWBR/NX49qKNd2EAxcAt0u6NB/vAEzJewF/XS7W8OX9fnuTZhf/AhxCuiHfmFTtuYb9mvMkjbf9CNDonV5N5Xzb/wHzZ+gmkAaoBwBnSfpbDTPuUMegcxiOAnbv1b2kDTFT2gJJPyB9kB1v+6V8TsCXgLVs71suXWtyIZRfLOxcJ5P0feDNwK/oW+Zg2weUS9W7JG1AWt7XqEJ9P3ByDZXjJM0h9V982SUquekGkPQx22cO9eS4lg/vvNd9BVID9M81XZpdy+ChW36noF9V50G5ry9ux5L0P7a3V6qY2nzj0/j3qK1i6mb0zfbeYvuuwpFaIukhUnXwybb/OODakbZPLJNs+CRtS2qhMoP077AGsL/t64sGa1F+v92C1AJmC2B5UkeJqlq9SZpAGtytQf+Jimrea3tdDEpbIOm3ttdu9VonGmJZXFXLzWrYqzhcknakqVjCgKJBXUPSJNuHlM4xkFKbp/cPdb2Gm+5uJWkrYG2npu4rA8u6gqbu8TsVXkkDXherAMvU8LpokKRat6s0y0unG0UKf2P7+YX8SMeQ9D3SQ+TZwB3A7cDtHlBluxaSHiRV4J1O//2Y8V5biVi+25raKvS9jKQtSEs0VhmwxG8sMKpMqrbdJml921UsVxqKpK+RKjg3Wqocliunfm4BP1arTq02+kI3fHBJOm1B120furiyLAp6eVP3JamnqXtX/E41U+qhPAlYj/RvMQp4tqatBpKutb3tws51skFeF6Op53UBpKnp0hlGSqn38Pvo20K0raRqtkmQWhu+Cvgt8Bipem3H901egJm2LysdIrQvBqWt+YWko4ETmt9QJX2J9ISpBksCy5D+7Zv3lT5Dfb0xzyUNTB8HnqfCZXHZ+4GNm5aEn0uqhteNg9JOVc2y9YWYWjrAIrYzuak7gO0/SRpsP3wn+gWkwngDZ7AGO1eJbwN7kvb8TQD2BdYqmmiYJI0h7YddWdIK9D1kHgusVixYe2p+XXSTy4F/MGBmrha2/zVvQXsTabLis8AGkp4iFTtaYAGhDnRMroh8LemeEKiyzkjPikFpaw4BzgEeVuqvZVLj87tJDbk7nlO7mhsl/cD2/+YPMjdKy1fm+6QiCVV+IAywPKlXLPS18QiLie1PA0h6DfAVYDXb/yZpfWAL2+cUDThMts9tPq789Q0VN3Vv/E4BF5M+J5pdRCoEVh3bD0saZXseMFlSLYWnPgYcThqATqVvUPoM8J1SodpU7etCuaWTpN1tX1g6zwj9S4UPwfvJEyz3S/ob8HT+2p7Upqe2Qen+pErCo2mqMwLEoLQSMShtQa6uu7uk8cD6pA+1IxuV1yqzrKR7gBUBJM0C9quhKE2T33fJUo2vAvdIup70O7U1qWptN+r0JfA/IC2HOyofP0TqF1bFoLQhF506j/T6lqSZwL4VVvartqm7UjukNwHLSdql6dJYYEyZVCM2J1fqnCbpJODP9FV072i2TwVOlXSI7Uml84zQYK+LswtnGq73S/oi8HnSjHvNfibpvbavLh2kHZIOJc2Qbgm8SG4HQ3rgP71gtHZtZHvD0iFC+6LQURskbUnqCfaspL1JT8FPrWn/UH66fVSjSpykdwFfsf32osFaIOl00gzj5VS+VEPSqqR9pQLusP144UivCEkfsf2D0jmGIulO25urqdefpGm2Ny6drRXd8PpukLQdqak7wNWupKm7pJ2ADwI7klpdNMwGfmy7lhnG+XIV3r+QtoEcQVrVcbrtwaoMd6z80GZ9mh4O2P5huUSta3pdCLiqotfFycDBpIcZc8jbbhp/VrY/eWfSXt4lSIO6qv4bJH2T3JvU9p9L5xkpSWcBp9ReZ6SXxaC0DUp98zYitSM5jzSLsovtdxYN1gJV3vQZQNLkQU5X1xKmGwpvNEhah1T9bmBJ9m2KhWqBpBuAXYFrbG+aC7ucWNNrG7rj9d0g6bWkpWQG7qztgY2kLWzfVjrHoiLp1cDqth8snaUduUjQu0iD0iuAfyO1VKmtpsJ8ueDOnrbPX+g3dwhJl9reqXSOkZA0g/TgaXo3FG6qnaQHgPHA76i7zkjPiuW77Zmb93PsRJohPUfSfqVDtWhGLtB0Xj7em/RCrkZtPbQG6rLCGw0XAt8lLbGspol4k8+QZrXGS/oFsAr1FQCDLnh9A0g6CDgauI70+pgk6Xjb3y+brCUPS/oCfRU6Aajt4RmApB2Ar5NmSteUtDGpb/eOZZO1ZDfSQ+V7bO+f95FXsfRV0ljgU8DrSO9T1+TjicA0+iq4dzzbO+X/95vnU3fYnlkyUxt+C9wfA9KO8a+lA4SRiUFpe2ZL+jzpRm/r/JRydOFMrToAOI60AVzATaRN4tXIM6Uv+zCo6GZvYOGNhtnUV3ijYa7tM0qHaIekJUjL+d5JX9+5B22/WDRYe6p/fWcTgU1sPwkgaSXScrOaBqWXAjcDP6fOBzXNjiXNWt8AYHuapDeUi9OW52y/JGluHuQ9AYwrHWqYzgP+Str3dxDp9bEksJPtaSWDtUrS7qQHHDfQ98Bpou2LigZrzZ+BGyT9jP5biGppCdNt4uFA5WJQ2p49gA8BB9p+XNLqwMmFM7UkN0euqmfhIP6n6e9jSGXy/1QoSztuBX4C7GZ7Up5t3xV4FLigZLARuFzSJ4FL6P8h/dTQP9IZ8o3qN2xvAdRWEKifLnl9Q+qbN7vpeDbwh0JZ2rWU7SNLh1hE5tp+OnWRqNZdkpYnreaYCvwd+GXZSMM2rlHIJbe+mEVaSj17wT/Wkb4IbG77CQBJq5Ae3NQ0KP1d/loyf4Wyfkrf/uQxwJrAg6SCc6ECsae0R0maQKrw+gb6Lymrdu19nun6eUX7F+8G3mP7KUlbAz8mtR3aGFivxj1OkgZbImrbVcxESDoOuA/47xqXZEm6nAU8La5smSWSfghsSJptNLATaQDxENQxIyHpy8Cttq8onWWkJJ1D6gH4OdIDtEOB0bY/XjRYm/Is71jb9xWOMiyS7ra96VDHNZE0vblSav78vjeqp4ZFRdKmwMdsf6x0ljA8MShtgaTZDH7DV1XFNQBJD5KW/vTr8VlTBeGBJK0L/NR2Lc3c5xeekfQdYKbtY/NxdRVfu0F+jS9NWmb5HJW9tiU1CjKJNBN0UPN1pz7F1chFaYZk+7jFlaVdTb9TL+Svqn6nmklaitQuqVEN+Srgy7b/US7V8OQb1CHZvntxZWmXpHnAs41D4NU0VbCt6XcqV+F9MzAln9oDuK+LVhWEDlDzg5teFIPSHiXpFttblc4xEk0PCRol5R8HPm/74qLBhknS/cDGtudK+g1wsO2bGtdsb1A2YeskjQY+Qeq1Cmm/0JmV7susWnNbmxBGKtdO+JrtiaWztEOpD/RQXMsKm26S+/duRd73bvuSwpFCxSR9pulwCVK7xpVsv69QpNCi2FPau47Je1KupdIen7aXLZ1hhKYAN0qaRZqVuxlA0lrA0yWDjcAZpKJfp+fjffK5g4b8iQ4jaUeaBtW2/2dB39/Bqn/imLcZHMXLWwxVs81AaQPmh4E1bZ8g6fXAqrZr2ccIgO15kjYrnaNdtt9dOkPoL99vVHPP0SDpRNtHStrd9oWl84T5mu8J55L2mFYxSRGSmCntUZJ+BLyRVNClsXy3xh6fr+PlN6w3lUvUmtwHc1XgatvP5nPrAMvUsJxsoNr7Y0r6GqlFQaO1wl7AVNufK5dq+CSt2HR4Pakf4/yqNDUUnGrWDdsMJJ1Byr6N7fVy+6erbW++kB/tOJK+AaxNav3UWEZa1cNMSfsOdt72Dxd3llAnSdNJs3B3xNLQEBadmCntXRvVXlBA0omkfSi/pq/VgkntL6pg+/ZBzj1UIssiMk/SeNuPAEgaR11tMN5PWlL9EoCkc4F7SIVdajCVviXtAM0PNkw9rS8aZtq+rHSIEXqr7U0l3QOpMrKkWit1rgg8CTQvdTV1zXY1PwwYA2xLep3EoDQM15WkysdLS3qGvi1E1e3t7Sb5gf5/8PICnrE0vxIxKO1dt0ta3/avSwcZgQ8C69p+fqHfGRaXicD1kmaQPqDXoL7+mMsDjRnF5UoGaZXtNUtnWMSq32YAvJj3Yxrmt754acE/0plsv+y1LKmqGV/bhzQfS1qO1P8zLEaStgeuaDwArEneVz1R0qW2dyqdJ8x3IfBd4GzqehgeshiU9q6tgP1yC4/n6XvCV81eLWAGaf9iDEo7hO1rJa0NrEv6nfpNZQ8Nvgrck4uiiLS39AtlI7VO0pbANNvPStqbtNTsW7Z/Xzhaq/YnbTMYTdM2A+qamTuN1Lf3nyX9J7AbqUdjtSStD+xJWt7+NDChbKIRmUNakhwWrz2BUyVdDEy2/UDpQK2yvZOk19A3+36H7ZklM/W4ubbPKB0itC/2lPYoSWsMdr6yvVoXAxvx8lmUQ4uF6lGStrF9Xa6m+DI1zWxJWpV0kyHSTcbjhSO1TNJ9pNfGm0mzQOcAu9h+5wJ/sMMM7GVYK0lvJC0TFXBtjTfg+TNjr/w1l7QKYoLtR0vmatWAXr6jgPWAn9Syb7ybSBpL+n3an/RvMhmYYnt20WDDJGl34OukKvMC3gFMtH1RyVy9StKxwBOkh4DN94RV1VLoZTEo7WGSNiK9iQLcbPveknlaJWm/wc7bPndxZ+l1ko6zfYykyYNcrqaAlqRrbW+7sHOdrtGbTdLRwGO2z6mxX5uks4BTKt9mQC5u9Hr673OqppCZpFtJS9l/DPzY9m8l/a7G5eJNvXwhDa7/1/YfS+XpdZJWBvYGDgceANYCTrM9qWiwYZB0L7Cd7Sfy8SrAz2sp7Ndt8sq/gWy7tloKPSuW7/YoSYcBH6VvGdyPJH2vhg+Chhh8dg7bx+Q/a9s/CoCkMcBSwMp5ANEoFDQWWK1YsPbNlvR50s3e1nlP4+jCmdpR/TYDSScAHwEeoW+GzvQvFtTpZgL/ArwGWAX4LZW2HLJ9o6TXAm8h/Tc8UjhST5K0A3AAMJ60muMttp+QtBRpcFrDvcgSjQFp9iSpP2YooMaHZKG/mCntUXl53xZNbUiWBm6r4WYvL7/6HnCl7RcHXBtHugF81Pb3C8TraQOaVzc8TWqrMm1x5xmu/JDmcNIA9LGmS7OBs2x/u0iwNuWb7g8Bd9q+WdLqwLtqa3vRJdsMHgQ2tP1C6SwjkQsC7UpabrkWqSDY+2rrtyrpIOBo4DrSQ453AsfH58XiJemHwNmDtXCTtK3tawvEaomkk0lbJKbkU3sA99k+slyq3iNpK9u3LOD6WGB12/cvxlihDTEo7VG5z9bmtv+Rj8eQbmA7fv9WvuH+DOkG6SnSU/wxwJrAw8C3bV9aLmHvknQBqejJ5fnUB4A7ScVqLrR9UqlsC5IriP4R2M32pLw0fFfgUeDY2JNSThdsM7gY+MSAGZWqSfpn0g34XsDrbb++cKRhyw8J3m77yXy8EnCr7XXLJgs1ynUUtiI94LjJ9iWFI/UcSacAb/IQcvkAABTnSURBVCW16plK3z3hWsC7SfvfP2v7zmIhw7DEoLRH5Rmt/UgbwiG1V/mB7W+VS9U6SW8AVgWeAx6yPadooB4n6SpgV9t/z8fLABcBO5NmS9cvmW8oku4G3mP7KUlbk/bOHQJsDKxne7eiAYdJ0mwGX1ZZZf+8QbYZ7AxUtc1A0gTgUuB++hff2LFYqEVI0hqNmWtJkwa2XOk0kq4F/q0xc517xl5h+z1lk/UWSW8jLdFdD1iSVHTq2dreo0JnyNtudgO2pO+e8AHgpwuaRQ2dJQalPUzSpvR/wndP4UihcpIeADZquuF7Fak1yXqS7rG9SdmEg5N0b6M4haTvADNtH5uPp9neuGS+XlXzNoMGSb8CzgSm09Sf1PaNxUK9QmooppWXjW5IelBgYCfgl8BDALa/WS5d75B0F6ktzIWk1TX7AmvZPqposBBCMVHoqAdJWoK072EDoJoKkKEKFwC3S2osn94BmJIHE51cQXWUpH+yPZfUuuPgpmvxPlmO6N8EfR59RahqMcv2aaVDhPkeoX9xo8Z71bIFsvQ02w9LGmV7HjA5V3kOIfSouNnqQbZfknSvpNVt/750ntA9bJ8g6Qr6ZuA/bvuufPnD5ZIt1BTgRkmzSMt+bgaQtBapUFMoYzJwh6TmbQbnFMzTjqmSvgpcRv/lu/FAsADbxwFIWjYdpq0GYbGbk5dOT5N0EvBnYOnCmVoiaXvS0u+XFvrNIYSFiuW7PUrSdcDmpGVLzzbO17TPSdJmtqcOOLeD7cuH+pnwypO0FbC27cm5b9sytgfrH9ZR8h6nVYGrm5aLrkPKHwOIQmrfZiDp+kFO23ZNLWGGpZOX6DdI2oDUgmTFfGoWsK/tX5VL1XtyZe2/kPaTHkHqg3u67YeLBmuBpB8BWwAXA5NtP1A4UghVi0FpjxrQQHy+mvY55eI0+9meno/3Ag63/dayyXqXpGNI+4PWtb2OpNVIVXe3LBwtVCRXQ17Z9s8GnN8ReGzgw6hOJmmc7RkLO9cNJH3E9g9K51iQvET0KNvX5+N3AV+x/faiwXpQfmiJ7Zmls7QrtxvZC9iftEd5MjDF9uyiwXqUpLcDb6BpJWhtrdB6WQxKe5CkD5JKZU+3fVXpPO3KPUkvIi0L3YpUKGF727HcshBJ04BNgLsbMyaS7qupME0oT9INwEdsPzrg/Fqk6rvVzDIOVvxH0lTbm5XK1K68cmAiqcVC801fTf8e84uaLehceGVIEnAM8GnS6oclgLnAJNvHl8zWLkkrA3uTel0/QLq/Oq2mKuHdQNJ5wHhgGn21CGz70HKpQitiT2mPkXQ68CbgVuAESW+xfULhWG2xPUPSnsD/B/4AvNf2c4Vj9boXbFuSYX611BBatdLAASnML4yyUoE8LZP0RtJ77XK5l2HDWFIPvRpdCHwXOIv+BahqMkPSl0hLeCENJjp+e0EXOZzUtmPzxraO/ID5DElH2D6laLoWSNoBOIA0EDoPeIvtJyQtRRqcxqB08ZoArO+YbatWDEp7z9aklh3z8hvnzUBVg1JJ0+nfi3FFUo+zOyQRs3JF/UTSmcDykj5K+sA+u3CmUJ9XL+BaLQ861gW2B5YnVaFumE3qvVqjubbPKB1ihA4AjqOv9+1NpKWXYfHYF9jO9qzGifyAeW/gaqCaQSmwO3CK7ZuaT9qeI+mAQpl62f3Aa0lFs0KFYvlujxm4lKyGvnID5QIJQ2o0cg9lSNoOeC9padZVtq8pHClURtJ3gSeBLzY/9ZZ0HLCq7YOH/OEOImkUcKTtr5TOsihIOhZ4AriE/pWEnyqVqRV5D+MawMO2/1Y6Ty+SdH9uR9fStRCGIuly0kTFssDGpAKeze9P1RTw7HUxKO0xkuYAjep2Ii07eTj/3bXNMtZa6bVX5JvyPW2fXzpLqEde9n028BbS/iCAjYC7gI/WVERE0vW23106x6IgabD3Vtset9jDtEjSQcBXSD1K1wQOtn1Z2VS9Z0EPwmt7SJ4rtk8C1iNVER4FPGt7bNFgPWaowp0NNRXw7HUxKO0x3TTLGJVeO0euQPgp4HWkfozX5OOJwDTbOxWMFyqV95q9KR/+qsaKtZL+k9Tu4r/o334r2gwtRpLuB95te2b+vTrf9halc/UaSfNoeh00XwLG2B69mCO1TdJdwJ6kvdYTSEuT17J9VNFgPUrSibaPXNi50LliUNqj8kzEc7ZfyhUV3wj8zPaLhaMNW1R67RySLgX+CtwGbAusQHpyfJjtaQv62RCGIula29su7Fwn66Y+pZJGA58g1SYAuAE4s4bPjW7YuhI6i6S7bE9ovu+QdGu0FypjiErncU9YkSh01LtuAt4haQXgWtKyuD1I7VVqEZVeO8c42xsCSDqb1JB+9ZqWWYbOIWkMqaDRyvk9SvnSWGC1YsHa0C1Ld7MzgNHA6fl4n3zuoGKJhu9fJJ021HG0jQhtmCNpSWCapJNIBXbiPmQxk/QJ4JPAOEn3NV1altRpIlQiBqW9S7lC3IGk/mAnSbqndKgWDVbp9azCmXrV/JmSXNn5dzEgDSPwMVLriNWAqfQNSp8BvlMqVDskLUfqy9iYXbwROL7SfsqbD+jneZ2ke4ulac3EAcdTi6QI3WQfUp/VTwNHAK8Hdi2aqDddAPwM+Crwuabzs2spwhaSWL7bo/IA9JOk8usH2v6VpOmN2a5aRKXXzjBgn5BILT3m0FdAKwo/hJZJOqT2BvSSLia1Kjg3n9qH1JZrl6F/qjNJuhvY3fYj+XgccFE3LYOVNMn2IaVzhDrkAovYnlk6S5hfXPE1NE262f59uUShFTFT2rsOAz4PXJIHpOOAwfY+dbqHSIOen0taStKyMUO3+NkeVTpD6EqPN17Tkr4IbAp8ubIiQeNtN8+eHJf3w9doInC9pBmkB05r0H09PqNQXlggSSKtfvg06XWwhKS5pFVnxxcN18MkfRo4FvgL8FI+bSD2lFYiZkpDtfKS3YOBFW2Pl7Q28N2aiqCEEIbWKFKRWz99Ffg68AXbby0cbdgk3QZMtH1LPt4S+HqtlV8lvQpYl3Qz/hvbzy/kR6oSBZDCwkg6Ang/qa3Q7/K5caT91VfaPqVkvl4l6WHgrbafLJ0ltCdmSntUXnLy/0jtFsY0zldWEfJTpD6GdwDY/q2kfy4bKYSwCM3Lf34AOMP2pZKOLZinHZ8Azs17SwU8BXykaKIWSdrG9nWSBi45Hi8J2/9dJFgIZewLbGd7VuOE7RmS9gauJm2LCovfH4Aa9+qHLAalvet8Ut+87YGPA/sBte2JeN72C2klDUj6J9JSjRBCd3gsFzN7D3BinqVbonCmluSWSBvlXr7YfqZwpHa8E7gO2GGQawa6aVCqhX9L6HGjmwekDbkHbjV9VrvQDOAGST8F5q/gsP3NcpFCK2JQ2rtWsn2OpMNs3wjcKOnG0qFadKOkLwCvzgWPPglcXjhTCGHR+XfgX0nLXf8maVVeXkW1o0k6DJgMzAbOkrQp8DnbV5dNNny2j8l/dtv+0cGcWjpA6HgvtHktvLJ+n7+WzF+hMrGntEdJut322yRdBZwG/IlURXF84WjDJmkJ4ECaqu8CZzt+qUPoGnk/6dq2J+dtB8s09nHVQNK9tjeS9D7SloMvAZNr3Lco6TODnH4amJpnhDuepHVIDzbWoH+Fzpq2roSCBlSb73cJGGM7ZksLkrQsqQDm30tnCa2JmdLe9eW8x+mzwCRSU/ojykZq2buA821Hb9IQupCkY4AJpMI6k4HRwI+oq0JqYzno+0mD0XvV2HNQnwn5q7Ei5QPAncDHJV1o+6RiyYbvQuC7pJ7W8xbyvSG8TFSb70ySNgDOA1bMx7OAfW3/qmiwMGwxUxqqJemHwNuAJ4Gb89cttv9aNFgIYZHIrVM2Ae62vUk+d5/takr8S5oMvA5YE9gIGAXcYHuzosHakFfW7NqYgZC0DHARsDNptnT9kvmGQ9LUGv/fhxAWTNKtwFG2r8/H7wK+YvvtRYOFYYuZ0h4jaRILKAZk+9DFGGdEbO8LIGk1YDfgO8BqxO91CN3iBduWZABJS5cO1IYDgY2BGbbnSFqRent7rk7/PXMvAmvYfk5SLa1hLpf0SeAS+hdDeapcpBDCIrB0Y0AKYPuGSj8zelbcvPeeu5r+fhypAXSVcvn1dwAbArOAb5NmS0MI3eEnufru8rkv8QGkZZc12QKYZvvZ/J61KfUW07kAuF3Spfl4B2BKvvH7dblYLdkv/9lcMMvAuAJZQgiLzgxJXyIt4QXYG6im/kCI5bs9TdI9jSVxNcr7BR4h7Q+63vajZROFEBa1XFl7fjEz29cUjtQSSfeRlu2+mXSzdA6wi+13Fg3WJkmbAVuR/j1usX3XQn4khBBecZJWIE22NN6fbgKOjS1d9YhBaQ+TdHeNFSCbSXoTsDXpTWht4EHb+5RNFUIYKUmjSIPQ95TOMhKN91lJRwOP5VZc1b73dkE15NHAJ0ifGwA3AGfafrFYqBBCCLF8N9QrN6NfnVTa/w3AcsBLJTOFEBYN2/MkzZG0nO2nS+cZgdmSPg/sA7wjD7ar/OztkmrIZ5Byn56P98nnDiqWKITQNkmXLei67R0XV5YwMlV+MIb2SZpNX6GjpSQ907hE6us0tkyyttzS9PVt238snCeEsGj9A5gu6Rqa+gLWVJAN2AP4ELC/7cclbQ3UWnxjZ3I1ZADbf8o9AWuyue2Nmo6vk3RvsTQhhJHaAvgDMAW4g742XKEyMSjtMbZru4EYUk1tIUIIbflp/qpWHoheB3xI0o9IhTe+VThWu7qhGvI8SeNtPwIgaRzRrzSEmr0W2A7Yi/QA8KfAlOhPWp8YlIYQQuhIts8tnaFdktYB9iTdKD0J/BepjsO7iwYbmcGqIZ9dOFOrJgLXS5pBmlFZg3pb9ITQ82zPA64ErpT0KtJ77g2Sjrc9qWy60IoodBRCCKEjSVob+CqwPjCmcd52x7fvkPQSqUXVgbYfzudm1JB9QWqvhgyQb1zXJf03/MZ2LT1WQwiDyK/pD5AGpG8ALgO+b/uxkrlCa2KmNFRH0om2j5S0u+0LS+cJIbxiJpN6KZ8CvJs0o1XLfqFdSTOl10u6Evgx9WQfUh6EXgOpQrKkD9s+v3CshZK0je3rJO0y4NJ4Sdj+7yLBQggjIulcYAPgZ8Bxtu8vHCm0KWZKQ3UkTSc1oL+j1rYKIYSFkzTV9maSptveMJ+72fY7Smcbrrzv8oOkJ/jbAOcCl9i+umiwFuRK558CXkeagbgmH08EptneqWC8YZF0nO1jJE0e5LJtH7DYQ4UQRiyvSmkUwmse1NRYwLOnxaA0VEfSycDBpAqWc8hvPMQbUAhdRdIvgHcAFwHXAY8BX7O9btFgbZK0IrA7sIftbUrnGS5JlwJ/BW4DtgVWAJYEDrM9rWS2EEII3SEGpaFaki6t4Ql9CKE9kjYHHgCWB04AxgIn2769aLAeM2CmehQwC1jd9uyyyVon6TODnH4amBoD7BBCKCcGpaFqkl4DbJ4P77A9s2SeEMKiJ2lp288u/DvDK0HS3c1bJQYe10TSBcAE4PJ86gPAncAbgQttn1QqWwgh9LIlSgcIoV2Sdgd+SVoO9+/ALyXtVjZVCGFRkbSFpF+TZkuRtJGk0wvH6kUbSXomf80G3tz4u6RnSodr0UrAprY/a/uzpAHqKsDWwEdKBgshhF4W1XdDzb4IbG77CQBJqwA/J+0/CyHU71vA+0jFdbB9r6Sty0bqPbZHlc6wCK0OvNB0/CKwhu3nJEVrmBBCKCQGpaFmSzQGpNmTxOx/CF3F9h+kfp1U5pXKErrCBcDtuXgTwA7AlFwl+dflYoUQQm+LQWmo2ZWSrgKm5OM9gCsK5gkhLFp/kPR2wJKWBA4lL+UNoR22T5B0BbAVqWL7x23flS9/uFyyEELobVHoKFQtN0Jv3FzcZPuSwpFCCIuIpJWBU4H3kF7jV5PakDxZNFiomqStgLVtT87bPpax/bvSuUIIoZfFoDSEEEIIPUHSMaTiRuvaXkfSaqSqu1sWjhZCCD0tlu+GEELoKJKOXsBl2z5hsYUJ3WZnYBPgbgDbf5K0bNlIIYQQYlAaQgih0wzWk3Rp4EBSS48YlIZ2vWDbkgypB27pQCGEEGJQGiomaXvgCtsvlc4SQlh0bH+j8fc8i3UYsD/wY+AbQ/1cCMPwE0lnAstL+ihwAHB24UwhhNDzYk9pqJakHwFbABcDk21HVc4QuoSkFYHPkCqingucavuvZVOFbiBpO+C9pOJZV9m+pnCkEELoeTEoDVWTNBbYizSLYmAyMMX27KLBQghtk3QysAvwPeA7tv9eOFLoUpJGAXvaPr90lhBC6GUxKA3Vy20j9gYOJ/UwXAs4zfakosFCCG2R9BLwPDCX9LBp/iVSoaOxRYKFauUHmJ8CXgdcBlyTjycC02zvVDBeCCH0vBiUhmpJ2oG0H2g8cB5wru0nJC0FPGB7jaIBQwghdARJlwJ/BW4DtgVWAJYk9b2dVjJbCCGEGJSGikn6IXC27ZsGubat7WsLxAohhNBhJE23vWH++yhgFrB6bPUIIYTOEIPSEEIIIXQ1SXfb3nSo4xBCCGXFoDRUS9LbgEnAeqRlWKOAZ2O/WQghhGaS5tHX/1bAq4E5xD7lEELoCNGnNNTs28CewIXABGBfUpGjEEIIYT7bo0pnCCGEMLQYlIaq2X5Y0ijb84DJkm4tnSmEEEIIIYQwfDEoDTWbI2lJYJqkk4A/A0sXzhRCCCGEEEJowRKlA4QwAvuQfoc/Tdor9Hpg16KJQgghhBBCCC2JQkehapJWAbA9s3SWEEIIIYQQQutipjRUR8mxkmYBvwEekjRT0tGls4UQQgghhBBaE4PSUKPDgS2BzW2vZHsF4K3AlpKOKBsthBBCCCGE0IpYvhuqI+keYDvbswacXwW42vYmZZKFEEIIIYQQWhUzpaFGowcOSGH+vtLRBfKEEEIIIYQQ2hSD0lCjF9q8FkIIIYQQQugwsXw3VEfSPFILmJddAsbYjtnSEEIIIYQQKhGD0hBCCCGEEEIIxcTy3RBCCCGEEEIIxcSgNIQQQgghhBBCMTEoDSGEEEIIIYRQTAxKQwghhBBCCCEUE4PSEEIIIYQQQgjFxKA0hBBCCCGEEEIx/weC6abqaBikPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 17\n",
      "Best features : Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n",
      "Original features : Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEKCAYAAADenhiQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VPX1//HXO4EAAmEH2RdFAa2iRkRw30qrRa271oprbbVYbf2prbVWu7j2W6u2rih1X+qCFcGl4oZLwr4jq4Q17GELJDm/P+6NjuNkMiEzmUlyno/HPDL3zufeey7LnNx7P5/zkZnhnHPOJVNWugNwzjlX/3hycc45l3SeXJxzziWdJxfnnHNJ58nFOedc0nlycc45l3SeXJxzziWdJxfnnHNJ58nFOedc0jVKdwDp0r59e+vVq1e6w3DOuTpl0qRJa82sQ1XtGmxy6dWrFwUFBekOwznn6hRJSxNp57fFnHPOJZ0nF+ecc0nnycU551zSeXJxzjmXdJ5cnHPOJZ0nF+ecc0nnycU551zSeXJxzrkGYk3xDu4cN5dl67el/FgNdhClc841FIvXbuWRDxfxn8mF7Corp2fbPTh3UI+UHtOTi3PO1VPTlm3k4Q8X8tbMVTTOzuKMg7txxVF96N2+ecqP7cnFOefqETPjwy/X8tCEhXy6aB0tmzbiyqP34uKhvejYsmmtxeHJxTnn6oHSsnLenLGShz9YxOyVm+mU24Tf/rAf5w3qQcumjWs9Hk8uzjlXh23fWcZLk5bx6EeLWLZ+O306NOeuMw7g1IO60KRRdtri8uTinHN10IatO3nqs6U8OXEJ67fu5KAerbn55AGc2L8TWVlKd3ieXJxzri5ZvnE7j320iBfyl7FtZxnH9evIz47qw6DebZHSn1QqeHJxzrk6YN6qYh7+YCFjpq0AYPiBXbji6D702zM3zZHF5snFOecylJmRv2QDD32wkP/NXUOzxtlceHhPLjuyD11bN0t3eHF5cnHOuTgWr93KDS9P56v122jXIof2LZp8/bN9ixzaNY9cbkLb5jnkNKpZ8ZPycuPdOat56IOFTP5qI22b53DtCfvw08N70qZ5TpLOLLU8uTjnXAxmxiuTl/P712eS0yiL4/t1YsO2nazdUsKCNVso2lLCztLymNvmNm1E+5ZNaB+ReNq1yKFdiyZ0CH+2ax78zG3a6OtnJTtLy3ltynIe/nAhC4u20q1NM247dT/OOqQ7zXLS1/Nrd2RUcpE0DLgPyAYeM7M7oj4fAdwNLA9XPWBmj4Wf3QWcTFAv7R3gGjOzWgrdOVePFO/Yxc2vzeT1qSs4rHdb/n7uQDq3+vZtKDNjS0kp67bsZN3WEoqKg5/rtuxk3ZYS1m4JEtGXa7bw2aJ1bNi2K+axcrKzwsSTw5rNJawpLqF/51zuO3cgJ3+vM42y62YJyIxJLpKygQeBE4FCIF/SGDObHdX0BTO7OmrbIcBQ4IBw1cfA0cCElAbtnKt3pi7byMjnprB843Z+feI+/OLYvcmO0bVXEi2bNqZl08b0SqCcyq6ycjZs3fl10qlIRF8vbymhc6tm/GRwT47q2z6jen7tjoxJLsAgYIGZLQKQ9DxwKhCdXGIxoCmQAwhoDKxOUZzOuXqovNx4+MNF3Pv2PDrlNuWFKwaT16tt0vbfODuLjrlN6ZhbeyVY0imTkktXYFnEciFwWIx2Z0g6CpgPXGtmy8zsU0nvAysJkssDZjYnekNJVwBXAPTokdqKoM65umPN5h1c9+I0Pl6wlpO/15m//Ph7tGpW+yVT6pNMupkX6xow+pnJG0AvMzsAeBcYDSBpb6A/0I0gSR0XJqBv78zsETPLM7O8Dh06JDV451zd9P7cNQy77yMKlq7njh9/jwfOP8gTSxJk0pVLIdA9YrkbsCKygZmti1h8FLgzfH868JmZbQGQ9BYwGPgwZdE65+q0ktIy7nxrHqM+WUy/PVvywPmD2btjy3SHVW9k0pVLPtBXUm9JOcC5wJjIBpI6RywOBypufX0FHC2pkaTGBA/zv3NbzDnnABYWbeH0Bycy6pPFjBjSi9euGuqJJcky5srFzEolXQ2MJ+iKPMrMZkm6DSgwszHASEnDgVJgPTAi3Pxl4DhgBsGttHFm9kZtn4NzLrOZGS9NKuQPr8+iaeMsHvtpHicM6JTusOolNdShIHl5eVZQUJDuMJxztWTzjl387tWZvDFtBYf3acf/nTOQPVs1jJ5bySRpkpnlVdUuY65cnHMuVSZ/tYGRz01h5aYdXP/9fbny6L1ijl1xyZPS5CIpC2hhZptTeRznnIulrNx46IOF/O2d+XRu1ZSXrjycg3u0SXdYDULSH+hLelZSrqTmBAMg50m6PtnHcc65eFZv3sGFj3/O3ePn8YP992TsNUd6YqlFqbhyGWBmmyVdAIwFbgAmEdQEc865lHtvzmp+89I0duwq564zD+CsQ7rV+XIqdU0qkkvjsDvwaQQj5XdJapi9BpxztWrHrjLueGsuT05cwoDOudx//kHs1aFFusNqkFKRXB4GlgDTgA8l9QT8mYtzLqUWrCnm6menMHdVMZcM7c0NP9iXJo3qVpn6+iTpycXM/gH8I2LVUknHJvs4zjkHwdiVF/KXcesbs9gjpxGjRuRxXD8fu5JuSU8ukjoBfwG6mNkPJA0ADgceT/axnHOZZ9n6bXy2aB2fLVrP0nVbyZKQIDtLZElkZYls8fX7rPAzSWRL4XvIVkT7rLB9+MrOItxWfLm6mHfnrGHo3u34v7MHNpiqw5kuFbfFngSeAH4XLs8HXsCTi3P1UmQy+WzROpZv3A5A2+Y57NspKKlSZsausnLKyo1yg3Kzb96XG2VmlJtRHq4LPrOwXXB1UhZuY1GfN87O4oZh/fjZUX3I8rErGSMVyaW9mb0o6Sb4uqxLWQqO45xLg3jJ5LDebbniqD4M7tOOvh1b+Jd9A5aK5LJVUjvCcvmSBgObUnAc51wtWLZ+G58vXh8mlHUUbvBk4qqWiuRyHUE1470kfQJ0AM5MwXGccylQuGHb11clkcmkzR6NGdynHZcf6cnEVS2pySUs99KUoOT9vgQTgM0zs13JPI5zLnk8mbhUSGpyMbNySfea2eHArGTu2zmXPB/OL2LMtBWeTFzKpOK22NuSzgBesYZaz9+5DLawaAsXP5lPy6aNGNy7HZcd0ZvBe7Vjn44tPZm4pEnVM5fmQKmkHQS3xszMclNwLOdcNd01bi5NG2Xx7nVH075Fk3SH4+qpVIzQ97lCnctQk5auZ/ys1Vx34j6eWFxKpaLk/lGxXglsN0zSPEkLJN0Y4/MRkookTQ1fl4Xrj41YN1XSDkmnJfu8nKvrzIy/jp1Lh5ZNuOzI3ukOx9VzqbgtFjl3S1NgEEHJ/eMq20BSNvAgcCJQCORLGmNms6OavmBmV0euMLP3gYHhftoCC4C3a3oSztU3b89eTcHSDfz59P3ZI8cnoXWplYrbYj+KXJbUHbiris0GAQvMbFG4zfPAqQSTjVXHmcBbZratmts5V6+VlpVz17i59OnQnHPyuqc7HNcAJP22WAyFwP5VtOkKLIvapmuMdmdImi7p5TBpRTsXeK6yg0i6QlKBpIKioqKq4nau3nixoJCFRVu5YVg/GmXXxn9719Cloiry/YSlXwiS10CCuV3ibhZjXXQ35jeA58ysRNKVwGgibrVJ6gx8Dxhf2UHM7BHgEYC8vDzvJu0ahG07S/m/d+dzSM82nDTAS9G72pGKG68FEe9LCRLCJ1VsUwhEXol0A1ZENjCzdRGLjwJ3Ru3jbOBVrwbg3Lc9/tFiiopLeOgnB/tUv67WpCK5tDaz+yJXSLomel2UfKCvpN7AcoLbW+dH7aOzma0MF4cDc6L2cR5wU40id66eWbelhIc/XMRJAzpxSM+26Q7HNSCpuPl6UYx1I+JtYGalwNUEt7TmAC+a2SxJt0kaHjYbKWmWpGnAyMh9SupFcOXzQU2Dd64+uf9/C9i+q4z/N6xfukNxDUzSrlwknUdwtdFb0piIj1oC62Jv9Q0zGwuMjVp3S8T7m6jkysTMlhC7A4BzDdbSdVt55vOlnJ3Xnb07tkh3OK6BSeZtsYnASqA9cG/E+mJgehKP45xLwN3j59EoK4trT+ib7lBcA1RlclHwBPACoI+Z3SapB7CnmX0R2c7MlgJLgcNTEqlzLmHTlm3kv9NX8svj9vY55V1aJPLM5Z8ECeO8cLmYYDR9TJIGS8qXtEXSTkllkjYnIVbnXALMjDvemkvb5jlccVSfdIfjGqhEksthZnYVsAPAzDYAOXHaP0CQiL4EmgGXAffXME7nXIImzC/i00XrGHnc3rRs2jjd4bgGKpFnLrvC2l8GIKkDUB5vAzNbICnbzMqAJyRNrHmozrmqlJUbd741l57t9uD8w3qmOxzXgCWSXP4BvAp0lPRngvpdN8dpv01SDjBV0l0ED/mb1zhS51yVXp2ynLmrirn/vIPIaeRlXlz6VJlczOwZSZOA4wnKtJxmZtEDGCNdSHC77WrgWoLxJ2ckIVbnXBw7dpXxt7fncUC3Vpz8vc7pDsc1cHGTi6QsYLqZ7Q/MTWSHZrZUUjOgs5n9MQkxOucSMHriElZs2sE9Zx/o0xW7tIt73Wxm5cC0sPtxQiT9CJgKjAuXB0YNqnTOJdnGbTt58P0FHLNvB4bs1T7d4TiX0DOXzsAsSV8AWytWmtnwStrfSjA/y4Sw3dSwPItzLkX+OWEhxSWl3OBlXlyGSCS5VPfWVqmZbfLqq87VjuUbt/PkxCX8+KBu9O+cm+5wnAMSe6D/gaROwKHhqi/MbE2cTWZKOh/IltSXoMikd0V2LkXufXseANedtE+aI3HuG1X2VZR0NvAFcBbBnCmfSzozzia/BPYDSoBngU3Ar2oeqnMu2uwVm3l1ynIuHtKLrq2bpTsc576WyG2x3wGHVlythIMo3wVejmwk6SkzuxC43Mx+F27nnEuhO8fNJbdpY35xzN7pDsW5b0lklFVW1G2wdZVsd4iknsAlktpIahv5Skq0zrmvTVywlg/mF3HVsXvRag8v8+IySyJXLuMkjQeeC5fPAd6K0e4hgu7HfYBJBAMuK1i43jmXBOXlxl/fmkvX1s346eG90h2Oc99R5ZWLmV0PPAwcABwIPGJm/y9Gu3+YWX9glJn1MbPeEa8qE4ukYZLmSVog6cYYn4+QVCRpavi6LOKzHpLeljRH0mzv+uzqu//OWMmM5Zv49Un70LRxdrrDce47EpnPpTcw1sxeCZebSeoVzv74HWb28+oGERbGfBA4ESgE8iWNMbPZUU1fMLOrY+zi38CfzewdSS2oorCmc3XZztJy7hk/j/6dczltoE/A6jJTIs9cXuLbX9Zl4bpkGgQsMLNFZrYTeB44NZENJQ0AGpnZOwBmtsXMtiU5PucyxjOfL+Wr9du48Qf9vMyLy1iJJJdG4Rc+AOH7ePO57I6uwLKI5cJwXbQzJE2X9LKk7uG6fYCNkl6RNEXS3eGVkHP1TvGOXdz/vwUM3bsdR/X1Mi8ucyWSXIokfV3qRdKpwNokxxHr1y+LWn4D6GVmBxB0hR4drm8EHAn8hmCgZx9gRMyDSFdIKpBUUFRUlIy4natVD3+wiPVbd3LjsP54FQyXyRJJLlcCv5X0laRlwA3Az6IbSSqWtLmyVxXHKCQozV+hG7AisoGZrTOzknDxUeCQiG2nhLfUSoHXgINjHcTMHjGzPDPL69ChQxUhOZdZVm/ewWMfL2L4gV34XrdW6Q7HubgSKf+yEBgcPiiXmRVX0q4lgKTbgFXAUwRXJBcALas4TD7QN+w8sBw4Fzg/soGkzma2MlwcDsyJ2LaNpA5mVgQcBxRUdV7O1TV/f3c+ZeXG9d/fN92hOFelRMq/XCMpl6Ai8v9JmizppDibfN/M/mlmxWa22cz+RRWThYVXHFcD4wmSxotmNkvSbRG35EZKmiVpGkG9shHhtmUEt8TekzSDIKE9WtV5OVeXfLm6mBfyl/GTwT3p3naPdIfjXJUSGUR5iZndJ+n7QEfgYuAJ4O1K2pdJuoCgx5cB5xH0MIvLzMYCY6PW3RLx/ibgpkq2fYdgHI5z9dKd4+bRPKcRvzyub7pDcS4hiTxzqXhq+EPgCTObRuwH8BXOJyhwuTp8nUXULS7nXOLyl6zn3TmrufKYvWjbPNkdNZ1LjUSuXCZJehvoDdwkqSVxBimGgysTGqPinIvPzPjL2Dl0ym3CJUN7pzsc5xKWyJXLpcCNBJWRtxGMcbm4ssaS9pH0nqSZ4fIBkm5OSrTONTDjZ61iylcbufaEfWiW48O3XN2RSG2xcjObbGYbw+V1ZjY9ziaPEjwb2RW2n07Q+8s5Vw27ysq5a9w89u7YgjMP6ZbucJyrlkSuXKprDzP7ImpdaQqO41yt2lpSyszlm9ixq8r+KUnxQv4yFq3dyg3D+tEoOxX/VZ1LnUSeuVTXWkl7EY6wD2etXBl/E+cy27otJZzxr4ksWbeNLEGv9s3pv2cu++7Zkn57tqR/51y6tm6WtFpfW0tK+fu7X3Jorzac0L9jUvbpXG1KKLmEtbo6RbY3s68qaX4V8AjQT9JyYDHBQErn6qStJaVc8mQ+Kzft4PbT9mdtcQlzV21m5opNvDnjm9+bWjRpxL57tmTfPVvSf8+W9OscJJ/cptWfyOuxjxazdksJD194iJd5cXVSIiX3fwn8gaBbcUUvMaPycSVmZidIak4wi2VxOPLeuTpnV1k5Vz07mRnLN/HwhXmcOKDTtz7fWlLKvNXFzFtVzNyVm5mzqpj/TlvBs59/cye4a+tm9NuzJf06t6Tfnrn079ySXu2aV3qra+2WEh75cCHD9tuTQ3q2Sen5OZcqiVy5XAPsa2brEtznf4CDzWxrxLqX+aYWmHN1gplx0yszmDCviL+c/r3vJBaA5k0acXCPNhzco823tlu5aQfzVhUzZ9Vm5q4sZu6qzXwwv4jS8qAea06jLPp2bPF1sukX3mLr0LIJ/3jvS3aUlnP9MC/z4uquRJLLMmBTVY0k9QP2A1pJ+nHER7lA090Lz7n0ufft+bw8qZCRx/fl/MN6JLydJLq0bkaX1s04tt83z0tKSstYuGYrc1dtZu6qYuauKuajL4v4z+TCr9u0b5HDhm27OPfQ7uzVoUVSz8e52pRIclkETJD0JlBRlRgz+1tUu32BU4DWwI8i1hcDl9cwTudq1VOfLeWB9xdw7qHdufaE5JRcadIomwFdchnQJfdb69dtKQmvcoqZt2oza7fs5Fcn7JOUYzqXLokkl6/CVw5xJgkzs9eB1yUdbmafJik+52rduJmruOX1mZzQvyN/Om3/lD9Qb9eiCUP2bsKQvX3yL1d/JFJy/48AYdkXM7MtVWxypaQ5FYMuJbUB7jWzS2ocrXMplr9kPSOfn8LA7q25/7yDfXyJc7spkZL7+0uaAswEZkmaJGm/OJscUJFYAMxsA3BQzUN1LrXmry7m0ifz6da6GY9fdKiXW3GuBhL5tewR4Doz62lmPYFfE3++lKzwagUASW1JzWBN55Jm5abtXDTqC5o0zmb0JYO8+rBzNZTIl35zM3u/YsHMJoRjWCpzLzBR0ssE42HOBv5cszCdS51N23cxYlQ+xTtKeeFng30yLueSIKHeYpJ+TzBtMcBPCEbdx2Rm/5ZUQDDdsIAfm9nsGkfqXArs2FXG5f8uYNHaLYy+eBD7dfG56Z1LhkRui10CdABeAV4N31dacj/UFthqZvcDRYmM0Jc0TNI8SQsk3Rjj8xGSiiRNDV+XRXxWFrF+TALn5Bxl5cZ1L07li8Xruffsgd5by7kkSqS32AaCOesTIukPQB7BuJcngMbA08DQONtkAw8CJwKFQL6kMTGueF4ws6tj7GK7mQ1MNEbnzIzb3pjF2BmruPnk/gw/sEu6Q3KuXqk0uUj6u5n9StIbhBWOI5nZ8Eo2PZ2gd9jksN2KsBtzPIOABWa2KDz28wSzWfrtNJcS//pgIaM/XcrlR/bmsiP7pDsc5+qdeFcuFc9Y7qnmPneamUmqKLkf7+F/ha4EZWYqFAKHxWh3hqSjgPnAtWZWsU3T8DlPKXCHmb1WzZhdA/KfSYXcNW4epw7swk0/6J/ucJyrlyp95mJmk8K3A83sg8gXEO8W1IuSHgZaS7oceJf4XZchePD/nRCilt8AepnZAeE+R0d81sPM8oDzgb+H88l89yDSFZIKJBUUFRVVEZKrjybMW8MN/5nO0L3bcfeZByZt/hXn3Lcl8kD/ohjrRlTW2MzuIaiC/B+C5y63hA/24ykEukcsdwNWRO13nZlV1DZ7lIgqy2a2Ivy5CJhAJYM2zewRM8szs7wOHTpUEZKrb6YXbuQXz0xmn04teegnh5DTyEffO5cq8Z65nEdwJdA7qgdWSyBu+X0zewd4pxpx5AN9w15ly4Fzw2NHxtPZzCpmZhoOzAnXtwG2mVmJpPYEHQfuqsaxXQOwZO1WLn4in7bNc3jy4kNpuRsTeDnnEhfvmctEgumJ2xMMjKxQDEyPbiypmBgP/glueZmZ5cb4DIIPSyVdDYwHsoFRZjZL0m1AgZmNAUZKGk7wXGU931w99QcellROcCV2h4+rcZHWbinhoie+oNyMf18yiI65PgOEc6kms1j5oP7Ly8uzgoKCdIfhUmxrSSnnPfoZ81cX89zlgzmoh8/s6FxNSJoUPuOOK5HClYMl5UvaImlnOGBxcxXbHCHp4vB9e5/m2KXDrrJyfv7MZGat2MyD5x/sicW5WpTIE80HgPOAL4FmwGVApQ/ow0GUNwA3hatyCAZROldrzIwb/jOdD+cX8ZfT9+f4/t+dotg5lzoJdZcxswVAtpmVmdkTwLFxmp9O8MB9a7jtCoJOAM7VmrvHz+OVycu57sR9OOfQxKcods4lRyKFK7dJygGmSrqL4CF/vIGRuzOI0rmkGT1xCf+csJDzD+vBL4/bO93hONcgJXLlciFBD66rCa5GugNnxGm/O4MonUuKsTNWcusbszhxQCduPzX1UxQ752JLpHDl0vDtduCPCbS/R9KJwGa+GURZnTEvzu2Wzxet41cvTOXgHm24/7yDyPbR986lTbxBlDOIPW4FgLAMS6ztmgP/M7N3JO0L7CupsZntqnG0zlVi3qpiLvt3AT3a7sHjF+XRtLFPUexcOsW7cjkl/HlV+LOikOUFwLY4230IHBmOnH8XKADOCbdzLulWbdrBiCe+YI+cYIri1nv4FMXOpVulyaXidpikoWYWORfLjZI+AW6rZFOZ2TZJlwL3m9ldkqYkL2TnvrG1pJRLR+ezefsuXrpyCF1bN0t3SM45Enug31zSERULkoYQv7eYJB1OcKXyZrgukV5pzlVLWblxzfNTmLuqmAcvOJgBXSqtMOScq2WJfOlfCoySVDG5+EaCqY8rcw3BAMpXw/pgfYD3axamc9/1pzdn8+6cNdx+2v4cs2/HdIfjnIuQSG+xScCBknIJbnltqqL9hwTPXSqWF1GNaZKdS8ToiUt44pMlXHpEby4c3DPd4TjnosTrLfYTM3ta0nVR6wEws7+lODbnYvrf3NX8MRzL8tsf+kySzmWieFcuFc9VvHSLyxizVmzil89OYb8urbjv3IE+lsW5DBWvt9jD4c8qB04CSLrTzG6QdJaZvZSsAJ2rsGrTDi59soDcZo157KI89sjxfiLOZap4t8X+EW9DM4t+jvJDSTcTPMz35OKSqqLLcfGOXbz88yF08gm/nMto8X71m1TNfY0D1hJ0Xd5MOAMlCcxE6Vw8FV2O56zczOMjDqV/Z/+n5Fymi3dbbHR1dmRm1wPXS3rdzE7dnWAkDQPuIyiU+ZiZ3RH1+QjgbmB5uOoBM3ss4vNcYA5BN+irdycGl3m+7nJ86n4c612OnasTqrxpLakDweRfA4Cv70WY2XGx2pvZqZI6AYeGqz43s6IEjpMNPAicCBQC+ZLGmNnsqKYvxEkctwMfVHUsV3f8+9Ogy/ElQ3tz4eG90h2Ocy5BiYzQf4bgaqA3QVXkJUB+ZY0lnQV8AZwFnA18IenMBI4zCFhgZovMbCfwPJDwFZCkQ4BOwNuJbuMy2/tz13DrmFmc0L8TvzvZuxw7V5ckklzamdnjwC4z+8DMLgEGx2l/M3ComV1kZj8lSBq/T+A4XYFlEcuF4bpoZ0iaLullSd0BJGUB9wLXJ3AcVwfMXrGZq5+dzIAuufzjPO9y7Fxdk0hyqSiVv1LSyZIOArrF26eZrYlYXpfgcWJ9e0SX/H8D6BWW+38XqHgu9AtgrJktIw5JV0gqkFRQVFTlnTqXJqs37+DS0fnkNmvM4xcd6l2OnauDEvlf+6ewrtivgfuBXODaOO3HSRoPPBcunwOMTeA4hQSzXFboBqyIbGBm6yIWHwXuDN8fTlDm/xdACyBH0hYzuzFq+0eARwDy8vIqnavGpc/WklIuefKbKsfe5di5uimR5PJ5WE9sE3BsVY3N7HpJPwaOILgaecTMXk3gOPlAX0m9CXqDnQucH9lAUmczWxkuDid4FoSZXRDRZgSQF51YXOb7Vpfjiw71KsfO1WGJJJeJkhYDLwCvmNmGqjYws1eAV6oTiJmVSroaGE/QFXlUWFX5NqDAzMYAIyUNB0qB9cCI6hzDZbY/vzmHd+es4bZT9+PYft7l2Lm6TGZV3x2SNIjgSuI0YDbwvJk9neLYUiovL88KCgrSHYYL/fvTJdzy+iwuGdqbW340IN3hOOcqIWmSmeVV1S6RB+2Y2Rdmdh1Bz6/1fPMg3bka8y7HztU/VSYXSbmSLpL0FjARWEmQZGK1zZZUp69oXO2q6HLcv3OuVzl2rh5J5JnLNOA14DYz+zReQzMrk9RBUk44ENK5SlV0OW7ZNOhy3LyJdzl2rr5I5H9zH0vkwcw3lgCfSBoDbK1Y6ZOLuUjbdgZVjiu6HO/ZyrscO1efJDLNcXXHg6wIX1n4RGMuhrJyY+RzU5m9wrscO1dfJf0+RMXkYpKam9nWqtq7hucvY+fw7pzV/HG4dzl2rr5KqLdYdUg6XNJswgGOkg6U9M9kH8fVTU99uoTHP17MxUN7cdGQXukOxzmXIon0Frsr7DHWWNJ7ktZJXuM6AAAXZUlEQVRK+kmcTf4OfJ+gphhmNg04Kjnhurrs/Xlr+MOYWZzQvyM3n+xjWZyrzxK5cjnJzDYDpxDU/9qHKqoPxyggWbZ74bn6YvaKzVz9TEWX44O8y7Fz9Vwiz1wahz9/CDxnZuuluF8MyyQNAUxSDjCS8BaZa5i8y7FzDU8iVy5vSJoL5AHvhTNT7ojT/krgKoK5WJYDA8Nl1wBt3LaTS0fns2n7Lh4fkeddjp1rIBLpinyjpDuBzeEgya3EmSHSzNYCF1T2uWsYlq7byhOfLOHFgmXs2FXGYxflsV+XVukOyzlXS6pMLuG0xePCxHIzcDDwJ2BVJe37APcRzFZpwKfAtWa2KGlRu4xkZkxauoHHPlrM+NmraJQlfnRgFy4/sg/9O/tYFucakkRufv/ezF6SdARBL7B7gH8Bh1XS/lngQeD0cPlcgonDKmvv6rjSsnLGzVrFYx8tZuqyjbRq1phfHLMXPz28l0/25VwDlUhyqejpdTLwLzN7XdKtcdrLzJ6KWH46nKfF1TPFO3bxQv4ynvhkCcs3bqdXuz24/dT9OOOQbj41sXMNXCLfAMslPQycANwpqQkxOgJIahu+fV/SjcDzBLfFzgHeTFK8LgMs37idJz5ezPP5y9hSUsqg3m35w48GcHz/Tt7F2DkHJJZczgaGAfeY2UZJnYk9zmUSQTKp+Hb5WcRnBtxek0Bd+k1btpFHP1rEWzODx22nHNCZS4/ozQHdWqc5Mudcpkmkt9g2SQuB70v6PvCRmb0do13vmgYjaRhBZ4Bs4DEzuyPq8xHA3QRdnAEeMLPHJPUkmFY5m2Bczv1m9lBN43FBkcl3Zq/m8Y8Xkb9kAy2bNuKyI3pz0ZBedGndLN3hOecyVCK9xa4BLif48obgGcojZnZ/Je2zCZ7P9Ircf1Ul98PtHgROJKgEkC9pjJnNjmr6gplFP8NZCQwxsxJJLYCZ4bYrqjo/F9vWklJenlTIqE8Ws3TdNrq1acYtpwzg7EO708IHQTrnqpDIt8SlwGEVFY7DMS+fAjGTC/AGwSDLGUB5NWIZBCyo6LIs6XmC8TTRyeU7oiYmi/lMyCVm1aYdjP50Cc9+/hWbtu/i4B6tuWFYP04a0IlG2f7H6pxLTCLJRXy7NlgZ3zxXiaWbmR2wG7F0BSJrkhUSu/vyGZKOAuYTjJ9ZBiCpO0HHgb2B6/2qpXpmrdjE4x8tZsy0FZSbMWz/Pbn0iD4c0rNNukNzztVBiSSXJ4DPJb0aLp8GPB6n/VuSTor1XKYKsRJW9ERlbxDUNyuRdCUwGjgOvi6WeYCkLsBrkl42s9XfOoB0BXAFQI8ePaoZXv1TXm5MmL+GRz9czKeL1tE8J5sLD+/JJUN7073tHukOzzlXhyXyQP9vkiYARxAkgIvNbEqcTT4DXpWUBewKtzEzq2qIdiHQPWK5G8GMlpGxrItYfBS4M0a8KyTNAo4EXo767BHgEYC8vLzqzrBZr5gZVzxVwLtz1tC5VVN++8N+nHNoD1o1a1z1xs45V4W4ySVMENPNbH9gcoL7vBc4HJhRzSmS84G+knoT9AY7Fzg/Kp7OZrYyXBzONxOSdQPWmdl2SW2AoUDcDgQN3ZhpK3h3zhp+dUJfrjp2bxr78xTnXBLFTS5mVi5pmqQeZvZVgvv8EphZzcSCmZWGI/nHE3QpHmVmsyTdBhSY2RhgpKThQCmwHhgRbt4fuFdSxTibe8xsRnWO35Bs2raL2/87mwO7teKXx/X1gY/OuaRL5JlLZ2CWpC+ArRUrzWx4Je1XAhMkvQWURLSv8krCzMYCY6PW3RLx/ibgphjbvQPsTieCBunut+eyfutOnrx4kCcW51xKJJJc/ljNfS4OXznhy2WQKV9t4JnPv+LiIb3Zv6uXwHfOpUalyUXS3kAnM/sgav1RfDNC/jvMrLrJyNWS0rJyfvfqTDq1bMp1J+2T7nCcc/VYvCuXvwO/jbF+W/jZj2JtJOl9vtuFGDM7bncCdMnz5MQlzF65mX9dcLCPsnfOpVS8b5heZjY9eqWZFUjqFWe730S8bwqcQfAA3qXRio3b+ds78zl23w4M23/PdIfjnKvn4iWXeLM8VVqx0MwmRa36RNIHMRu7WnPbG7MpKzduO3V/JH+I75xLrXiDG/IlXR69UtKlBOX1Y5LUNuLVPqyk7L8qp9F7c1YzbtYqRh7f10feO+dqRbwrl18RjLS/gG+SSR5BD7DTK93q2/O6lBL0HLu05qG63bF9Zxm3vD6Lvh1bcPmRfdIdjnOugag0uYR1uYZIOhbYP1z9ppn9L94OkzGvi0uef/zvS5Zv3M4LVwwmp5GPwnfO1Y5Eaou9D7xfnZ1KGsJ353P5d3WDczUzb1Uxj364iLMO6cZhfdqlOxznXAOS9P6okp4C9gKm8k2pfgM8udSi8nLj5tdm0KJpI276Yf90h+Oca2BSMdghDxhQ3dpiLrlenlRI/pIN3HXGAbRt7oUSnHO1KxU34WfivcPSav3WnfzlrTkc2qsNZx7SLd3hOOcaoFRcubQHZoeFLiMLV1ZW6NIl2V/HzmHLjlL+fPr3yPLClM65NEhFcrk1Bft0Cfp80TpemlTIz4/Zi306tUx3OM65BirpySW60KWrPTtLy7n5tZl0a9OMkcf1TXc4zrkGzKsX1iOPfrSIL9dsYdSIPJrlZKc7HOdcA+aj6uqJr9Zt4x/vfcmw/fbkuH6d0h2Oc66BS1pykfRe+PPO3dx+mKR5khZIujHG5yMkFUmaGr4uC9cPlPSppFmSpks6p2ZnUveYGbeMmUmjLPGH4QPSHY5zziX1tlhnSUcDwyU9T1Bb7GtmNrmyDSVlAw8CJwKFBEUzx5jZ7KimL5jZ1VHrtgE/NbMvJXUBJkkab2Yba3pCdcW4mauYMK+I358ygM6tKi1Y7ZxztSaZyeUW4EagG/C3qM8MiDdZ2CBggZktAgiT06lAdHL5DjObH/F+haQ1QAegQSSXLSWl3PrGLAZ0zuWiw3umOxznnAOSmFzM7GXgZUm/N7Pbq7l5V2BZxHIhcFiMdmeE0yzPB641s8htkDSIoGrzwmoev87629vzWVNcwkM/OYRG2f4IzTmXGZL+bWRmt0saLume8HVKApvFGukXXT7mDYLZMQ8A3gVGf2sHUmfgKeBiMyuPeRDpCkkFkgqKiooSCCuzzVy+iScnLuaCw3pwUI826Q7HOee+lvTkIumvwDUEt7RmA9eE6+IpBLpHLHcDVkQ2MLN1ZlYx4v9R4JCIY+YCbwI3m9lnlR3EzB4xszwzy+vQoUOip5SRysqN3706g7bNc7j++/3SHY5zzn1LKsa5nAwMrLh6kDQamALcFGebfKCvpN7AcuBc4PzIBpI6m9nKcHE4MCdcnwO8CvzbzF5K5olksmc/X8q0wk3cd+5AWjVrnO5wnHPuW1I1iLI1sD5836qqxmZWKulqYDyQDYwys1mSbgMKzGwMMFLScILZLdcDI8LNzwaOAtpJqlg3wsymJutkMs2a4h3cNW4eR+zdnuEHdkl3OM459x2pSC5/BaZIep/gWcpRxL9qAcDMxgJjo9bdEvH+plj7MbOngadrGHOd8qf/zqGkrJzbT9sfyQtTOucyTypqiz0naQJwKEFyucHMViX7OA3VR18WMWbaCn51Ql96t2+e7nCccy6mlNwWC5+NjEnFvhuyHbvK+P1rM+ndvjlXHr1XusNxzrlKeeHKOuRfExayZN02nr70MJo29sKUzrnM5aPu6oiFRVv414SFnDqwC0f0bZ/ucJxzLi5PLnWAmfH712bSpHEWN5/shSmdc5nPk0sd8PrUFUxcuI4bhvWjQ8sm6Q7HOeeq5Mklw23atos/vTmbgd1bc/6gHukOxznnEuIP9DPcXePnsn7rTkZfMoisLB/T4pyrG/zKJYNN/moDz37xFRcP7c1+XaosdOCccxnDk0uGKi0r53evzmTP3KZce+I+6Q7HOeeqxZNLhnpy4hLmrNzMH360Hy2a+N1L51zd4t9aGWb15h089elSHvt4Ecf368j39+uU7pCcc67aPLlkiOmFGxn18WL+O30lZWac2L8Tf/LClM65OsqTSxqVlpXz9uzVjPp4MQVLN9CiSSMuGtKLiw7vRY92e6Q7POec222eXNJg0/ZdvJD/FaMnLmX5xu30aLsHt5wygLPyutGyqU/85Zyr+zy51KLFa7fy5CeLeWlSIdt2ljG4T1v+8KMBHN+/E9k+hsU5V494ckkxM2PiwnWM+ngx/5u3hsZZWQwf2IWLh/bysSvOuXorY5KLpGHAfQTTHD9mZndEfT4CuBtYHq56wMweCz8bBwwGPjazU2ot6Dh27Crj9anLGfXxEuatLqZ9ixyuOb4vFxzW0+uDOefqvYxILpKygQeBE4FCIF/SGDObHdX0BTO7OsYu7gb2AH6W2kirtmbzDp7+bClPf/4V67fupH/nXO4+8wCGD+xCk0Y+B4tzrmHIiOQCDAIWmNkiAEnPA6cC0cklJjN7T9IxqQuvajOXb2LUx4t5Y/oKSsuNE/p34pKhvRncp613J3bONTiZkly6AssilguBw2K0O0PSUcB84FozWxajTaUkXQFcAdCjR80rDJeVG+/MXsWoj5fwxZL1NM/J5ieDezJiSC96tvP57Z1zDVemJJdYv9pb1PIbwHNmViLpSmA0cFx1DmJmjwCPAOTl5UXvP2Gbd+zixfxlPDlxCYUbttO9bTN+H3YlzvWuxM45lzHJpRDoHrHcDVgR2cDM1kUsPgrcWQtxfcfTny3lr2PnsHVnGYN6t+Xmkwdw4gDvSuycc5EyJbnkA30l9SboDXYucH5kA0mdzWxluDgcmFO7IQa6tmnG9/ffk0uG9mb/rt6V2DnnYsmI5GJmpZKuBsYTdEUeZWazJN0GFJjZGGCkpOFAKbAeGFGxvaSPgH5AC0mFwKVmNj4VsR67b0eO3bdjKnbtnHP1hsx2+9FDnZaXl2cFBQXpDsM55+oUSZPMLK+qdj6fi3POuaTz5OKccy7pPLk455xLOk8uzjnnks6Ti3POuaTz5OKccy7pPLk455xLugY7zkVSEbA03XFUoT2wNt1BJEF9OQ/wc8lU9eVc6sJ59DSzDlU1arDJpS6QVJDIYKVMV1/OA/xcMlV9OZf6ch7gt8Wcc86lgCcX55xzSefJJbM9ku4AkqS+nAf4uWSq+nIu9eU8/JmLc8655PMrF+ecc0nnySXDSOou6X1JcyTNknRNumOqKUnZkqZI+m+6Y6kJSa0lvSxpbvj3c3i6Y9odkq4N/23NlPScpKbpjilRkkZJWiNpZsS6tpLekfRl+LNNOmNMVCXncnf472u6pFcltU5njDXhySXzlAK/NrP+wGDgKkkD0hxTTV1DmmYOTbL7gHFm1g84kDp4TpK6AiOBPDPbn2ByvnPTG1W1PAkMi1p3I/CemfUF3guX64In+e65vAPsb2YHAPOBm2o7qGTx5JJhzGylmU0O3xcTfIF1TW9Uu09SN+Bk4LF0x1ITknKBo4DHAcxsp5ltTG9Uu60R0ExSI2APYEWa40mYmX1IMBNtpFOB0eH70cBptRrUbop1Lmb2tpmVhoufAd1qPbAk8eSSwST1Ag4CPk9vJDXyd+D/AeXpDqSG+gBFwBPhLb7HJDVPd1DVZWbLgXuAr4CVwCYzezu9UdVYJzNbCcEvZ0B9mYf8EuCtdAexuzy5ZChJLYD/AL8ys83pjmd3SDoFWGNmk9IdSxI0Ag4G/mVmBwFbqTu3X74WPo84FegNdAGaS/pJeqNy0ST9juAW+TPpjmV3eXLJQJIaEySWZ8zslXTHUwNDgeGSlgDPA8dJejq9Ie22QqDQzCquIl8mSDZ1zQnAYjMrMrNdwCvAkDTHVFOrJXUGCH+uSXM8NSLpIuAU4AKrw2NFPLlkGEkiuK8/x8z+lu54asLMbjKzbmbWi+Ch8f/MrE7+lmxmq4BlkvYNVx0PzE5jSLvrK2CwpD3Cf2vHUwc7JkQZA1wUvr8IeD2NsdSIpGHADcBwM9uW7nhqwpNL5hkKXEjwW/7U8PXDdAflAPgl8Iyk6cBA4C9pjqfawiuvl4HJwAyC74A6Mypc0nPAp8C+kgolXQrcAZwo6UvgxHA541VyLg8ALYF3wv/7D6U1yBrwEfrOOeeSzq9cnHPOJZ0nF+ecc0nnycU551zSeXJxzjmXdJ5cnHPOJZ0nF5c2kkzSvRHLv5F0a5L2/aSkM5OxryqOc1ZYIfn9GJ/dHVYfvns39jsw07ugS9qym9udtjvFWHf3eC49PLm4dCoBfiypfboDiSQpuxrNLwV+YWbHxvjsZ8DBZnb9boQxEKhWclGgLvyfPg2o65W+XRXqwj9EV3+VEgzguzb6g+grj4rfWiUdI+kDSS9Kmi/pDkkXSPpC0gxJe0Xs5gRJH4XtTgm3zw6vKPLDOTN+FrHf9yU9SzC4MDqe88L9z5R0Z7juFuAI4KHoqxNJY4DmwOeSzpHUQdJ/wuPmSxoathskaWJYDHOipH0l5QC3AeeEA+nOkXSrpN9E7H+mpF7ha46kfxIMjOwu6SRJn0qaLOmlsE4d4Z/V7PC874lxjkdHDNydIqlluP76iD+vP8b6i6ysjaSfhuumSXpK0hBgOHB3eJy9wtc4SZPCv69+4ba9w/PIl3R7rOO6DGZm/vJXWl7AFiAXWAK0An4D3Bp+9iRwZmTb8OcxwEagM9AEWA78MfzsGuDvEduPI/gFqi9BbbCmwBXAzWGbJkABQRHHYwiKUfaOEWcXgrIpHQgKWP4POC38bALB3Cgxzy/i/bPAEeH7HgTlfQjPv1H4/gTgP+H7EcADEdvfCvwmYnkm0Ct8lQODw/XtgQ+B5uHyDcAtQFtgHt8MnG4dI943gKHh+xbhuZ5E8AuAwj/L/wJHRf2dxGwD7Bces33Yrm0lf7fvAX3D94cRlAmCoKzLT8P3V0X+efor81+NcC6NzGyzpH8TTGC1PcHN8i0ssS5pIVBRMn4GEHl76kUzKwe+lLQI6EfwRXhAxFVRK4LksxP4wswWxzjeocAEMysKj/kMwZfnawnGC0HiGCCpYjk3vDJoBYyW1BcwoHE19llhqZl9Fr4fTHDL6ZPwWDkEJUY2AzuAxyS9SZAAon0C/C08v1fMrFDSSQR/ZlPCNi0I/rw+jNiusjYHAi+b2VoAM4ueh6Wi+vcQ4KWIP5sm4c+hwBnh+6eAO6v8k3AZw5OLywR/J7il80TEulLC27YKvnVyIj4riXhfHrFczrf/TUfXNjKC365/aWbjIz+QdAzBlUssqmR9dWQBh5vZtxKopPuB983sdAXz90yoZPuv/zxCkVMTR8Yt4B0zOy96B5IGERSqPBe4Gjgu8nMzuyNMPD8EPpN0Qri/v5rZw3HOLWYbSSP57t9BtCxgo5kNrORzr09VR/kzF5d24W+0LxI8HK+wBDgkfH8qu/cb/VmSssLnMH0IbtGMB36uYFoDJO2jqif9+hw4WlL78GH/ecAH1YzlbYIvdMLjVnyZtiK4tQfBrbAKxQQFDCssISzxL+lgglt5sXwGDJW0d9h2j/AcWwCtzGws8CuCDgPfImkvM5thZncS3C7sR/DndUnEc5uukqIn46qszXvA2ZLahevbRp+bBXMVLZZ0VthGkg4M233CN1MwX1DJ+boM5cnFZYp7CZ4XVHiU4Av9C4L78JVdVcQzjyAJvAVcaWY7CKZbng1MljQTeJgqruDDW3A3Ae8D04DJZlbdsu4jgbzw4fZs4Mpw/V3AXyV9QjCffYX3CW6jTZV0DsH8Pm0lTQV+TjC/eqxYiwiS1HMKqjd/RpAkWgL/Ddd9QIxOFMCvwo4C0whuUb5lwSyVzwKfSppBUFE5MulRWRszmwX8Gfgg3GfFFBLPA9eHnQb2Ikgcl4ZtZhH8MgHBM7SrJOUTJGFXh3hVZOecc0nnVy7OOeeSzpOLc865pPPk4pxzLuk8uTjnnEs6Ty7OOeeSzpOLc865pPPk4pxzLuk8uTjnnEu6/w9Qg4kLJpPoCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#forest = RandomForestRegressor(n_estimators=242, n_jobs=-1, random_state=0)\n",
    "start=time.time()\n",
    "forest = ExtraTreesRegressor(n_estimators=250, \n",
    "                             n_jobs=-1, \n",
    "                             max_depth = 15, \n",
    "                             max_features = 2,\n",
    "                             random_state=0)\n",
    "\n",
    "forest.fit(trainDataset_X, trainDataset_y)\n",
    "M_FitTime = time.time() - start\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Measured Fit Time: \", M_FitTime)\n",
    "\n",
    "UseTScv=True\n",
    "if UseTScv:\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    #start=time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    #scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=tscv, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )        \n",
    "else:\n",
    "    from sklearn.model_selection import KFold\n",
    "    #start=time.time()\n",
    "    kfolds = KFold(n_splits=5,shuffle=False,random_state=0)\n",
    "    #scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=kfolds, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )\n",
    "    \n",
    "#XValidTime = time.time() - start\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#print(\"Cross Validation Performance: \")\n",
    "#print(\"Cross Validation Time: %0.2f\" % (XValidTime))\n",
    "#EV=scores['test_explained_variance'].mean()\n",
    "#print(scores['test_explained_variance'])\n",
    "#print(\"EV: %0.2f\" % (EV))\n",
    "##MAE is less sensitive to outliers, The contant value that minimizes the MAE is the median of the target values\n",
    "#print(-1*scores['test_neg_mean_absolute_error'])\n",
    "#MAE=-1*scores['test_neg_mean_absolute_error'].mean()\n",
    "#print(\"MAE: %0.2f\" % (MAE))\n",
    "##MAE considers outliers, The contant value that minimizes the MSE is the mean of the target values\n",
    "##If you think your outliers are erros in the data use MAE, if you think the outliers are true datapoints use MSE.\n",
    "##It is easier to optmize MSE than RMSE because RMSE requires an adjustable learning rate.\n",
    "#print(-1*scores['test_neg_mean_squared_error'])\n",
    "#MSE=-1*scores['test_neg_mean_squared_error'].mean()\n",
    "#print(\"MSE: %0.2f\" % (MSE))\n",
    "#print(np.sqrt(-1*scores['test_neg_mean_squared_error']))\n",
    "#RMSE=np.sqrt(-1*scores['test_neg_mean_squared_error'].mean())\n",
    "#print(\"RMSE: %0.2f\" % (RMSE))\n",
    "##Optimizing R2 and optimizing for MSE is the same, since R2 = 1-(MSE/Constant)\n",
    "#print(\"XV R2 Actuals:\",scores['test_r2'])\n",
    "#R2=scores['test_r2'].mean()\n",
    "#print(\"Cross Validation R2: %0.2f\" % (R2))\n",
    "#print(\"XVR_fit_time Actuals: \", (scores['fit_time']))\n",
    "#XVR_FT=scores['fit_time'].mean()\n",
    "#print(\"XVR_fit_time: %0.2f\" % (XVR_FT))\n",
    "#print(\"score_time Actuals: \", (scores['score_time']))      \n",
    "#ST=scores['score_time'].mean()\n",
    "#print(\"score_time: %0.2f\" % (ST))\n",
    "Params=forest.get_params(deep=True)\n",
    "#print(\" \")\n",
    "#print(\" \")\n",
    "print(\"Model Parameters: \")\n",
    "print(Params)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "d=[]\n",
    "for f in range(trainDataset_X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], trainDataset_X.columns[indices[f]], importances[indices[f]]))\n",
    "    d.append({'Feature': trainDataset_X.columns[indices[f]]})\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "selector = RFECV(forest, step=1, cv=tscv, min_features_to_select=5,n_jobs=-1,verbose=0)\n",
    "selector = selector.fit(trainDataset_X, trainDataset_y)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "print(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "print(\"Cross Validation scores:\")\n",
    "print(selector.grid_scores_)\n",
    "\n",
    "trainDataset_XS=pd.DataFrame(trainDataset_X[trainDataset_X.columns[selector.support_]])\n",
    "#print(trainDataset_XS.head(3))\n",
    "\n",
    "FI=[]\n",
    "importancesS = selector.estimator_.feature_importances_\n",
    "indicesS = np.argsort(importancesS)[::-1]\n",
    "for f in range(trainDataset_XS.shape[1]):\n",
    "    #print(\"%d. feature %d %s (%f)\" % (f + 1, indicesS[f], trainDataset_XS.columns[indicesS[f]], importancesS[indicesS[f]]))\n",
    "    FI.append({'Rank':f+1, 'Feature':trainDataset_XS.columns[indicesS[f]], 'Importance':importancesS[indicesS[f]]})\n",
    "\t\n",
    "Feat_Imp = pd.DataFrame(FI)\n",
    "print(Feat_Imp)\n",
    "#print(flow_variables)\n",
    "print(trainDataset_X.shape)\n",
    "print(trainDataset_XS.shape)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(trainDataset_X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(trainDataset_X.shape[1]), trainDataset_X.columns[indices],rotation=90)\n",
    "plt.xlim([-1, trainDataset_X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Optimal number of features :', selector.n_features_)\n",
    "print('Best features :', trainDataset_X.columns[selector.support_])\n",
    "print('Original features :', trainDataset_X.columns)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score \\n of number of selected features\")\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "plt.show()\n",
    "plt.savefig('C:\\\\Benchmarking\\\\FeatureImportance.png')\n",
    "output_table = trainDataset_XS.join(trainDataset_y, lsuffix='_X', rsuffix='_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection \n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T13:20:39.322423Z",
     "start_time": "2019-05-27T13:18:08.605803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-208c08eb11f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeature_selection_univariate_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutual_info_regression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_selected_features_univariate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selection_univariate_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataset_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainDataset_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reduced data set shape is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_selected_features_univariate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_func_ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\mutual_info_.py\u001b[0m in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \"\"\"\n\u001b[0;32m    369\u001b[0m     return _estimate_mi(X, y, discrete_features, False, n_neighbors,\n\u001b[1;32m--> 370\u001b[1;33m                         copy, random_state)\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\mutual_info_.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[1;32m--> 289\u001b[1;33m           x, discrete_feature in moves.zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\mutual_info_.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n\u001b[1;32m--> 289\u001b[1;33m           x, discrete_feature in moves.zip(_iterate_columns(X), discrete_mask)]\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\mutual_info_.py\u001b[0m in \u001b[0;36m_compute_mi\u001b[1;34m(x, y, x_discrete, y_discrete, n_neighbors)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_compute_mi_cc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\feature_selection\\mutual_info_.py\u001b[0m in \u001b[0;36m_compute_mi_cc\u001b[1;34m(x, y, n_neighbors)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradius_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mradius\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mnx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mradius_neighbors\u001b[1;34m(self, X, radius, return_distance)\u001b[0m\n\u001b[0;32m    769\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind_neighbor\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m                 \u001b[0mneigh_ind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind_neighbor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m                     \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "feature_selection_univariate_model = SelectKBest(mutual_info_regression, k=4)\n",
    "X_selected_features_univariate = feature_selection_univariate_model.fit_transform(trainDataset_X,trainDataset_y)\n",
    "print(\"Reduced data set shape is \",X_selected_features_univariate.shape)\n",
    "\n",
    "mask = feature_selection_univariate_model.get_support() #list of booleans\n",
    "print(\"Selected features = \",data.feature_names[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "We now apply principal component analysis. Since we need to decide how many component to select and for this purpose we apply PCA and plot the explained variance ratio and the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "full_pca_model = PCA()\n",
    "full_fitted_model = full_pca_model.fit(trainDataset_X)\n",
    "print(full_fitted_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(full_fitted_model.explained_variance_ratio_.cumsum(), '--o');\n",
    "plt.xticks(np.arange(0,12,1));\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can note, four components can actually explain most of the variance in the data so we apply PCA and select the first four components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_pca_model = PCA(n_components=4)\n",
    "fitted_model = feature_selection_pca_model.fit(trainDataset_X)\n",
    "\n",
    "X_selected_features_pca = fitted_model.transform(trainDataset_X)\n",
    "print(\"Explained Variance: %s\" % fitted_model.explained_variance_ratio_)\n",
    "print(\"Reduced data set shape is \", X_selected_features_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:02:44.757032Z",
     "start_time": "2019-04-27T15:02:44.740031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "5\n",
      "0.16\n",
      "0.14\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "LearningRate = [0.14, 0.15, 0.16, 0.13, 0.13]\n",
    "EvalMetric = ['RMSE','R2']\n",
    "max_depth= [4,5,7,8]\n",
    "print(type(LearningRate))\n",
    "print(len(LearningRate))\n",
    "print(LearningRate[int((len(LearningRate)/2)-0.5)])\n",
    "print(LearningRate[0])\n",
    "print(LearningRate[len(LearningRate)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T13:22:45.708806Z",
     "start_time": "2019-05-14T13:22:45.688805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'EvalMetric': 'R2', 'LR': 0.1433033736034484, 'Max_depth': 4}\n",
      "R2\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# Create the domain space\n",
    "#LearningRate = hp.uniform('LearningRate', 0.14, 0.16)\n",
    "#EvalMetric = hp.choice('EvalMetric',['RMSE','R2','MAE'])\n",
    "#max_depth=hp.choice('max_depth', [4,5])\n",
    "#print(type(EvalMetric))\n",
    "space={'LR': hp.uniform('LearningRate', 0.14, 0.16),\n",
    "       'EvalMetric': hp.choice('EvalMetric',['RMSE','R2','MAE']),\n",
    "       'Max_depth': hp.choice('max_depth', [4,5])\n",
    "      }\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "samples=[]\n",
    "print(type(samples))\n",
    "for _ in range(5):\n",
    "    samples.append(sample(space))\n",
    "print(samples[0])\n",
    "print(samples[0]['EvalMetric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T21:34:53.578712Z",
     "start_time": "2019-04-25T21:34:53.568712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RMSE': 4}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={}\n",
    "?a\n",
    "a['RMSE']=4\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Feature importances and Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns:\n",
      "Index(['StoreType', 'AssortmentType'], dtype='object')\n",
      " \n",
      " \n",
      "Head of Training Data:\n",
      "   StoreID  IsHoliday  IsOpen  HasPromotions  StoreType  AssortmentType  \\\n",
      "0     1000          0       1              0          0               0   \n",
      "1     1000          0       1              0          0               0   \n",
      "2     1000          0       1              0          0               0   \n",
      "\n",
      "   NearestCompetitor  Region  NumberOfSales  Region_AreaKM2  Region_GDP  \\\n",
      "0                326       7           5676            9643       17130   \n",
      "1                326       7           8111            9643       17130   \n",
      "2                326       7           8300            9643       17130   \n",
      "\n",
      "   Region_PopulationK  Year  Month (number)  Week  Day of year  Day of month  \\\n",
      "0                2770  2016               3    10           61             1   \n",
      "1                2770  2016               3    10           62             2   \n",
      "2                2770  2016               3    10           64             4   \n",
      "\n",
      "   Day of week (number)  \n",
      "0                     3  \n",
      "1                     4  \n",
      "2                     6  \n",
      " \n",
      " \n",
      "Train Data Type Descriptions:\n",
      "StoreID                 int32\n",
      "IsHoliday               int32\n",
      "IsOpen                  int32\n",
      "HasPromotions           int32\n",
      "StoreType                int8\n",
      "AssortmentType           int8\n",
      "NearestCompetitor       int32\n",
      "Region                  int32\n",
      "NumberOfSales           int32\n",
      "Region_AreaKM2          int32\n",
      "Region_GDP              int32\n",
      "Region_PopulationK      int32\n",
      "Year                    int32\n",
      "Month (number)          int32\n",
      "Week                    int32\n",
      "Day of year             int32\n",
      "Day of month            int32\n",
      "Day of week (number)    int32\n",
      "dtype: object\n",
      " \n",
      " \n",
      "Feature Columns:\n",
      "Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Measured Fit Time:  1.0066440105438232\n",
      "Using Time Series Cross Validation\n",
      " \n",
      " \n",
      "Cross Validation Performance: \n",
      "Cross Validation Time: 8.94\n",
      "[0.65366408 0.50842293 0.47459996 0.61972558 0.39699894]\n",
      "EV: 0.53\n",
      "[1049.49583483 1371.75359231 1321.84140112 1028.53560687 1305.02858933]\n",
      "MAE: 1215.33\n",
      "[2457903.34035326 4386336.06559434 3677100.90866691 2740695.37281478\n",
      " 5999530.01001014]\n",
      "MSE: 3852313.14\n",
      "[1567.77018097 2094.35815122 1917.57683253 1655.50456744 2449.3938046 ]\n",
      "RMSE: 1962.73\n",
      "XV R2 Actuals: [0.59961468 0.50834681 0.47195162 0.61962855 0.39681197]\n",
      "Cross Validation R2: 0.52\n",
      "XVR_fit_time Actuals:  [1.7638638  0.40660071 0.53040123 0.73700309 0.96720171]\n",
      "XVR_fit_time: 0.88\n",
      "score_time Actuals:  [0.45340085 0.43680072 0.45240045 0.45240092 0.43780088]\n",
      "score_time: 0.45\n",
      " \n",
      " \n",
      "Converting Validation Categorical Columns to Numbers:\n",
      " \n",
      " \n",
      "Head of Validation Data:\n",
      "   StoreID  IsHoliday  IsOpen  HasPromotions  StoreType  AssortmentType  \\\n",
      "0     1145          0       1              0          0               1   \n",
      "1     1145          0       1              0          0               1   \n",
      "2     1145          0       1              0          0               1   \n",
      "\n",
      "   NearestCompetitor  Region  NumberOfSales  Region_AreaKM2  Region_GDP  \\\n",
      "0                838      10           5605            7215       11849   \n",
      "1                838      10           6635            7215       11849   \n",
      "2                838      10           5981            7215       11849   \n",
      "\n",
      "   Region_PopulationK  Year  Month (number)  Week  Day of year  Day of month  \\\n",
      "0                1293  2016               3    10           61             1   \n",
      "1                1293  2016               3    10           62             2   \n",
      "2                1293  2016               3    10           64             4   \n",
      "\n",
      "   Day of week (number)  \n",
      "0                     3  \n",
      "1                     4  \n",
      "2                     6  \n",
      " \n",
      "Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n",
      " \n",
      " \n",
      "Prediction Time:  0.10920000076293945\n",
      " \n",
      "Validation Performance with ~20.000 records:\n",
      "Validation Set Explained Variance (EV): 0.60\n",
      "MAE: 1109.07\n",
      "MSE: 2394670.59\n",
      "RMSE: 1547.47\n",
      "Validation Set R2: 0.60\n",
      " \n",
      " \n",
      "Feature Importances:\n",
      "1. feature 6 IsOpen (0.466111)\n",
      "2. feature 13 StoreID (0.081406)\n",
      "3. feature 4 HasPromotions (0.070828)\n",
      "4. feature 8 NearestCompetitor (0.066025)\n",
      "5. feature 0 AssortmentType (0.051350)\n",
      "6. feature 2 Day of week (number) (0.046155)\n",
      "7. feature 11 Region_GDP (0.045231)\n",
      "8. feature 14 StoreType (0.027476)\n",
      "9. feature 12 Region_PopulationK (0.024956)\n",
      "10. feature 9 Region (0.021080)\n",
      "11. feature 1 Day of month (0.020623)\n",
      "12. feature 10 Region_AreaKM2 (0.019297)\n",
      "13. feature 3 Day of year (0.017966)\n",
      "14. feature 15 Week (0.016538)\n",
      "15. feature 7 Month (number) (0.014597)\n",
      "16. feature 16 Year (0.008076)\n",
      "17. feature 5 IsHoliday (0.002285)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAFuCAYAAAB5kb4YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XmYZVV19/HvDwSJCBKFOIGAE4oDKoNjtKOYiAo4vmrEeYxxignRqHGMiUOicVZUQFFR0aioKA7YzqggICKiiCiIRogTzoLr/WOfS1+K6u6q6qZ3nVvfz/PU033OuVW1TtW9dc86e++1UlVIkiRJktTDZr0DkCRJkiStXCalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSpCVK8oYk/9o7DkmSxiz2KZUkbWpJzgauDlw8tfuGVXXeBnzNVcDbq2rHDYtunJIcDpxbVc/uHYskSYvhSKkkqZf9q+rKUx9LTkg3hiRX6Pn9N0SSzXvHIEnSUpmUSpKWlSS3SfLFJD9PcsowAjo59ogkpye5MMlZSR437N8a+ChwrSS/Gj6uleTwJP829fmrkpw7tX12kqcn+Trw6yRXGD7vfUnOT/K9JE9eR6yXfP3J107yz0l+kuRHSe6V5O5Jvp3kp0meOfW5z0vy3iTvHs7na0n2mDp+4ySrh5/DaUkOmPN9X5/kmCS/Bh4FPBj45+HcPzQ87hlJvjt8/W8muffU13h4ks8n+c8kPxvOdb+p41dNcliS84bjH5g6ds8kJw+xfTHJzaeOPT3JD4fveUaSuyzg1y5JWsFMSiVJy0aSawMfAf4NuCrwT8D7kuwwPOQnwD2BbYFHAK9Icquq+jWwH3DeEkZeHwTcA9gO+BPwIeAU4NrAXYCnJvmbBX6tawBbDZ/7HOBNwEHAnsBfAs9Jct2pxx8IHDWc6zuBDyTZIskWQxwfB/4CeBLwjiS7TX3u3wIvArYB3ga8A3jpcO77D4/57vB9rwI8H3h7kmtOfY1bA2cA2wMvBd6SJMOxI4ArATcZYngFQJJbAYcCjwOuBrwRODrJFYf4ngjsXVXbAH8DnL3An50kaYUyKZUk9fKBYaTt51OjcAcBx1TVMVX1p6r6BHACcHeAqvpIVX23ms/Qkra/3MA4XlVV51TVb4G9gR2q6gVV9YeqOouWWD5wgV/rj8CLquqPwLtoyd4rq+rCqjoNOA24+dTjT6yq9w6Pfzktob3N8HFl4MVDHMcBH6Yl0BMfrKovDD+n380XTFUdVVXnDY95N/AdYJ+ph3y/qt5UVRcDbwWuCVx9SFz3Ax5fVT+rqj8OP2+AxwBvrKovV9XFVfVW4PdDzBcDVwR2T7JFVZ1dVd9d4M9OkrRCmZRKknq5V1VtN3zca9i3M3D/qWT158AdaMkSSfZLcvwwFfbntGR1+w2M45yp/+9MmwI8/f2fSSvKtBD/NyR4AL8d/v3fqeO/pSWbl/neVfUn4FzgWsPHOcO+ie/TRmDni3teSR46Nc3258BNufTP68dT3/83w3+vDOwE/LSqfjbPl90Z+Mc5P6OdgGtV1ZnAU4HnAT9J8q4k11pfnJKklc2kVJK0nJwDHDGVrG5XVVtX1YuTXBF4H/CfwNWrajvgGGAy3XS+cvK/pk1BnbjGPI+Z/rxzgO/N+f7bVNXdN/jM5rfT5D9JNgN2BM4bPnYa9k1cB/jhWuK+zHaSnWmjvE8Erjb8vL7Bmp/XupwDXDXJdms59qI5P6MrVdWRAFX1zqq6Ay15LeAlC/h+kqQVzKRUkrScvB3YP8nfJNk8yVZDAaEdgS1pU0PPBy4aivL89dTn/i9wtSRXmdp3MnD3oWjPNWijeOvyFeCXQ7GePxtiuGmSvTfaGV7anknuk1b596m0abDHA1+mJdT/PKwxXQXsT5sSvDb/C0yvV92alhSeD61IFG2kdL2q6ke0wlGvS/LnQwx3HA6/CXh8klun2TrJPZJsk2S3JHcebiD8jjYyfPFavo0kSYBJqSRpGamqc2jFf55JS6bOAQ4GNquqC4EnA+8BfkYr9HP01Od+CzgSOGuYVnotWrGeU2jFdj4OvHs93/9iWvJ3C+B7wAXAm2mFgi4PHwQeQDufhwD3GdZv/gE4gLau8wLgdcBDh3Ncm7fQ1nL+PMkHquqbwH8BX6IlrDcDvrCI2B5CWyP7LVqBqacCVNUJtHWlrxniPhN4+PA5VwRePMT8Y1qBpGciSdI6pGq+2U6SJOnylOR5wPWr6qDesUiS1JMjpZIkSZKkbkxKJUmSJEndOH1XkiRJktSNI6WSJEmSpG5MSiVJkiRJ3Vyh1zfefvvta5dddun17SVJkiRJl6MTTzzxgqraYX2P65aU7rLLLpxwwgm9vr0kSZIk6XKU5PsLeZzTdyVJkiRJ3ZiUSpIkSZK6MSmVJEmSJHVjUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbq7QO4BNLukdwbpV9Y5AkiRJkjYZR0olSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSJEmS1I1JqSRJkiSpG5NSSZIkSVI3JqWSJEmSpG5MSiVJkiRJ3ZiUSpIkSZK6MSmVJEmSJHVjUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbkxKJUmSJEndmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd0sKClNcrckZyQ5M8kz1vG4+yWpJHttvBAlSZIkSbNqvUlpks2B1wL7AbsDD0qy+zyP2wZ4MvDljR2kJEmSJGk2LWSkdB/gzKo6q6r+ALwLOHCex70QeCnwu40YnyRJkiRphi0kKb02cM7U9rnDvkskuSWwU1V9eCPGJkmSJEmacQtJSjPPvrrkYLIZ8ArgH9f7hZLHJjkhyQnnn3/+wqOUJEmSJM2khSSl5wI7TW3vCJw3tb0NcFNgdZKzgdsAR89X7KiqDqmqvapqrx122GHpUUuSJEmSZsJCktKvAjdIsmuSLYEHAkdPDlbVL6pq+6rapap2AY4HDqiqEy6XiCVJkiRJM2O9SWlVXQQ8ETgWOB14T1WdluQFSQ64vAOUJEmSJM2uKyzkQVV1DHDMnH3PWctjV214WJIkSZKklWAh03clSZIkSbpcmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSJEmS1I1JqSRJkiSpG5NSSZIkSVI3JqWSJEmSpG5MSiVJkiRJ3ZiUSpIkSZK6MSmVJEmSJHVjUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbkxKJUmSJEndmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1s6CkNMndkpyR5Mwkz5jn+OOTnJrk5CSfT7L7xg9VkiRJkjRr1puUJtkceC2wH7A78KB5ks53VtXNquoWwEuBl2/0SCVJkiRJM2chI6X7AGdW1VlV9QfgXcCB0w+oql9ObW4N1MYLUZIkSZI0q66wgMdcGzhnavtc4NZzH5Tk74GnAVsCd94o0UmSJEmSZtpCRkozz77LjIRW1Wur6nrA04Fnz/uFkscmOSHJCeeff/7iIpUkSZIkzZyFJKXnAjtNbe8InLeOx78LuNd8B6rqkKraq6r22mGHHRYepSRJkiRpJi0kKf0qcIMkuybZEnggcPT0A5LcYGrzHsB3Nl6IkiRJkqRZtd41pVV1UZInAscCmwOHVtVpSV4AnFBVRwNPTLIv8EfgZ8DDLs+gJUmSJEmzYSGFjqiqY4Bj5ux7ztT/n7KR45IkSZIkrQALmb4rSZIkSdLlwqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSJEmS1I1JqSRJkiSpG5NSSZIkSVI3JqWSJEmSpG5MSiVJkiRJ3ZiUSpIkSZK6MSmVJEmSJHVjUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbkxKJUmSJEndmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSJEmS1I1JqSRJkiSpG5NSSZIkSVI3C0pKk9wtyRlJzkzyjHmOPy3JN5N8Pcmnkuy88UOVJEmSJM2a9SalSTYHXgvsB+wOPCjJ7nMedhKwV1XdHHgv8NKNHagkSZIkafYsZKR0H+DMqjqrqv4AvAs4cPoBVfXpqvrNsHk8sOPGDVOSJEmSNIsWkpReGzhnavvcYd/aPAr46HwHkjw2yQlJTjj//PMXHqUkSZIkaSYtJCnNPPtq3gcmBwF7AS+b73hVHVJVe1XVXjvssMPCo5QkSZIkzaQrLOAx5wI7TW3vCJw390FJ9gWeBdypqn6/ccKTJEmSJM2yhYyUfhW4QZJdk2wJPBA4evoBSW4JvBE4oKp+svHDlCRJkiTNovUmpVV1EfBE4FjgdOA9VXVakhckOWB42MuAKwNHJTk5ydFr+XKSJEmSJF1iIdN3qapjgGPm7HvO1P/33chxSZIkSZJWgIVM35UkSZIk6XJhUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbkxKJUmSJEndmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktSNSakkSZIkqRuTUkmSJElSNyalkiRJkqRuTEolSZIkSd2YlEqSJEmSujEplSRJkiR1Y1IqSZIkSerGpFSSJEmS1I1JqSRJkiSpG5NSSZIkSVI3JqWSJEmSpG5MSiVJkiRJ3ZiUSpIkSZK6MSmVJEmSJHVjUipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JJkiRJUjcmpZIkSZKkbkxKJUmSJEndmJRKkiRJkroxKZUkSZIkdWNSKkmSJEnqxqRUkiRJktTNgpLSJHdLckaSM5M8Y57jd0zytSQXJbnfxg9TkiRJkjSL1puUJtkceC2wH7A78KAku8952A+AhwPv3NgBSpIkSZJm1xUW8Jh9gDOr6iyAJO8CDgS+OXlAVZ09HPvT5RCjJEmSJGlGLWT67rWBc6a2zx32aQasWrWKVatW9Q5DkiRJ0gq1kKQ08+yrpXyzJI9NckKSE84///ylfAlJkiRJ0gxZSFJ6LrDT1PaOwHlL+WZVdUhV7VVVe+2www5L+RKSJEmSpBmykKT0q8ANkuyaZEvggcDRl29YkiRJkqSVYL1JaVVdBDwROBY4HXhPVZ2W5AVJDgBIsneSc4H7A29MctrlGbQkSZIkaTYspPouVXUMcMycfc+Z+v9XadN6JUmSJElasIVM35UkSZIk6XJhUipJkiRJ6sakVJIkSZLUzYLWlGqZyXytY5fR16wltbGVJEmStAI5UipJkiRJ6sakVJIkSZLUjUmpJEmSJKkbk1JpmVi1ahWrVq3qHYYkSZK0SVnoSP1cHgWbNiYLNi3JJLFevXp11zgkSZI0Do6USpIkSZK6caRU2lAbe8TX9jySJElaQRwplSRJkiR140jpCre6dwCSJEmSVjSTUmmZWN07AEmSJKkDp+9K0oyyzZAkSRoDk1JJkiRJUjdO35V0+fSMHXkVYfutSpIkbRqOlEqSJEmSujEplSRJkiR14/RdSbNjFqYhz8I5SJIkLYIjpZIkSZKkbkxKJUmSJEndOH1XkuaxuncAkiRJK4QjpZIkSZKkbkxKJUm6nK1ateqS3reSJOnSnL4raaNa3TsAXWJ17wA2kkkyt3r16q5xSJKky4cjpZIkaUEc8ZUkXR4cKZUkbXz2W5UkSQtkUipJ0tps7OTaxFqSpMswKZUkLWurewegmeM6ZUlaXkxKJUmaZbMylXpWzmMjmZXEelbOQ9KGMSmVJEkryupe33hWEutZOY+NZFYS61k5D42TSakkSZez1b0D0MxZ3TuAjWR1z2/umnFp2TAplSRJkpZodc9v7qi1ZoRJqSRJWpDVvQOQdLlZ3TsArWib9Q5AkiRJkrRyLSgpTXK3JGckOTPJM+Y5fsUk7x6OfznJLhs7UEmSJEkzKFneH7rcrTcpTbI58FpgP2B34EFJdp/zsEcBP6uq6wOvAF6ysQOVJEmSJM2ehYyU7gOcWVVnVdUfgHcBB855zIHAW4f/vxe4S+JtBUmSJEkrQO/R3JGP9i6k0NG1gXOmts8Fbr22x1TVRUl+AVwNuGD6QUkeCzx22PxVkjOWEvQysz1zznOD9HnSzMI5wGycxyycA3ge8/M5tSFm4Txm4RzA85ifz6kNMQvnMQvnAJ7H/HxObYidF/KghSSl853B3PrOC3kMVXUIcMgCvudoJDmhqvbqHceGmIVzgNk4j1k4B/A8lpNZOAeYjfOYhXMAz2M5mYVzgNk4j1k4B/A8lpNZOIfFWMj03XOBnaa2dwTOW9tjklwBuArw040RoCRJkiRpdi0kKf0qcIMkuybZEnggcPScxxwNPGz4//2A46rslitJkiRJWrf1Tt8d1og+ETgW2Bw4tKpOS/IC4ISqOhp4C3BEkjNpI6QPvDyDXmZmYTryLJwDzMZ5zMI5gOexnMzCOcBsnMcsnAN4HsvJLJwDzMZ5zMI5gOexnMzCOSxYHNCUJEmSJPWykOm7kiRJkiRdLkxKJUmSJEndmJRKkiRJGqUkmye5b+84tGFMSqWOhj+kb+8dx8aS5Iq9Y1iqJFsluV+SVyY5Ksnbkvxzkpv0jm0lG/NzStJlDe97n+wdhy4tyZ8nuUmS6yYZVX5QVRcDT+0dx8aQ5J5j+/lvLCvypDdUkhsmeVOSjyc5bvLRO67FSHKzJPcfPm7aO56lSnK9yUVrklVJnpxku95xLdTwh3SHod3SaCXZJ8mpwHeG7T2SvLpzWAuW5HnAF4HbAV8G3gi8B7gIeHGSTyS5eb8IFy7JXyT57yQfTvIfSbbtHdNSjP05BZDkYWvZv0WSIzd1PBtiRt737pPkO0l+keSXSS5M8svecS1WkhsluUuSK8/Zf7deMS3G8L73myRX6R3Lhkqy3zz7Ht8jlqVIcpUkzxz+1h7Pmve+7w83Z/+qb4SLcmySpya5ZpJtJx+9g1qCBwLfSfLSJDfuHcymZPXdJUhyCvAG4ETg4sn+qjqxW1ALNLwJfBDYCfg6EOBmwA+AA6tqVG/QSU4G9gJ2obUtOhrYraru3jOuxUjyRuBWtNh/PdlfVS/vFtQiJTkeeADwgaq65bDvG1U1ihseSe5RVR9Zx/G/AK5TVSdswrCWJMnHaH+bPgvcE9imqh7eNaglGPtzCiDJ14A3VNUhU/u2Bj4A/KCqHtUtuEUa8/vexNC2bv+qOr13LEuV5MnA3wOnA7cAnlJVHxyOfa2qbtUzvoVK8h7gNsAnuPT73pO7BbUESb4IPLuqjhu2nw6sqqrLJKvLUZJPAG8DPlRVP59zbE/gIcCpVfWWHvEtRpJz5tldVXWdTR7MBhqS6QcBjwAKOAw4sqou7BrY5Wy9fUo1r4uq6vW9g1iiFwInAHeuqj8BDNMEXgy8CHhSx9iW4k9DL917A/9dVa9OclLvoBbpvOFjM2CbzrEs1WZV9f0k0/suXtuDl5uq+kiSzYEXV9XB8xz/CfCTTR/Zklyjqp41/P/YITEao1E/pwb7Ah9LslVVvSrJDsAxwKeq6hmdY1usMb/vTfzvmBPSwWOAPavqV0l2Ad6bZJeqeiXtJvNYfGT4GLsDgA8nORi4G3CjYd8oVNVd0/7I7gj8fM6xE2k3oUahqnbqHcPGUlW/TPI+4M9o05LvDRyc5FVVNaoZQ4thUro0H0ryBOD9wO8nO6vqp/1CWrB9gZtPElKAqvpTkmcCp/YLa8n+mORBwMOA/Yd9W3SMZ9Gq6vkASbZpm/WrziEtxTlJ9gFqSO6eBHy7c0yLUlUXJ9kzSWrcU0iS5M9Zc4G6+fT2SP5OwWw8p36aZF/go0muBRwIvL6qXtU5tKUY7ftekvsM/z0hybtpI9XT5/A/XQJbms0n7xFVdXaSVbTEdGdGlJRW1Vt7x7AxVNUFSQ4APklL4O43tvePqqokHwD27B3LhkpyI2B3YKvJvqp6Z7+IFi/J/sAjgesBRwD7VNVPklyJNkNiZpNSp+8uQZLvzbO7quq6mzyYRUpyclXdYrHHlqskuwOPB75UVUcm2RV4QFW9uHNoC5a2pvcI4KrDrguAh1bVaf2iWpxheuuraDc9QpuS9cSquqBrYIuU5L+AGwBHcekpZaO5aE1yNvAn5r9AHcXfKZiN59RUMrQN8HLgU8C7JsdH9rwa8/veYes4XFX1yE0WzAYa1vE+rapOntp3BeBQ4MFVtXm34BYhyQ2A/+CyCcSyfz4BJLmQNq1yYktaDYKiPadGtZYxyWuBw6vqq71jWaokzwb+mjZafSzwN8Dnq+o+6/zEZSbJ24A3V9Vn5zl2l6r6VIewNgmT0hUmybdo89TnXrAGeHtVrahF1cvBsCblWVX16WF7FfDvVXW7roGtQGu5eB3VRauWj1lKhmZBkttX1RfWt285S7IjbSr1j+c5NppzSfJ54LnAK2iznB5BuyZ9btfAVqgk3wR2A86m3ZAN7W/UKAr8AQzFmm4BfK2q9khyTeCNVTWa6dQrnUnpEgxD6E+jFT557HDHb7eq+nDn0NYryWoufXfvUqpqTJXWSHJ74HnAzrTp6JM/pKO42wqtgEhV7bG+fcvZsLbpFcBth11fAP6xqs7uFNKKNoyc7Ee7YwzwTeDYqrqoX1SL43NqeUmyBfB3wB2HXatpF3x/7BbUIs1XCGhMxYEAklx1XcfHMJ0aIMmJVbVnklOr6mbDvs9V1V/2jm0xhvWYDwZ2raoXJtkJuGZVfaVzaIsyTP++jKr6/qaOZamSfKWq9klyIrAK+BWtSNNoiuMBJLkNbYrujWkj8JsDvx7b6PtSuKZ0aQ6jrR2YjGSdS5vut+yT0qpa1TuGjewtwD8wpyLkyJyV5F9pU3gBDgLmmyq3nB0JHEKrlgrwt8O+2671M5ahJDcEXg9cvapumtYG5oCq+rfOoS3YsHbx08CPgJNoN2ruCbw8yV9V1Xk941uEmXhOwSW9Vu9LqxJ+yftuVb2gV0xL8Hraev3XDdsPGfY9ultEC5TktrT36x2SPG3q0La0C74xuYB2zTG5wTQ966mAsdyQ/d1QZPE7SZ4I/BD4i84xLcXraMsl7kwrJPkr4LXA3j2DWqyhqNwdgBtU1WFDUbYrr+/zlpmT0loCHkor6PlLYIyF/l5DawtzFK27xEOB63eNaBNxpHQJkpxQVXslOanWtCoYxcjW1BqneY1pjRNAki9X1a17x7EhhiI0zwfuMOz6LPC8mlOefTmb7/cwxt9Nks8AB9NGgMbahuRw4OSq+u85+59Mq9o5b+/M5WZWnlNwSZueX3DZdir/1S2oRRrzjI4kd6KNnDye1tZm4kJaK4zv9IhrKZK8knYuX6DdpPn82ArrACTZm1a0ZTtaMrct8LKqOr5rYIs0GWkf4/XgtCTPpSVAu1XVDYebm0dV1e07h7YkSa4PbFtVo0tKp3KMr0+mTyf54kpY0uVI6dL8IcmfMUyDTXI9pir5LXP7r+NYAaNKSoFPJ3kZLe7paopj+kO0b83pzZbk/rS7ZGNxXJJ/ohVxKdro1ocyNK6u8fS/vVJVfSWXbkMymimvg9vUPH1Jq7UkOaNDPEs1K88pgB2r6m69g9hAFye5XlV9FyDJdRnJ7JSq+gzwmSSHj2k64nyq6inDlNFVtNHqVyf5OK2q82hm2EwK6qQVO39E73g2wB/TqoNPrgd3oI2cjs29gVsyjCxW1XlpHQFGJckDgetV1YuS7JRkzxpRL+XBb5JsCZyc5KW0WU9bd45pkzApXZrnAh8DdkryDuD2wMO7RrRAI//jP5/JqMleU/uKNpVmLP6Fyyag8+1bzg4a/n3KnP2Po/0+xtK8+oLhJtPkAuN+tDeEMfntOo79ZpNFseFm5TkF8MUkN6uqMbbdmjiYdhPwLNqU0Z1pxWnG5IpJDuGy06jH9H7BMDL66bSe3A+kjTR+B3hT18AWYZhS/RbaFNHrJNkDeFxVPaFvZIv2KlqbpKsneRFwP+DZfUNakj9UVSWZvPeNLglK8hraEoM7Ai+iFWx6AyObSk272bQ58ETa8rSdaMs/Zp7Td5coydWA29DenI8fU5sCgCRXB/4duFZV7ZfWWuW2VfWWzqGtGEn2A+4O/D/g3VOHtgV2r6p9ugS2gg2jP4fQ1p/9jLa298FjGl0ZkoZ/mu8Q8NKqut4mDmnFGypbXp/2fPo9I6xsCZesjd2NFv+3qmosM4SANq2SdpE6dxr1aEZShmThQNrMgR1os4TeXVXndA1skZJ8mZbAHT3WpRITab0x70J7XXyqqk7vHNKiDbNSbgDcldaq55HAO6tqND0xZ2Uq9UrmSOnS3Ym2BrBod2be3zecRTucVrDpWcP2t2mJ0aiS0iRXoY1cTypCfgZ4QVX9ol9UC3YebTH+AbSLpIkLaXfHRiPJ8bTiAkdW1YW941mqqjoL2He48NtspOfyGdY+Tf8yfc+Wq1l5Tg326x3AUiW5c1UdN089guslGVsdgouq6vW9g9hAP6GNih4JnEm7Btl7WKM5qt9HVZ0zZ6nEKKaDz2N74DeTAkFJdh3TVGqAqvrPJHelFQe6IfCcqvpE57AW649D8azJaO/VGNFU6rSWNuvqjjGqm5hLYVK6BEleR7vrfeSw63FJ9q2qv+8Y1mJtX1XvSfIvAFV1UZIxviEcCnyDNtoIbdrDYcCyb5ZcVacApyR5x5hadawXQeX2AAAgAElEQVTFw2lT+U5J67t6WI2wwfPwJvZchhtOab30XlBV/9c3soWboSn6D2cGnlNwSWXLPYBJu4vPDa//MbgTcBzz3+gYWx2CDyV5Au0m8nQNglG0URkcRfu534g1LZ8mxvT7OCfJ7Wh/Z7cEnkwrfDQq0wWCaNceWwBvpy3rGptTgUm9lDEuNXgt8D5ale3n064Ln983pEW55/DvJJeYdGR4MONaerNkTt9dgiSnATedVLwb7sycWlU36RvZwqX1K70v8IlhusNtgJdU1Z36RrY4SU6uqlusb99ylOQ9VfX/1nZ3bIx3xYaCDwfQSpr/gXbT4NVjqSSc5BO00cS3D7seDKyqqn37RbU4SXYEdqmqzw/bT2NNaf93VtWZ3YJbgrE/pwCSPAV4DGsShnsDh4xpatwsSDLf6FXVuPpaX2VtM4GS7D0pILTcJdkeeCWwL23a68eBp4zpBiC06w2GAkFTU0a/Prb37ySPBp5DuwEV2s2oF1TVoV0DW4AkxwBPqKqzk9yENc+pT1bVN/pGt3hJvjC36vF8+2aRI6VLcwatyMZkndlOwNf7hbMkTwOOpk3B+gJtbcr9+oa0JL9NcoepC/Dbs+5CL8vJpIDLPdf5qJEY1iU/gjai8kHgHbQRx+OAsTSnv2pVvXBq+9+S3KtbNEvzMtrPfuJxtHWyV6LdNX5wj6CWYkaeUwCPAm5dVb8GSPIS4Eu0BumjkEv395z4BXBiVZ28qeNZiqratXcMG8Gnkty1qn42vXOYenko7XpkDP5UVaP5W7QOoy8QNDgYuOXkpsAwa+iLtOfUcnc48PEkb6XVTTitczwbaus517W3w+q7WoerAacn+cqwvTfwpSRHA1TVAd0iW4BhZHcr2p2wSdGKM6rqj10DW5q/A946rC0N8FPGUwl5UtX1CVX19Oljw0Xr0y/7WcvTULTit7Q3sOdU1eTGwBeGGwVj8emhpPx7hu37AR/pGM9S7FZVH57a/k0N/TCTfK5TTIs2Q88paH+bppdHXDzsG5O9ho8PDdv3AL4KPD7JUVX10m6RLVCSLWjvGZMaBKtpPYnH9N73RtrfqbtW1fkASf6WVm30Hl0jW5wvD6OMhwIfm8w8G6H3JHkjsF2Sx9AKBI2mCvKUc2n1LCYuBEZRPGtYivYR2kjvCUmOYGotaVW9vFtwS/Mo4NDhuhbg57Tn1cxz+u4SpDXiXquhJ9qyluRLVXXb3nFsLBln70JgTcW4OftGMf0nyX2q6n+S3LCqvt07nqVKciFtCnVodyQnb2ibAb+qqm17xbZYSb5ZVbtPbV91smZu7rHlaFaeU9OGUcaHsaYg3r2Aw6vqv/tFtThJjgXuW1W/GravDLyXNhX5xOX+vAJI8mbamr+3DrseAlxcVY/uF9XiJXkI8M/AX9Oq8D4euFtVnd0zrsVIq3C0L+1iex9aocXDx/KaT/JU4AvAScBf0X4XAY4dU4GgqRkQtwBuRpuRUrQKz1+pqsf3im0xhnXJzwD+lvZcmk5Kx7Su9BLDdW1GUrhzo3CkdGl+ANyE9sI9fajYOTYfT3Jf4H/GeIcyyUFV9fa5U8omlfzGcGcsyd8BTwCum2R6+vc2tGkzY/Bs2nNoFBcSa1NVo2sSvg4XTid0UwnpjYBfdY1sYWbiOTWtql4+rOO/A+3C9RFVdVLfqBbtOrQ1vRN/BHauqt8mGUtrmL3ntIc4Lq1NzKhU1RFJfkdLiH4A3H5sazGH645PAJ9I8le0dfxPGH4fz6iqL3UNcP12pK2JvRFt+dYXaUnqaNoLDSbvfd8dPiY+2CGWJUlyN+DltCVpt6qqURYFmoXr2g1lUroIw12LNwN7AqfQLi72SHIi8KiRjdI9jTYidHGS37Kmb95YRoQm8+vnSybGkmS/E/gorSfYM6b2XziyapAzJcnNgV2Y+vs4pjYLtOrBH05r5P61Yd+ewDNZs45Zm0CSbavql0muCpw9fEyOXXVkr/N3AscnmVys7g8cOayh+2a/sBbl4iTXq6rvwiV9iUdVdX6qMF5o68SvRpvOO6ret8OaxYNoo9X/CzyJllTcglZheFmv/62qf4JLRuj2ovW2fiTwpiQ/H8PMARjvKOIczwLuPwtrSYd/Z+km+aI4fXcRkhxOu6h4QVX9adgX4F+B61fVQ/tFtzIluX1VfWF9+5a7JHcAblCtz9n2wDY1gj5nSX5D65V3mUOM6AJpIsmhwM2B01gz/aeqalTrOZLclDa9b1IR/BvAy8ZQiXCWnlNJPlxV9xyqvk6/2U7OZTRVXwGS7Mma0d7PV9UJnUNalCR3obXtOIt2DjvTRq0/3TWwRUiy87qOV9X313V8uUjybVrLi8Oq6tw5x55eVS/pE9niDOv+bktrAXNbYDtaN4ZRteZKshctuduZS9+QHc3fW42fSekiJPlOVd1gsceWqyQHMFXwYU5xlFFYy3rMy+xbzjLV56yqbpjkWsBRYyj/ndYe6e5rOz6WC6SJMay53FiSvLqqntQ7jrlm7Tk1S+bcPNsBuPIYbp5NS3JF1hT4+1ZVjWXq8UxJkjEuHZpIcgjtpt+FwJeB44Hj51ZFHoskZ9Aq8J7Kpddj+vd2E0nyqnUdr6onb6pYenH67uKMrVriWiV5Ma1q8KR1xFOGEtTPWMenLRtJbkubLrPDnPn32wKb94lqye7N0OcMoKrOSzKW6Rt/mLE3rS8l2b2qxjIdcUMs15ses/acIsmnquou69u3nE3fPKONNm5BWwe4XJ9Hl5HW8/ZvWDM9/y5JRrlWK623+KuBGwNb0t73fj2WJThjTkgH1wGuCHwH+CGteu1oeifP4/yqOrp3ECvc2NYjb3QmpYvzhSTPAV44/Qc1yb/S7pKNyd2BW0xNQ34rrWjCKJJS2pvwlWnP4ekE7peMr9/qmPucfQEgya5zR0zm2zcCb6Ulpj8Gfs8Ip4zOgFFNvV+XJFvR1v1tn+TPWXNjc1vgWt0CW5ox3zyb+BDwO+aMBo3Ua4AH0tZf7gU8FLh+14hWkKq627B86ya0G+T/CNw0yU+BL1XVc7sGuHjPHapTf4r23geMrp7CqFXVW6e3h7+vNal4vhKYlC7Ok4C3AGem9dcqWgP3r9H6Co3NdrS+ngBXWdcDl5tqbXc+k+Twqvr+yF+8o+1zVlVPHP77PtprYdp7aQV2xuRQWuGNWbhoHaXJcyrJ1YF/B65VVfsl2R24bVW9pWuAi/M44Km0BPRE1iSlvwRe2yuoJRrzzbOJHWfpBlNVnZlk86q6GDgsybKv2p7kJVX19CT3r6qjesezIYbBiW8k+Tnwi+HjnrQWN2NLSh9BqyS8BVP1FACT0k1sqAlxBHDVtpnzgYfOQCGn9TIpXYShuu79k1wP2J12gfH0SSW/kfkP4KQkn6adxx1p1TnHZpskJ9FevCS5AHjYGAq6TFTVfya5K+1C9YbAc2okfc6GNiM3Aa6S5D5Th7YFtuoT1Qb5wQqawrTclyMcTpsm+qxh+9u0/nOjSUqr6pXAK5M8qape3TueDTTfzbM3d45psT6a5K+r6uO9A9kIfjNUfj05yUuBH7Gmeudydvckzwb+hTbKO0pJnkwbIb09rT3SF4Av0W5sntoxtKXao6pu1jsIAXAI8LRJAbYkq2gDFbfrGdSmYFK6NNcAjquqXyc5KMnfA68c0zqoqjoyrW/e3qxJrn/cN6olme/Fewjje/GeCvwZ7c7kmN7QdqPdGd6O1iJi4kLgMV0i2jDfSvJO2jS/WZ/C9MreAazH9lX1niT/AlBVFyUZVfuOiap69XD3e3embtZU1dv6RbU4c26e7caIbp5NOR54f5LNaInE2FqhTXsIsBnwROAfgJ2A+3aNaGE+BlwAbJ3klwy/A8b3u9iFNhvoH6rqR51j2RiOX0H1FJa7racrglfV6pHOTFk0q+8uQZKvA3vQWkccQbtzf5+qulPXwBZhFgpvACQ5pS7dDH3efctZkkcDzwGOo70x34nWdujQroEtQpLb1vJvdr5eSQ6bZ/foWsIAJLkhrZri3BL/d+4W1CIMN83uC3yiqm41FHZ5yZj+zk4MRYJW0ZLSY4D9aC1Vxrb+/RJD0aAHVtU71vvgZSLJWcC9aC07Rn/xk+TPgOtU1Rm9Y1msJB+sqgN7x6EmyenA9YDvYT2FrpK8n7Ys8Ihh10HAXlV1r35RbRompUswaTkyFD36YVW9ZSxtSKYKb3yadpE0XXjjo1V1406hLcksvHiHUuy3q6r/G7avBnyxqnbrG9nCDe0hHsOaqpYAjDGZmxVJTgHeQFvLeMkIY1WNosJfklvRqovelNZndQfgflX19a6BLUGSU2k3Mk+qqj2G9bJvrqr91/Op3SXZFvh74NrA0cAnhu2DgZPHlFgkORbYb1Lgb8yS7A/8J7BlVe2a5Ba0m5kHdA5twYbXwd7D5per6vye8axkWUv/2zHNAJwVQ1G857OmJ/RngeeNtd3QYjh9d2kuHKaUHQTccbhjvEXnmBZqbuGNiQsZX+ENaOuank9bjD958Y6qaTWtlPyFU9sXAud0imWpPgh8DvgkUwnQ2AwjpZe5UzfS5Pqiqnp97yCWYpheuRVt1sCkp+QZVfXHroEt3W+r6k9JLhqSvJ8A1+0d1AIdAfyMtl7u0bRkdEvgwKo6uWdgS/AjYHWSj3Lp6fmjawkDPI9WUGc1QFWdnGSXfuEsTpL705Lq1bTX96uTHFxV7+0a2MrlCNUyMSSfM9+TdD4mpUvzAOBvgUdV1Y+TXAd4WeeYFuqLwHtoIw6vTvIw2hS5s4F39gxsKWbkxftD4MtJPkh7YzgQ+Mqk/+pILpiuVFVP7x3ERvDhqf9vRWuDcV6nWDbUh5I8AXg/l74A/+naP2V5GBK4/6qq2wKzUHHwhCTb0YpVnAj8CvhK35AW7LqTAihDy4gLaFNGL1z3py1L3xs+thw+xuyiqvpF60oySs8G9q6qn8Als20+SVunqU3vI6xZ27sVsCtwBq2QoTaBJB9iHTcHxjQLYqmcvrvCJPkasG9V/TTJHYF30Vrd3AK48djWOCXZi1Y1eBcuPW10NOsghvVma1VVz99UsSxVkn+jTTk+pncsG9MwYvfJsazDnJZkvh6xVVWjGKFL8nzg68D/zML6v4lhNGvbsUxDnrs0ZSxLVWZdkrfQeko+g3Zj+cnAFlX1+K6BLVCSU6ervQ5/a0+xAuzyMCyfeFxVPa53LCtFkkm9hNBuYD56+ni1VogzzaR0EZJcyPx3MUZTNW66CFCS1wLnV9Xzhu2Tq+oWPeNbrGE95sHM6SvpOohNa3htbA38YfgYzWtiXZLsBnykqmxKv4lNPacuBn7LCJ9Tw4XdWlXV1zZVLEs1VDz+9WSTViX8N4zw9zFLklyJ1i7pr4ddxwL/VlW/6xfVwiV5Ga1Y5JHDrgcAX5+RGTczwRtQ/SQ5qapu2TuOTc3pu4tQVdv0jmEj2DzJFarqIuAuwGOnjo3x+XB+jbyv5DDa+ywuWyV1NKO9M/LamL7xNGlT8GNglBdJSbYA/o7Wgxja2q03jmVd5ow8p/5rHccKWPYj8FW1ee8YdGlDHYvnV9XBrOnjOypVdfDQ23pSzOWQqnp/57BWrMlyocFmwK0AC0/1syJHDMeYhGjDHAl8JskFtNGHzwEkuT7wi56BLdFzh3VOn2K8fSXfwTyjvWOStrDpwcCuVfXCJDsB16yqsaybA2YmEZp4Pa0A2+uG7YcM+x691s9YZpIcwFRSXVUfXtfjl5uq+qveMQiSvKSqnp7k/lV1VO94NlRVXZxkz95xbKjhfXpM79WzbPq97yLaGtP3dYplRUpy1anNzYcqvJcsGh9DPYgN5fTdFWjo93dN4ONV9eth3w2BK49hOtm0JG8HbkQrhjJJ6EbVVzLJ56vqDr3j2BBJXk/7+d+5qm48/DH9eFXtvZ5PXXaSXJvLjlp/tl9ESzP2Hr5JXkxrFzHpg/kg4MSqeka/qJYmyUPn219Vb9vUsaxEQ0ueW9HajszEdMQk/wXcADiKNdOrx3ZDVtJgqAMxmak112jqQWwIR0pXoKo6fp593+4Ry0awxwwURpiF0d5bD717T4JWFTnJ6KpbJnkJbW3TN1nT2qZorYbG5uIk16uq7wIkuS7jatdzd+AWk56SSd4KnEQr7DI20zdntqItnfgaYFK6aXyMVjV46yS/ZM30/DGvi70q8H9cegp44cijlmAYmPgnLls0ctkvMZgVVbVr7xh6MynV2B2fZPeq+mbvQDbAI2ijvVswNdrLuC4u/jiscyq4pLz/GKci3wvYrap+v95HLn8HA59Ochbt4ntnxtfDdztgMmXpKj0D2RBV9aTp7SRXofX/1CYwrL08OMkHq+rA3vFsDFV1mddyktHMTElyT+CYyU0ndXcU8AbgzYzr5uXMSXJ74OSq+nWSg2izPP67qn7QObTLndN3NWpJTgeuR+s993vW3PkeTZGguaXxxyjJg2kjjLcC3grcD3j22NZvJfkocP+q+lXvWDaGJFcEdqO9Lr41pmQ7yYOAFwOfpsV/R+CZVXXkOj9xBIYiVF+vqhv3jmWlSXJ11oxcf7mqRl3MJcnuwANp09t/UVV7dQ5pQYalN7elrVs8rKpO7xzSipbkxKoa/TrlWZDk68AetOrURwBvAe5TVXda5yfOAJNSjVqSnefbP6aWMEneBLxi5KO9JLkRbVpigE+N8SIjyftobwZzp1I/uVtQi5TkzlV13FDZ8jLGNC08yTVpCURoCcSPO4e0JHOaom8O3Bh4zxjXx45ZkvsD/0mrRB3gL4GDq+q9PeNarOF970HDx0W0WRB7VdXZPeNarCTb0s7hEbTXx2HAkVV1YdfAVqAkzwN+AryfS7/3zXxxneVm0oonyXOAH1bVW1ZKex6TUo1ekj1oFxcAn6uqU3rGs1izMNoLMBQ32olLr0cZW+Gsh823v6reuqljWaokz6+q5yY5bJ7DoykCluRTVXWX9e0bg6mm6NCSiO9X1bm94lmpkpwC3LWqfjJs7wB8cizFvwCSfJE2lf1dwLuq6jtJvjfW9WhJtgcOAp4KnA5cH3hVVb26a2ArzFBkZ64VUVxnuUnyGdo6+EfQZgidT5vOO+oZdQvhmlKNWpKnAI9hzfrLtyc5ZGRvaHfrHcCGSvJC4OHAd1kzIjSKPozTxpR8rk1VPXf4d2zrRwFIshVwJWD7OSXxtwWu1S2wDVBVn0lyDWAf2uviu51DWqk2mySkg/+j9WQck/OBHYGrAzsA32GEPQ2T7A88knZD9ghgn6r6SZIr0ZLTMb2Hj95Yb2rMqAcAfws8qqp+nOQ6wMs6x7RJOFKqURvm3t92qrXN1sCXRjjKOPbR3jOAm1XVH3rHshTD9MpDgI9V1R/nHLsuLeE+u6oO7RDeksxphj7xC1pblZM3dTwLNdxoeiotAf3h1KELgTdV1Wu6BLYBkjwaeA5wHC3JvhPwgjE9n2ZBkpfR1mlN1iU/gLa29+n9olq8oVDWfWlTX69PKwj2N2PqC53kbcCb52u3leQuVfWpDmGtOEnuUFWfX8fxbYHrVNU3NmFYWqFMSjVqQ/+5vavqd8P2VsBXxzTNYZ7R3nsDoxrtHdZi/t2cUYjRGEaxnka70PspbTRiK2BX4EzgNVX1wX4RLl6SdwJ7AR8adt0D+Cqt0vNRVfXSXrGty1BB9FzgflX16mFK9X2Bs4HnjXGN03DT5nZV9X/D9tWAL1bVbn0jW3mGtdZ3oN0c+GxVvb9zSBskyV/QkusHATtV1U6dQ9KIJHkFcGvadNETWfPed33gr2jrlf+xqr7aLcgVIsmFzD/rYcytqxbFpFSjNowGPYy2OB9aS4/Dq+q/+0W1OLMw2ptkL+CDwDe4dJGEA7oFtURJdgGuCfwW+HZV/aZrQEuU5FjgvpNKwkmuDLyXdtPjxKravWd8a5Pka8C+VfXTJHekrZ17EnAL4MZVdb+uAS5Bkk8B+01mEgw9fI+pqn37RqZZkmTnSZG/JK+e24poOUlyG9oU3RsDW9IKgP16JVx4LzfDMon7AbdnzXvf6cBH1jWKKm1srinVqFXVy5OsZs2d70dU1Ul9o1q0cOm+YBezZh3dWLwVeAlwKuPsT3qJoYLl2Z3D2BiuA0xPp/4jsHNV/TbJcm4Ns/nUaOgDaLMG3ge8L8mynXa8Hj8Evpzkg7Q74QcCX5lMsa6ql/cMTrNhTtX523cLZGFeQ2tlcxRtRsdDaaNz2sSq6mfAm4YPqRuTUo1Wks1o64FuCoyqyusch9EuWKdHe9/SMZ6luKCqXtU7CF3KO4Hjh0QIYH/gyGEkfjm3H9o8yRWq6iJai6HHTh0b63vWd7l0caPJ72SbDrFIy0JVnZlk86q6GDhsqCwsaYUa6xu8RFX9KckpSa5TVT/oHc9Szcho74lJ/gM4mktP3x3zzYJRq6oXJjmGNc+rx1fVCcPhB/eLbL2OBD6T5ALaNLLPASS5Pq1Q0+hU1fMBkmzTNtuUam1aSe5JmzY96tkcM+I3wzT2k5O8FPgRsHXnmCR15JpSjVqS44C9ga8Av57sH8NaxqGgy/ZV9dE5+w+gNUw+sU9ki5fk0/PsrqoaVUuYJHvO/bkn2b+qPrS2z1nOktwBuEFVHTb0ZLxyVc3Xj25ZGdabXRP4+NRa6xvS4h/djY4kN6W1vbjqsOsC4KFVdVq/qFaeJG8Hbgu8Dzisqk7vHNLlJslJVXXL3nGsTZKdgf+lrSf9B1rv1ddV1ZldA5PUjUmpRm1OU/pLVNVnNnUsizWMjj58WMM4vf/6tHV0o0nokly3qs5a377lbiiy87CqOnXYfhDw1Kq6dd/IFi/Jc2lrtXarqhsmuRat6u5yX2s2c4Zpic+qqk8P26uAf6+q23UNbAUaWlw8iNaYvmjLJ46sqgu7BraRJXl4VR3eO451GW6UUVXn945FkOR2wC5MzaKsqrd1C0grjkmpRivJvWiFEU6tqmN7x7NYSU5dW+uaJKdU1R6bOqalSvK1qrrVnH0nVtWevWJaiqEn6Xtp01vvQCu+cc+qGt200aEo0C2Br01GTJJ8fUxVnWfFfK/nsb3GZ0mS7YGDaP1wT6e9j7xqZG24bggcTGvZMZ1ELOubmUkCPBd4Im1ZwWbARcCrq+oFPWNbyZIcAVwPOJk1hRerqp7cLyqtNK4p1SgleR1wE+CLwAuT7FNVL+wc1mL92TqOjWJtTZIb0X4PVxn6/01sS+t1NipVdVaSBwIfAM4B/rqqfts5rKX6Q1VVkoJLWg2pj7OS/CttCi+0hGjZT6OeNUn2Bx5Ju/g+Atinqn6S5Eq05HQ0SSmtau0baBVTL17PY5eTp9IqA+89WUow3Ax8fZJ/qKpXdI1u5doL2L0cqVJHJqUaqzsCe1TVxcMFxeeAsSWln0zyIuDZ028ESZ4PHNcvrEXZDbgnsB2tuuvEhcBjukS0BElO5dJNq69K65v35SSMdHTxPUneCGyX5DG0i/E3d45ppXok8Hzgf4btz9Kmj2rTuj/wiqr67PTOqvpNkkd2immpLqqq1/cOYgkeCty1qi6Y7BhuBh4EfBwwKe3jG8A1aAWnpC6cvqv/3969B9tZ1Wcc/z7ERBATBGWs0BIJAvWCaAAVAS/cbOuFgiLoiKig9QJFbRFpVUBRK2IthotIbIYqggZlQB25yFVaoeUSAS+1GDtaaYtBhEiogfD0j/VusxNPEs45yVl77f18Zvacs983e+ZhOGfv83vftX6/Jq2+XHSs5aODrrtzNR94HmXJDMDOwE3AW1vZ4yRpGnCc7Y/VzjJRXdONNVpt/l8zJO0H7E9ZJneZ7SsqRxo53b652cCdtn9dO08MB0knAncDF7Fqx/Nfrek1g0DSHd0Yt3Gdiw1D0tcpF2RnAs+hNI3s/3ka+KaRMTxSlEaTJC0Del36RFmOdWf3vVu6s9UtXXpm9/T7rTUHgtJ91/ZLa+dYH1rtWLsu3cWDQ22fVzvLqJB0JPAxyozSbYG32b6kbqrR1XV1ngc8ndL1dRrwgO1ZVYNNgKSx3pNse86UhxmHtV1AbvHicuvW1Cyyp4WmkTE8UpRGk4bpzpakK23vs65jg6xbhrwZ8GVWHc3T1PiOYehY23UXfRewNWVu7BXd82OBRbYPqBhvpEi6A3ip7V92F5/Os7177VyjStJNwKGU/Zi7UpaSPs3231YNNkIkraDvM6L/FLCx7elTHCkASZ+wfdy6jkVsSNlTGk3qFZ3dEtgHbT/SdSP8Y+Bba33xgJC0MaWh0ZMkbU75UIbSJGirasEmpjfaor97ooGB7gQ5hgPpOtYC2L5L0sy6kcbtC8C9wHeBIynF6AzgANuL1vbCWO+W98ZddPvmHls70KizfaekabZXAAu6cT3NkTQdeAelvwLANcDZth+qFupRsD2tdoYY037A6gXon45xLGKDSVEarbsO2Ksr6q6k7Mc8hDLSY9D9BaUT4VbAzawsSu8HzqgVaiKGZekuw9Gxdk5v1JCk+cASYJtW9igPmT+U9Jk1Pc+4hSm3TNIMYJGkUyhNXVr8HQc4C5gOnNk9P6w7dmS1RNEcSe8A3gnMkXRb36mZlOkGEVMmy3ejab09KJKOBjaxfYqkW3tzGVsg6eiW5uONRdJmlNlzvav21wIfbm2+p6S/BranXDX+OKVr6pda+v8zDE3AhoWkw9d23va5U5Ulfrft438pKwfeQ9lycKbtO9f6wgGU2bexPnSf3ZtTPu/e33dq6aA3zYrhk6I0mibpVspVvk8DR9j+vqTbe3eKWiDpYOBS20slfQCYC5zc0n5MSV+ltJTv/ZF9GGVkz0FrftVgar1j7Wp7tkSZh7uMlU3AmmvqMuwkzbN9dO0co6BrXkZvWXWrJN0CHGz7J93zOcCFuQAVE9U1w3syfasobf+sXqIYNVm+G607BjgeuKgrSOcAV1fONF4ftL2w6/r6MuBUyjKs59eNNS7b2X513z3OP9gAAAx9SURBVPOTJLW6f/HHlOLt25IeJ2lmS0tfs2erSc000mqRJFFWchxFuTizkaSHgXm2P7zWFw+uY4GrJS2m/DfNJrNvY4IkHQWcSFlJ8Eh32EAzkwyifSlKo2ndEPTr+p4vBlrbp7Wi+/py4CzbF3cz6FryoKQ9bV8PIGkP4MHKmcZN0luBtwFbUMYMbQ18FmimE3JE/J53Uwr/3XrjnboLmGdJeo/tT1dNNwG2r5S0PbAjpSj9ke3fruNlEWvybkrX+XtqB4nRleW70bRuKdb7KHM+N+4dt91M11dJ3wB+AewL7EIp5v61pb1Bkp5DWbq7GeUPpF8Bb7L9varBxqm7u/s84MbevuTWloNHe7Lvd8PqtnnsZ3vJase3BC5vrAfB3ravkjTm1gjbX5vqTNE+SVdTfkcerp0lRlfulEbrzqPMxnwF8HbgcKC1vUKvBf4EONX2ryU9hbI0qxndqJGduxmZ2L6/cqSJ+q3t5WW1H0h6DGUJU8SGpHX/k5iE6asXpFD2lXajVVryYuAq4JVjnDOQojQmYjFwjaRvAr+742777+tFilGTojRa90Tbn5d0jO1rgWslXVs71HjYXibpbmBP4D+Ah7uvzZB0DLAAWAqcI2ku8H7bl9dNNm7XSvobYJOu4dE7ga9XzhTD77TaAYbc8gmeGzi2T+i+Zv9orE8/6x4zukfElMvy3WiapBtsv0DSZcBngLsoHQi3qxztUZN0ArArZT/HDpK2Ahbabqb5SW8UgaSXAe8CPggsaG1JoqSNgCPo674LzHfeKGMSJO1AWf0wm1U7WzazzaBlq3WkXuUUsLHt1u6WIum9Yxy+D7i5W7kSMW6SZlIa/f2mdpYYPblTGq07uZuz9VfAPGAWZf5cSw4EngvcAmD7ru6DoSW95Yd/RilGv6feGti2vAQ4z/Y5tYPEUFlIaZh1Disbm8UUGdKO1Lt2j95KjpcD/wa8XdJC26dUSxbNkfQs4AuUJn9IWgK80fb3qwaLkZKiNJpm+xvdt/cBL62ZZRKW27YkA0jatHagCbhZ0uXAtsDxXVH9yDpeM4jeBHxW0j3Ad7rH9bbvrZoqWvew7bNqh4ih8kRgbu+OVrfi5kLgRcDNQIrSGI/PAe+1fTWApJdQLqK9sGaoGC0pSqNJkuaxlgY0tlsaC/MVSWcDT+hGkryF8mHQkiOA5wCLuz2yW9DgzDzbbwTollC/BjgD2Iq8V8bkfF3SO4GLWLWJyK/qRYrGbcOq+2EfAmbbflBSRsPEeG3aK0gBbF/T6AXyaFj+0IpW3dT3/UmUwehNsn1q11TnfsrMuQ/ZvqJyrPHaHVhk+wFJbwDm0mDzli77XsBOwBLgdMrd0ojJOLz72t9V28CcClliOHwJuEHSxd3zVwLnd4XED+rFikYtlvRByhJegDcAP62YJ0ZQGh1F8yTd2tKcuX6SpgGX2d63dpbJkHQbsDPwbMqH2ueBg2y/uGqwcer20fyEsv/vatv/WTdRRMTYJO1C6douyjaDm9bxkogxSdqccoG/9/N0HXBitq7EVMqd0hgGzV5Zsb1C0jJJm9m+r3aeSXi42xd7AHBaN6bn8HW+asDYfpKkZ1L2ZX1U0vbAv9s+rHK0aFg3C/MdlJ8rgGuAs20/VC1UDINNgPttL5C0paRtbefuVoxbV3y2tO0phlCK0oj6/g+4XdIV9I0taGxf7FJJxwOHAXt1d4Cbe3+RNIuyV2s28FRgM9ps2BSD5SxgOnBm9/yw7tiR1RJF0/pHiVFmRE8Hvgg0M0os6pN0ydrO237VVGWJaO6PxggASUtZeYf0cZLu752izNiaVSfZhHyze7TsEOD1wJtt/4+kFwEtNkm4vu9xuu3/qpwnhsNutnfue36VpO9VSxPDYBhGiUV9uwM/B84HbmTleLeIKZeiNJpke2g+fG2fWzvDZHWF6FXA6yV9kdIg4R8qxxo328+unSGG0gpJ29n+CYCkOWReaUzOMIwSi/r+ANgPeB3lwvI3gfMznzRqSFEaUVm3b/HjwDOAjXvHbQ98Z05JOwCHUj7Q7gG+TGmg1urM2IgN4VjgakmLKXciZtPgyKQYKGONEptfOVM0xvYK4FLgUkmPpXyWXyPpw7bn1U0XoybddyMqk3Q9ZaTNpylt/d9M+d0c+DE3kh6hjEw5wvad3bHFLRTUEVOp+4NvR0pR+iPbmSUZk9KNEtuf8jN1WYOjxGIAdO9NL6cUpE8FLgH+0fYvauaK0ZOiNKIySTfb3kXS7bZ36o59x/ZetbOti6QDKXdKX0i52noBMN/2tlWDjZOkT9g+TtLBthfWzhPDQdLetq+SdNBY521/baozxXDqmssdavu82lmiHZLOBZ4FfAu4wPYdlSPFCEtRGlGZpH8G9gIuBK4CfgH8ne0dqwYbh24/059TrrTuDZwLXGT78qrBHiVJtwNzgRttz62dJ4aDpJNsnyBpwRinbfstUx4qmtZ1CH8XsDXljtYV3fNjgUW2D6gYLxrTrXbqdf3vLwhabBoZjUtRGlGZpN2AHwJPAD4CzAI+afuGqsEmSNIWwMHAIbb3rp3n0ZD0SeBtlI7By+g+kMkHc0QMEEkXA/cC3wX2ATYHZgDH2F5UM1tExGSkKI0YEJI2tf3Auv9lbCiSLs6dhljfJL13jMP3ATenkIjxWG2bxzRgCbCN7aV1k0VETM5GtQNEjDpJu0v6AeVuKZJ2lnRm5VgjyfYBkp4s6RXdY8vamWIo7Aq8nbLkcmvKXfmXAOdIel/FXNGeh3rfdJ1Tf5qCNCKGQe6URlQm6UbgNcAltp/bHbvD9rPqJhs9kg4GTgWuoSzd3Qs41vaFNXNF2yRdBrza9m+654+n7CE/kHK39Bk180U7JK1g5R5AAZvQt+UgWw0iolWZUxoxAGz/XFL/oRW1soy4DwC72b4boLtT+m1KARExUdsAy/uePwTMtv2gpIyGiUfN9rTaGSIiNoQUpRH1/VzSCwFLmgH8Jd1S3phyG/UK0s49ZJtDTN6XgBu6JjVQ5hGf33Wt/kG9WBEREYMhy3cjKpP0JOA0YF/KEqzLKZ0U76kabAR1XXifDZzfHToEuM32cfVSxTCQtAuwJ+V3/HrbN1WOFBERMTBSlEZE9JF0ECuLh+tsX1Q5UgwBSXsC29te0C0Lf7ztn9bOFRERMQhSlEZUIulDazlt2x+ZsjARscFIOoHSgXdH2ztI2gpYaHuPytEiIiIGQvZKRdTzwBgPgCOALBeNGB4HAq+i+x23fRcws2qiiIiIAZJGRxGV2P5U73tJM4FjgDcDFwCfWtPrIqI5y21bkgG6BkcRERHRyZ3SiIokbSHpZOA2ykWiubaPW60DbEwRSa+QlPfFWN++Iuls4AmS3koZMzS/cqaIiIiBkT2lEZV0nV4PAj4HnGH7N5UjjTxJXwR2B74KLLCd0TyxXkjaD9if0kDrMttXVI4UERExMFKURlQi6RHgt8DDQP8voiiNjmZVCTbiJM0CXkdZSm1gAXC+7aVVg8XQkDQNONT2ebWzREREDIIsU4uoxPZGtjexPdP2rL7HzBSk9di+n3Kn9ALgKZQmNbdIOrpqsGiOpFmSjpd0uqT9VRwFLAZeWztfRETEoMid0oiIjqRXAm8BtgO+AJxr+25JjwN+aHt21YDRFEkXA/cC3wX2ATYHZgDH2F5UM1tERMQgSVEaEdGR9E/AfNvXjXFuH9tXVogVjZJ0u+2duu+nAUuAbbIUPCIiYlUpSiMiIjYASbfYnrum5xEREVGkKI2I6Eh6ATAPeDplmeU04IHs8Y2JkLQCeKD3FNgEWEaamUVERKziMbUDREQMkNOBQ4GFwK7AG4GnVU0UzbI9rXaGiIiIFqQojYjoY/tOSdNsrwAWSPqX2pkiIiIihlmK0oiIlZZJmgEsknQK8N/AppUzRURERAy1zCmNiFjpMMr74lGUvYB/BLy6aqKIiIiIIZdGRxERfSRtCWD7l7WzRERERIyC3CmNiJGn4kRJS4AfAT+W9EtJH6qdLSIiImLYpSiNiIB3A3sAu9l+ou3NgecDe0h6T91oEREREcMty3cjYuRJuhXYz/aS1Y5vCVxu+7l1kkVEREQMv9wpjYiA6asXpPC7faXTK+SJiIiIGBkpSiMiYPkEz0VERETEJGX5bkSMPEkrKCNgfu8UsLHt3C2NiIiI2EBSlEZEREREREQ1Wb4bERERERER1aQojYiIiIiIiGpSlEZEREREREQ1KUojIiIiIiKimhSlERERERERUU2K0oiIiIiIiKjm/wH6slOzIYWY6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model Parameters:\n",
      "Win\n",
      "7\n",
      "3.6.7\n",
      "sklearn\n",
      "0.20.2\n",
      "['Win', '7', 'Python', '3.6.7', 'sklearn', '0.20.2', 'ExtraTreesRegressor', 1.0066440105438232, 0.8810141086578369, 0.44656076431274416, 8.939087629318237, 0.5306822996170675, 1215.3310048931853, 3852313.1394878863, 1962.731041046604, 0.5192707252591598, 0.10920000076293945, 0.6028330361805241, 1109.069321244793, 2394670.5888502817, 1547.4723224827906, 0.6003824226912086, {'bootstrap': False, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}, [{'1. feat: 6 IsOpen (0.466111)'}, {'2. feat: 13 StoreID (0.081406)'}, {'3. feat: 4 HasPromotions (0.070828)'}, {'4. feat: 8 NearestCompetitor (0.066025)'}, {'5. feat: 0 AssortmentType (0.051350)'}, {'6. feat: 2 Day of week (number) (0.046155)'}, {'7. feat: 11 Region_GDP (0.045231)'}, {'8. feat: 14 StoreType (0.027476)'}, {'9. feat: 12 Region_PopulationK (0.024956)'}, {'10. feat: 9 Region (0.021080)'}, {'11. feat: 1 Day of month (0.020623)'}, {'12. feat: 10 Region_AreaKM2 (0.019297)'}, {'13. feat: 3 Day of year (0.017966)'}, {'14. feat: 15 Week (0.016538)'}, {'15. feat: 7 Month (number) (0.014597)'}, {'16. feat: 16 Year (0.008076)'}, {'17. feat: 5 IsHoliday (0.002285)'}], StoreID                 int32\n",
      "IsHoliday               int32\n",
      "IsOpen                  int32\n",
      "HasPromotions           int32\n",
      "StoreType                int8\n",
      "AssortmentType           int8\n",
      "NearestCompetitor       int32\n",
      "Region                  int32\n",
      "NumberOfSales           int32\n",
      "Region_AreaKM2          int32\n",
      "Region_GDP              int32\n",
      "Region_PopulationK      int32\n",
      "Year                    int32\n",
      "Month (number)          int32\n",
      "Week                    int32\n",
      "Day of year             int32\n",
      "Day of month            int32\n",
      "Day of week (number)    int32\n",
      "dtype: object, 'Default Settings, TS CV, only 1 win. open']\n",
      "***Process Completed***\n"
     ]
    }
   ],
   "source": [
    "#Calculate Feature importances and Graph:\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Feature Importances:\")\n",
    "global d\n",
    "d=[]\n",
    "for f in range(trainDataset_X.shape[1]):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], trainDataset_X.columns[indices[f]], importances[indices[f]]))\n",
    "    d.append({\"%d. feat: %d %s (%f)\" % (f + 1, indices[f], trainDataset_X.columns[indices[f]], importances[indices[f]])})\n",
    "\n",
    "Feat_Imp = pd.DataFrame(d)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(trainDataset_X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(trainDataset_X.shape[1]), trainDataset_X.columns[indices],rotation=90)\n",
    "plt.xlim([-1, trainDataset_X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "print(\" \")\n",
    "print(\"Model Parameters:\")\n",
    "Params\n",
    "\n",
    "\n",
    "#InsertHeader()\n",
    "InsertValues()\n",
    "print('***Process Completed***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:53:42.351221Z",
     "start_time": "2019-04-25T20:53:41.890199Z"
    },
    "code_folding": [],
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns:\n",
      "Index(['StoreType', 'AssortmentType'], dtype='object')\n",
      "Feature Columns:\n",
      "Index(['AssortmentType', 'Day of month', 'Day of week (number)', 'Day of year',\n",
      "       'HasPromotions', 'IsHoliday', 'IsOpen', 'Month (number)',\n",
      "       'NearestCompetitor', 'Region', 'Region_AreaKM2', 'Region_GDP',\n",
      "       'Region_PopulationK', 'StoreID', 'StoreType', 'Week', 'Year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Load Train Set:\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import csv\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "trainBench = pd.read_csv(\"c:/Benchmarking/trainBench.csv\")\n",
    "#testBench = pd.read_csv(\"c:/Benchmarking/testBench.csv\")\n",
    "\n",
    "trainBench = trainBench.drop(\"ID\", axis=1)\n",
    "\n",
    "cat_columns = trainBench.select_dtypes(['object']).columns\n",
    "print(\"Categorical Columns:\")\n",
    "print(cat_columns)\n",
    "trainBench[cat_columns] = trainBench[cat_columns].astype('category')\n",
    "cat_columns = trainBench.select_dtypes(['category']).columns\n",
    "trainBench[cat_columns] = trainBench[cat_columns].apply(lambda x: x.cat.codes)\n",
    "Int64columns = trainBench.select_dtypes(['int64']).columns\n",
    "#Int64columns\n",
    "trainBench[Int64columns] = trainBench[Int64columns].astype(np.int32)\n",
    "#trainBench.info()\n",
    "mask = trainBench.columns.difference(['NumberOfSales'])\n",
    "trainDataset_X = trainBench[mask]\n",
    "print(\"Feature Columns:\")\n",
    "print(mask)\n",
    "trainDataset_y = trainBench['NumberOfSales']\n",
    "del trainBench\n",
    "gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "#Load Validation Set:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "validBench = pd.read_csv(\"c:/Benchmarking/validBench.csv\")\n",
    "validBench = validBench.drop(\"ID\", axis=1)\n",
    "#Int64columns = validBench.select_dtypes(['int64']).columns\n",
    "#Int64columns\n",
    "validBench[Int64columns] = validBench[Int64columns].astype(np.int32)\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Converting Validation Categorical Columns to Numbers:\")\n",
    "cat_columns\n",
    "validBench[cat_columns] = validBench[cat_columns].astype('category')\n",
    "cat_columns = validBench.select_dtypes(['category']).columns\n",
    "validBench[cat_columns] = validBench[cat_columns].apply(lambda x: x.cat.codes)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Head of Validation Data:\")\n",
    "print(validBench.head(3))\n",
    "print(\" \")\n",
    "print(mask)\n",
    "validBench_X = validBench[mask]\n",
    "validBench_y = validBench['NumberOfSales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Cross Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:52:03.572887Z",
     "start_time": "2019-06-15T14:52:03.414878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=5)\n",
      "TRAIN: [    0     1     2 ... 16760 16761 16762] TEST: [16763 16764 16765 ... 33518 33519 33520]\n",
      "16762 16757\n",
      "TRAIN: [    0     1     2 ... 33518 33519 33520] TEST: [33521 33522 33523 ... 50276 50277 50278]\n",
      "33520 16757\n",
      "TRAIN: [    0     1     2 ... 50276 50277 50278] TEST: [50279 50280 50281 ... 67034 67035 67036]\n",
      "50278 16757\n",
      "TRAIN: [    0     1     2 ... 67034 67035 67036] TEST: [67037 67038 67039 ... 83792 83793 83794]\n",
      "67036 16757\n",
      "TRAIN: [    0     1     2 ... 83792 83793 83794] TEST: [ 83795  83796  83797 ... 100550 100551 100552]\n",
      "83794 16757\n",
      "<generator object TimeSeriesSplit.split at 0x00000000341BC888>\n"
     ]
    }
   ],
   "source": [
    "#Testing Time series cross validation:\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "print(tscv)\n",
    "#scores = cross_validate(forest, trainDataset_X, trainDataset_y, cv=kfolds, scoring=('r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error') )\n",
    "\n",
    "X = trainDataset_X\n",
    "y = trainDataset_y\n",
    " \n",
    "for train_index, test_index in tscv.split(X):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   train = X[min(train_index):max(train_index),:]\n",
    "   test = X[min(test_index):max(test_index),:]\n",
    "   print(len(train),len(test))\n",
    "\n",
    "del(train,test)\n",
    "print(tscv.split(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T16:23:20.415375Z",
     "start_time": "2019-06-17T16:23:20.376373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AssortmentType</th>\n",
       "      <th>Day of month</th>\n",
       "      <th>Day of week (number)</th>\n",
       "      <th>Day of year</th>\n",
       "      <th>HasPromotions</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>IsOpen</th>\n",
       "      <th>Month (number)</th>\n",
       "      <th>NearestCompetitor</th>\n",
       "      <th>Region</th>\n",
       "      <th>Region_AreaKM2</th>\n",
       "      <th>Region_GDP</th>\n",
       "      <th>Region_PopulationK</th>\n",
       "      <th>StoreID</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Week</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>9643</td>\n",
       "      <td>17130</td>\n",
       "      <td>2770</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AssortmentType  Day of month  Day of week (number)  Day of year  \\\n",
       "0               0             1                     3           61   \n",
       "1               0             2                     4           62   \n",
       "2               0             4                     6           64   \n",
       "\n",
       "   HasPromotions  IsHoliday  IsOpen  Month (number)  NearestCompetitor  \\\n",
       "0              0          0       1               3                326   \n",
       "1              0          0       1               3                326   \n",
       "2              0          0       1               3                326   \n",
       "\n",
       "   Region  Region_AreaKM2  Region_GDP  Region_PopulationK  StoreID  StoreType  \\\n",
       "0       7            9643       17130                2770     1000          0   \n",
       "1       7            9643       17130                2770     1000          0   \n",
       "2       7            9643       17130                2770     1000          0   \n",
       "\n",
       "   Week  Year  \n",
       "0    10  2016  \n",
       "1    10  2016  \n",
       "2    10  2016  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset_X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.593px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 180.89234399999998,
   "position": {
    "height": "39.9957px",
    "left": "694.805px",
    "right": "20px",
    "top": "-4.01372px",
    "width": "480.515px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
